{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 0.5,
  "eval_steps": 500,
  "global_step": 100,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "bce_eff_w": 0.001,
      "bce_loss": -0.4642454981803894,
      "epoch": 0,
      "step": 0
    },
    {
      "epoch": 0.005,
      "grad_norm": 85.84219360351562,
      "learning_rate": 0.0,
      "loss": 10.7676,
      "step": 1
    },
    {
      "bce_eff_w": 0.002,
      "bce_loss": -0.4629226326942444,
      "epoch": 0.005,
      "step": 1
    },
    {
      "epoch": 0.01,
      "grad_norm": 104.53201293945312,
      "learning_rate": 5.000000000000001e-07,
      "loss": 11.7053,
      "step": 2
    },
    {
      "bce_eff_w": 0.003,
      "bce_loss": -0.46219390630722046,
      "epoch": 0.01,
      "step": 2
    },
    {
      "epoch": 0.015,
      "grad_norm": 82.5286636352539,
      "learning_rate": 1.0000000000000002e-06,
      "loss": 11.7021,
      "step": 3
    },
    {
      "bce_eff_w": 0.004,
      "bce_loss": -0.46316054463386536,
      "epoch": 0.015,
      "step": 3
    },
    {
      "epoch": 0.02,
      "grad_norm": 80.69847106933594,
      "learning_rate": 1.5e-06,
      "loss": 12.2601,
      "step": 4
    },
    {
      "bce_eff_w": 0.005000000000000001,
      "bce_loss": -0.461592435836792,
      "epoch": 0.02,
      "step": 4
    },
    {
      "epoch": 0.025,
      "grad_norm": 86.84578704833984,
      "learning_rate": 2.0000000000000003e-06,
      "loss": 11.7389,
      "step": 5
    },
    {
      "bce_eff_w": 0.006,
      "bce_loss": -0.45473015308380127,
      "epoch": 0.025,
      "step": 5
    },
    {
      "epoch": 0.03,
      "grad_norm": 73.65128326416016,
      "learning_rate": 2.5e-06,
      "loss": 13.1643,
      "step": 6
    },
    {
      "bce_eff_w": 0.007000000000000001,
      "bce_loss": -0.46241703629493713,
      "epoch": 0.03,
      "step": 6
    },
    {
      "epoch": 0.035,
      "grad_norm": 81.01696014404297,
      "learning_rate": 3e-06,
      "loss": 12.0964,
      "step": 7
    },
    {
      "bce_eff_w": 0.008,
      "bce_loss": -0.46094831824302673,
      "epoch": 0.035,
      "step": 7
    },
    {
      "epoch": 0.04,
      "grad_norm": 75.89124298095703,
      "learning_rate": 3.5000000000000004e-06,
      "loss": 12.8837,
      "step": 8
    },
    {
      "bce_eff_w": 0.009,
      "bce_loss": -0.45338815450668335,
      "epoch": 0.04,
      "step": 8
    },
    {
      "epoch": 0.045,
      "grad_norm": 66.14452362060547,
      "learning_rate": 4.000000000000001e-06,
      "loss": 12.8063,
      "step": 9
    },
    {
      "bce_eff_w": 0.010000000000000002,
      "bce_loss": -0.45690488815307617,
      "epoch": 0.045,
      "step": 9
    },
    {
      "epoch": 0.05,
      "grad_norm": 73.90782928466797,
      "learning_rate": 4.5e-06,
      "loss": 12.1405,
      "step": 10
    },
    {
      "bce_eff_w": 0.011000000000000001,
      "bce_loss": -0.46364596486091614,
      "epoch": 0.05,
      "step": 10
    },
    {
      "epoch": 0.055,
      "grad_norm": 75.31083679199219,
      "learning_rate": 5e-06,
      "loss": 11.8675,
      "step": 11
    },
    {
      "bce_eff_w": 0.012,
      "bce_loss": -0.46306291222572327,
      "epoch": 0.055,
      "step": 11
    },
    {
      "epoch": 0.06,
      "grad_norm": 72.13374328613281,
      "learning_rate": 5.500000000000001e-06,
      "loss": 11.4638,
      "step": 12
    },
    {
      "bce_eff_w": 0.013000000000000001,
      "bce_loss": -0.4640568792819977,
      "epoch": 0.06,
      "step": 12
    },
    {
      "epoch": 0.065,
      "grad_norm": 83.87628936767578,
      "learning_rate": 6e-06,
      "loss": 11.5718,
      "step": 13
    },
    {
      "bce_eff_w": 0.014000000000000002,
      "bce_loss": -0.46282026171684265,
      "epoch": 0.065,
      "step": 13
    },
    {
      "epoch": 0.07,
      "grad_norm": 78.27046966552734,
      "learning_rate": 6.5000000000000004e-06,
      "loss": 12.3899,
      "step": 14
    },
    {
      "bce_eff_w": 0.015,
      "bce_loss": -0.46452537178993225,
      "epoch": 0.07,
      "step": 14
    },
    {
      "epoch": 0.075,
      "grad_norm": 84.27233123779297,
      "learning_rate": 7.000000000000001e-06,
      "loss": 12.3777,
      "step": 15
    },
    {
      "bce_eff_w": 0.016,
      "bce_loss": -0.4637390375137329,
      "epoch": 0.075,
      "step": 15
    },
    {
      "epoch": 0.08,
      "grad_norm": 86.64652252197266,
      "learning_rate": 7.5e-06,
      "loss": 11.894,
      "step": 16
    },
    {
      "bce_eff_w": 0.017,
      "bce_loss": -0.45734113454818726,
      "epoch": 0.08,
      "step": 16
    },
    {
      "epoch": 0.085,
      "grad_norm": 66.97266387939453,
      "learning_rate": 8.000000000000001e-06,
      "loss": 11.1208,
      "step": 17
    },
    {
      "bce_eff_w": 0.018,
      "bce_loss": -0.46201974153518677,
      "epoch": 0.085,
      "step": 17
    },
    {
      "epoch": 0.09,
      "grad_norm": 92.46121978759766,
      "learning_rate": 8.500000000000002e-06,
      "loss": 12.2197,
      "step": 18
    },
    {
      "bce_eff_w": 0.019000000000000003,
      "bce_loss": -0.4628230333328247,
      "epoch": 0.09,
      "step": 18
    },
    {
      "epoch": 0.095,
      "grad_norm": 63.311439514160156,
      "learning_rate": 9e-06,
      "loss": 11.4324,
      "step": 19
    },
    {
      "bce_eff_w": 0.020000000000000004,
      "bce_loss": -0.4605138897895813,
      "epoch": 0.095,
      "step": 19
    },
    {
      "epoch": 0.1,
      "grad_norm": 69.92082214355469,
      "learning_rate": 9.5e-06,
      "loss": 11.1109,
      "step": 20
    },
    {
      "bce_eff_w": 0.021,
      "bce_loss": -0.45598578453063965,
      "epoch": 0.1,
      "step": 20
    },
    {
      "epoch": 0.105,
      "grad_norm": 76.43155670166016,
      "learning_rate": 1e-05,
      "loss": 11.6818,
      "step": 21
    },
    {
      "bce_eff_w": 0.022000000000000002,
      "bce_loss": -0.4571077227592468,
      "epoch": 0.105,
      "step": 21
    },
    {
      "epoch": 0.11,
      "grad_norm": 69.51325988769531,
      "learning_rate": 1.05e-05,
      "loss": 11.0156,
      "step": 22
    },
    {
      "bce_eff_w": 0.023000000000000003,
      "bce_loss": -0.4625406861305237,
      "epoch": 0.11,
      "step": 22
    },
    {
      "epoch": 0.115,
      "grad_norm": 74.79623413085938,
      "learning_rate": 1.1000000000000001e-05,
      "loss": 12.8388,
      "step": 23
    },
    {
      "bce_eff_w": 0.024,
      "bce_loss": -0.4615333080291748,
      "epoch": 0.115,
      "step": 23
    },
    {
      "epoch": 0.12,
      "grad_norm": 75.76701354980469,
      "learning_rate": 1.1500000000000002e-05,
      "loss": 12.4748,
      "step": 24
    },
    {
      "bce_eff_w": 0.025,
      "bce_loss": -0.4551323354244232,
      "epoch": 0.12,
      "step": 24
    },
    {
      "epoch": 0.125,
      "grad_norm": 76.49307250976562,
      "learning_rate": 1.2e-05,
      "loss": 10.6097,
      "step": 25
    },
    {
      "bce_eff_w": 0.026000000000000002,
      "bce_loss": -0.46041569113731384,
      "epoch": 0.125,
      "step": 25
    },
    {
      "epoch": 0.13,
      "grad_norm": 63.68626022338867,
      "learning_rate": 1.25e-05,
      "loss": 12.6196,
      "step": 26
    },
    {
      "bce_eff_w": 0.027000000000000003,
      "bce_loss": -0.46431347727775574,
      "epoch": 0.13,
      "step": 26
    },
    {
      "epoch": 0.135,
      "grad_norm": 85.33142852783203,
      "learning_rate": 1.3000000000000001e-05,
      "loss": 10.958,
      "step": 27
    },
    {
      "bce_eff_w": 0.028000000000000004,
      "bce_loss": -0.4645507335662842,
      "epoch": 0.135,
      "step": 27
    },
    {
      "epoch": 0.14,
      "grad_norm": 75.46675872802734,
      "learning_rate": 1.3500000000000001e-05,
      "loss": 10.5198,
      "step": 28
    },
    {
      "bce_eff_w": 0.028999999999999998,
      "bce_loss": -0.46384236216545105,
      "epoch": 0.14,
      "step": 28
    },
    {
      "epoch": 0.145,
      "grad_norm": 88.0983657836914,
      "learning_rate": 1.4000000000000001e-05,
      "loss": 10.2405,
      "step": 29
    },
    {
      "bce_eff_w": 0.03,
      "bce_loss": -0.46386048197746277,
      "epoch": 0.145,
      "step": 29
    },
    {
      "epoch": 0.15,
      "grad_norm": 95.4078140258789,
      "learning_rate": 1.45e-05,
      "loss": 9.293,
      "step": 30
    },
    {
      "bce_eff_w": 0.031,
      "bce_loss": -0.4539857506752014,
      "epoch": 0.15,
      "step": 30
    },
    {
      "epoch": 0.155,
      "grad_norm": 62.74131774902344,
      "learning_rate": 1.5e-05,
      "loss": 11.161,
      "step": 31
    },
    {
      "bce_eff_w": 0.032,
      "bce_loss": -0.462363600730896,
      "epoch": 0.155,
      "step": 31
    },
    {
      "epoch": 0.16,
      "grad_norm": 62.125511169433594,
      "learning_rate": 1.55e-05,
      "loss": 11.6598,
      "step": 32
    },
    {
      "bce_eff_w": 0.033,
      "bce_loss": -0.46432462334632874,
      "epoch": 0.16,
      "step": 32
    },
    {
      "epoch": 0.165,
      "grad_norm": 72.71183776855469,
      "learning_rate": 1.6000000000000003e-05,
      "loss": 12.7297,
      "step": 33
    },
    {
      "bce_eff_w": 0.034,
      "bce_loss": -0.463609904050827,
      "epoch": 0.165,
      "step": 33
    },
    {
      "epoch": 0.17,
      "grad_norm": 67.61325073242188,
      "learning_rate": 1.65e-05,
      "loss": 12.8161,
      "step": 34
    },
    {
      "bce_eff_w": 0.034999999999999996,
      "bce_loss": -0.46319258213043213,
      "epoch": 0.17,
      "step": 34
    },
    {
      "epoch": 0.175,
      "grad_norm": 81.17929077148438,
      "learning_rate": 1.7000000000000003e-05,
      "loss": 11.6377,
      "step": 35
    },
    {
      "bce_eff_w": 0.036,
      "bce_loss": -0.464988112449646,
      "epoch": 0.175,
      "step": 35
    },
    {
      "epoch": 0.18,
      "grad_norm": 63.93184280395508,
      "learning_rate": 1.75e-05,
      "loss": 10.1599,
      "step": 36
    },
    {
      "bce_eff_w": 0.037,
      "bce_loss": -0.4650863707065582,
      "epoch": 0.18,
      "step": 36
    },
    {
      "epoch": 0.185,
      "grad_norm": 68.13023376464844,
      "learning_rate": 1.8e-05,
      "loss": 10.0638,
      "step": 37
    },
    {
      "bce_eff_w": 0.038000000000000006,
      "bce_loss": -0.46326056122779846,
      "epoch": 0.185,
      "step": 37
    },
    {
      "epoch": 0.19,
      "grad_norm": 71.80599975585938,
      "learning_rate": 1.85e-05,
      "loss": 10.8613,
      "step": 38
    },
    {
      "bce_eff_w": 0.03900000000000001,
      "bce_loss": -0.4649215638637543,
      "epoch": 0.19,
      "step": 38
    },
    {
      "epoch": 0.195,
      "grad_norm": 59.63525390625,
      "learning_rate": 1.9e-05,
      "loss": 10.1153,
      "step": 39
    },
    {
      "bce_eff_w": 0.04000000000000001,
      "bce_loss": -0.46308764815330505,
      "epoch": 0.195,
      "step": 39
    },
    {
      "epoch": 0.2,
      "grad_norm": 71.82445526123047,
      "learning_rate": 1.9500000000000003e-05,
      "loss": 10.0241,
      "step": 40
    },
    {
      "bce_eff_w": 0.041,
      "bce_loss": -0.463169664144516,
      "epoch": 0.2,
      "step": 40
    },
    {
      "epoch": 0.205,
      "grad_norm": 51.83140182495117,
      "learning_rate": 2e-05,
      "loss": 9.4505,
      "step": 41
    },
    {
      "bce_eff_w": 0.042,
      "bce_loss": -0.4597465395927429,
      "epoch": 0.205,
      "step": 41
    },
    {
      "epoch": 0.21,
      "grad_norm": 59.13808059692383,
      "learning_rate": 2.05e-05,
      "loss": 11.4873,
      "step": 42
    },
    {
      "bce_eff_w": 0.043000000000000003,
      "bce_loss": -0.464954137802124,
      "epoch": 0.21,
      "step": 42
    },
    {
      "epoch": 0.215,
      "grad_norm": 79.29312896728516,
      "learning_rate": 2.1e-05,
      "loss": 9.6266,
      "step": 43
    },
    {
      "bce_eff_w": 0.044000000000000004,
      "bce_loss": -0.46223780512809753,
      "epoch": 0.215,
      "step": 43
    },
    {
      "epoch": 0.22,
      "grad_norm": 53.06949996948242,
      "learning_rate": 2.15e-05,
      "loss": 10.1356,
      "step": 44
    },
    {
      "bce_eff_w": 0.045000000000000005,
      "bce_loss": -0.46459031105041504,
      "epoch": 0.22,
      "step": 44
    },
    {
      "epoch": 0.225,
      "grad_norm": 60.753143310546875,
      "learning_rate": 2.2000000000000003e-05,
      "loss": 10.0095,
      "step": 45
    },
    {
      "bce_eff_w": 0.046000000000000006,
      "bce_loss": -0.4616676867008209,
      "epoch": 0.225,
      "step": 45
    },
    {
      "epoch": 0.23,
      "grad_norm": 47.26900863647461,
      "learning_rate": 2.25e-05,
      "loss": 9.6224,
      "step": 46
    },
    {
      "bce_eff_w": 0.047,
      "bce_loss": -0.4622891843318939,
      "epoch": 0.23,
      "step": 46
    },
    {
      "epoch": 0.235,
      "grad_norm": 51.378639221191406,
      "learning_rate": 2.3000000000000003e-05,
      "loss": 10.2359,
      "step": 47
    },
    {
      "bce_eff_w": 0.048,
      "bce_loss": -0.46392500400543213,
      "epoch": 0.235,
      "step": 47
    },
    {
      "epoch": 0.24,
      "grad_norm": 67.93489837646484,
      "learning_rate": 2.35e-05,
      "loss": 11.5626,
      "step": 48
    },
    {
      "bce_eff_w": 0.049,
      "bce_loss": -0.46516716480255127,
      "epoch": 0.24,
      "step": 48
    },
    {
      "epoch": 0.245,
      "grad_norm": 77.96381378173828,
      "learning_rate": 2.4e-05,
      "loss": 10.4671,
      "step": 49
    },
    {
      "bce_eff_w": 0.05,
      "bce_loss": -0.4636910557746887,
      "epoch": 0.245,
      "step": 49
    },
    {
      "epoch": 0.25,
      "grad_norm": 55.74419403076172,
      "learning_rate": 2.45e-05,
      "loss": 9.1566,
      "step": 50
    },
    {
      "bce_eff_w": 0.051000000000000004,
      "bce_loss": -0.464697003364563,
      "epoch": 0.25,
      "step": 50
    },
    {
      "epoch": 0.255,
      "grad_norm": 54.877891540527344,
      "learning_rate": 2.5e-05,
      "loss": 9.0644,
      "step": 51
    },
    {
      "bce_eff_w": 0.052000000000000005,
      "bce_loss": -0.4566374719142914,
      "epoch": 0.255,
      "step": 51
    },
    {
      "epoch": 0.26,
      "grad_norm": 52.321197509765625,
      "learning_rate": 2.5500000000000003e-05,
      "loss": 9.6476,
      "step": 52
    },
    {
      "bce_eff_w": 0.053000000000000005,
      "bce_loss": -0.4653679132461548,
      "epoch": 0.26,
      "step": 52
    },
    {
      "epoch": 0.265,
      "grad_norm": 69.16309356689453,
      "learning_rate": 2.6000000000000002e-05,
      "loss": 9.511,
      "step": 53
    },
    {
      "bce_eff_w": 0.054000000000000006,
      "bce_loss": -0.46489137411117554,
      "epoch": 0.265,
      "step": 53
    },
    {
      "epoch": 0.27,
      "grad_norm": 55.92780303955078,
      "learning_rate": 2.6500000000000004e-05,
      "loss": 9.1931,
      "step": 54
    },
    {
      "bce_eff_w": 0.05500000000000001,
      "bce_loss": -0.4645611643791199,
      "epoch": 0.27,
      "step": 54
    },
    {
      "epoch": 0.275,
      "grad_norm": 49.04477310180664,
      "learning_rate": 2.7000000000000002e-05,
      "loss": 8.9484,
      "step": 55
    },
    {
      "bce_eff_w": 0.05600000000000001,
      "bce_loss": -0.46528157591819763,
      "epoch": 0.275,
      "step": 55
    },
    {
      "epoch": 0.28,
      "grad_norm": 43.635948181152344,
      "learning_rate": 2.7500000000000004e-05,
      "loss": 7.7267,
      "step": 56
    },
    {
      "bce_eff_w": 0.056999999999999995,
      "bce_loss": -0.4646148383617401,
      "epoch": 0.28,
      "step": 56
    },
    {
      "epoch": 0.285,
      "grad_norm": 45.855125427246094,
      "learning_rate": 2.8000000000000003e-05,
      "loss": 8.772,
      "step": 57
    },
    {
      "bce_eff_w": 0.057999999999999996,
      "bce_loss": -0.46465709805488586,
      "epoch": 0.285,
      "step": 57
    },
    {
      "epoch": 0.29,
      "grad_norm": 48.71824645996094,
      "learning_rate": 2.8499999999999998e-05,
      "loss": 8.9446,
      "step": 58
    },
    {
      "bce_eff_w": 0.059,
      "bce_loss": -0.46454954147338867,
      "epoch": 0.29,
      "step": 58
    },
    {
      "epoch": 0.295,
      "grad_norm": 56.35414123535156,
      "learning_rate": 2.9e-05,
      "loss": 8.6354,
      "step": 59
    },
    {
      "bce_eff_w": 0.06,
      "bce_loss": -0.46507030725479126,
      "epoch": 0.295,
      "step": 59
    },
    {
      "epoch": 0.3,
      "grad_norm": 61.885379791259766,
      "learning_rate": 2.95e-05,
      "loss": 7.756,
      "step": 60
    },
    {
      "bce_eff_w": 0.061,
      "bce_loss": -0.46450287103652954,
      "epoch": 0.3,
      "step": 60
    },
    {
      "epoch": 0.305,
      "grad_norm": 70.33609008789062,
      "learning_rate": 3e-05,
      "loss": 9.2761,
      "step": 61
    },
    {
      "bce_eff_w": 0.062,
      "bce_loss": -0.4644707143306732,
      "epoch": 0.305,
      "step": 61
    },
    {
      "epoch": 0.31,
      "grad_norm": 50.78771209716797,
      "learning_rate": 3.05e-05,
      "loss": 9.4511,
      "step": 62
    },
    {
      "bce_eff_w": 0.063,
      "bce_loss": -0.4639449417591095,
      "epoch": 0.31,
      "step": 62
    },
    {
      "epoch": 0.315,
      "grad_norm": 36.99050521850586,
      "learning_rate": 3.1e-05,
      "loss": 8.1254,
      "step": 63
    },
    {
      "bce_eff_w": 0.064,
      "bce_loss": -0.46293339133262634,
      "epoch": 0.315,
      "step": 63
    },
    {
      "epoch": 0.32,
      "grad_norm": 45.17348098754883,
      "learning_rate": 3.15e-05,
      "loss": 8.3631,
      "step": 64
    },
    {
      "bce_eff_w": 0.065,
      "bce_loss": -0.4628065526485443,
      "epoch": 0.32,
      "step": 64
    },
    {
      "epoch": 0.325,
      "grad_norm": 38.60160446166992,
      "learning_rate": 3.2000000000000005e-05,
      "loss": 9.1988,
      "step": 65
    },
    {
      "bce_eff_w": 0.066,
      "bce_loss": -0.46536409854888916,
      "epoch": 0.325,
      "step": 65
    },
    {
      "epoch": 0.33,
      "grad_norm": 55.475196838378906,
      "learning_rate": 3.2500000000000004e-05,
      "loss": 7.2113,
      "step": 66
    },
    {
      "bce_eff_w": 0.067,
      "bce_loss": -0.4654145836830139,
      "epoch": 0.33,
      "step": 66
    },
    {
      "epoch": 0.335,
      "grad_norm": 51.06420135498047,
      "learning_rate": 3.3e-05,
      "loss": 7.8376,
      "step": 67
    },
    {
      "bce_eff_w": 0.068,
      "bce_loss": -0.4652462303638458,
      "epoch": 0.335,
      "step": 67
    },
    {
      "epoch": 0.34,
      "grad_norm": 47.43837356567383,
      "learning_rate": 3.35e-05,
      "loss": 7.1985,
      "step": 68
    },
    {
      "bce_eff_w": 0.06899999999999999,
      "bce_loss": -0.4650663733482361,
      "epoch": 0.34,
      "step": 68
    },
    {
      "epoch": 0.345,
      "grad_norm": 60.30381774902344,
      "learning_rate": 3.4000000000000007e-05,
      "loss": 7.0575,
      "step": 69
    },
    {
      "bce_eff_w": 0.06999999999999999,
      "bce_loss": -0.4634578227996826,
      "epoch": 0.345,
      "step": 69
    },
    {
      "epoch": 0.35,
      "grad_norm": 35.12934875488281,
      "learning_rate": 3.45e-05,
      "loss": 7.7633,
      "step": 70
    },
    {
      "bce_eff_w": 0.071,
      "bce_loss": -0.46530693769454956,
      "epoch": 0.35,
      "step": 70
    },
    {
      "epoch": 0.355,
      "grad_norm": 37.8218879699707,
      "learning_rate": 3.5e-05,
      "loss": 6.7034,
      "step": 71
    },
    {
      "bce_eff_w": 0.072,
      "bce_loss": -0.4647942781448364,
      "epoch": 0.355,
      "step": 71
    },
    {
      "epoch": 0.36,
      "grad_norm": 47.83405685424805,
      "learning_rate": 3.55e-05,
      "loss": 8.2298,
      "step": 72
    },
    {
      "bce_eff_w": 0.073,
      "bce_loss": -0.4654041826725006,
      "epoch": 0.36,
      "step": 72
    },
    {
      "epoch": 0.365,
      "grad_norm": 36.304893493652344,
      "learning_rate": 3.6e-05,
      "loss": 6.6585,
      "step": 73
    },
    {
      "bce_eff_w": 0.074,
      "bce_loss": -0.4651496410369873,
      "epoch": 0.365,
      "step": 73
    },
    {
      "epoch": 0.37,
      "grad_norm": 29.828407287597656,
      "learning_rate": 3.65e-05,
      "loss": 7.5218,
      "step": 74
    },
    {
      "bce_eff_w": 0.07500000000000001,
      "bce_loss": -0.46520867943763733,
      "epoch": 0.37,
      "step": 74
    },
    {
      "epoch": 0.375,
      "grad_norm": 50.624053955078125,
      "learning_rate": 3.7e-05,
      "loss": 7.695,
      "step": 75
    },
    {
      "bce_eff_w": 0.07600000000000001,
      "bce_loss": -0.46449705958366394,
      "epoch": 0.375,
      "step": 75
    },
    {
      "epoch": 0.38,
      "grad_norm": 27.447288513183594,
      "learning_rate": 3.7500000000000003e-05,
      "loss": 7.0293,
      "step": 76
    },
    {
      "bce_eff_w": 0.07700000000000001,
      "bce_loss": -0.4654144048690796,
      "epoch": 0.38,
      "step": 76
    },
    {
      "epoch": 0.385,
      "grad_norm": 35.95426940917969,
      "learning_rate": 3.8e-05,
      "loss": 7.5175,
      "step": 77
    },
    {
      "bce_eff_w": 0.07800000000000001,
      "bce_loss": -0.46535220742225647,
      "epoch": 0.385,
      "step": 77
    },
    {
      "epoch": 0.39,
      "grad_norm": 42.05027770996094,
      "learning_rate": 3.85e-05,
      "loss": 6.9445,
      "step": 78
    },
    {
      "bce_eff_w": 0.07900000000000001,
      "bce_loss": -0.4651479125022888,
      "epoch": 0.39,
      "step": 78
    },
    {
      "epoch": 0.395,
      "grad_norm": 25.042736053466797,
      "learning_rate": 3.9000000000000006e-05,
      "loss": 6.8957,
      "step": 79
    },
    {
      "bce_eff_w": 0.08000000000000002,
      "bce_loss": -0.4653324484825134,
      "epoch": 0.395,
      "step": 79
    },
    {
      "epoch": 0.4,
      "grad_norm": 33.85810089111328,
      "learning_rate": 3.9500000000000005e-05,
      "loss": 6.0275,
      "step": 80
    },
    {
      "bce_eff_w": 0.08100000000000002,
      "bce_loss": -0.46461957693099976,
      "epoch": 0.4,
      "step": 80
    },
    {
      "epoch": 0.405,
      "grad_norm": 29.217506408691406,
      "learning_rate": 4e-05,
      "loss": 6.9704,
      "step": 81
    },
    {
      "bce_eff_w": 0.082,
      "bce_loss": -0.4644656479358673,
      "epoch": 0.405,
      "step": 81
    },
    {
      "epoch": 0.41,
      "grad_norm": 33.76744842529297,
      "learning_rate": 4.05e-05,
      "loss": 7.4049,
      "step": 82
    },
    {
      "bce_eff_w": 0.083,
      "bce_loss": -0.46388813853263855,
      "epoch": 0.41,
      "step": 82
    },
    {
      "epoch": 0.415,
      "grad_norm": 46.36192321777344,
      "learning_rate": 4.1e-05,
      "loss": 7.8569,
      "step": 83
    },
    {
      "bce_eff_w": 0.084,
      "bce_loss": -0.4648004472255707,
      "epoch": 0.415,
      "step": 83
    },
    {
      "epoch": 0.42,
      "grad_norm": 28.30721092224121,
      "learning_rate": 4.15e-05,
      "loss": 7.714,
      "step": 84
    },
    {
      "bce_eff_w": 0.085,
      "bce_loss": -0.46544307470321655,
      "epoch": 0.42,
      "step": 84
    },
    {
      "epoch": 0.425,
      "grad_norm": 26.806928634643555,
      "learning_rate": 4.2e-05,
      "loss": 5.5955,
      "step": 85
    },
    {
      "bce_eff_w": 0.08600000000000001,
      "bce_loss": -0.46513012051582336,
      "epoch": 0.425,
      "step": 85
    },
    {
      "epoch": 0.43,
      "grad_norm": 35.44376754760742,
      "learning_rate": 4.25e-05,
      "loss": 8.5204,
      "step": 86
    },
    {
      "bce_eff_w": 0.08700000000000001,
      "bce_loss": -0.4653310775756836,
      "epoch": 0.43,
      "step": 86
    },
    {
      "epoch": 0.435,
      "grad_norm": 35.75395584106445,
      "learning_rate": 4.3e-05,
      "loss": 5.841,
      "step": 87
    },
    {
      "bce_eff_w": 0.08800000000000001,
      "bce_loss": -0.4650801718235016,
      "epoch": 0.435,
      "step": 87
    },
    {
      "epoch": 0.44,
      "grad_norm": 30.042537689208984,
      "learning_rate": 4.35e-05,
      "loss": 7.2162,
      "step": 88
    },
    {
      "bce_eff_w": 0.08900000000000001,
      "bce_loss": -0.46536293625831604,
      "epoch": 0.44,
      "step": 88
    },
    {
      "epoch": 0.445,
      "grad_norm": 38.24144744873047,
      "learning_rate": 4.4000000000000006e-05,
      "loss": 6.157,
      "step": 89
    },
    {
      "bce_eff_w": 0.09000000000000001,
      "bce_loss": -0.4653686583042145,
      "epoch": 0.445,
      "step": 89
    },
    {
      "epoch": 0.45,
      "grad_norm": 24.191007614135742,
      "learning_rate": 4.4500000000000004e-05,
      "loss": 6.3941,
      "step": 90
    },
    {
      "bce_eff_w": 0.09100000000000001,
      "bce_loss": -0.46525514125823975,
      "epoch": 0.45,
      "step": 90
    },
    {
      "epoch": 0.455,
      "grad_norm": 29.804889678955078,
      "learning_rate": 4.5e-05,
      "loss": 6.4391,
      "step": 91
    },
    {
      "bce_eff_w": 0.09200000000000001,
      "bce_loss": -0.4648509621620178,
      "epoch": 0.455,
      "step": 91
    },
    {
      "epoch": 0.46,
      "grad_norm": 28.295347213745117,
      "learning_rate": 4.55e-05,
      "loss": 6.8257,
      "step": 92
    },
    {
      "bce_eff_w": 0.09300000000000001,
      "bce_loss": -0.46477019786834717,
      "epoch": 0.46,
      "step": 92
    },
    {
      "epoch": 0.465,
      "grad_norm": 20.946346282958984,
      "learning_rate": 4.600000000000001e-05,
      "loss": 7.4958,
      "step": 93
    },
    {
      "bce_eff_w": 0.094,
      "bce_loss": -0.4649941325187683,
      "epoch": 0.465,
      "step": 93
    },
    {
      "epoch": 0.47,
      "grad_norm": 25.049283981323242,
      "learning_rate": 4.6500000000000005e-05,
      "loss": 6.3141,
      "step": 94
    },
    {
      "bce_eff_w": 0.095,
      "bce_loss": -0.46483832597732544,
      "epoch": 0.47,
      "step": 94
    },
    {
      "epoch": 0.475,
      "grad_norm": 24.783510208129883,
      "learning_rate": 4.7e-05,
      "loss": 6.5741,
      "step": 95
    },
    {
      "bce_eff_w": 0.096,
      "bce_loss": -0.465366929769516,
      "epoch": 0.475,
      "step": 95
    },
    {
      "epoch": 0.48,
      "grad_norm": 24.337745666503906,
      "learning_rate": 4.75e-05,
      "loss": 5.3924,
      "step": 96
    },
    {
      "bce_eff_w": 0.097,
      "bce_loss": -0.46531906723976135,
      "epoch": 0.48,
      "step": 96
    },
    {
      "epoch": 0.485,
      "grad_norm": 18.799118041992188,
      "learning_rate": 4.8e-05,
      "loss": 6.0324,
      "step": 97
    },
    {
      "bce_eff_w": 0.098,
      "bce_loss": -0.46547946333885193,
      "epoch": 0.485,
      "step": 97
    },
    {
      "epoch": 0.49,
      "grad_norm": 27.329959869384766,
      "learning_rate": 4.85e-05,
      "loss": 6.5217,
      "step": 98
    },
    {
      "bce_eff_w": 0.099,
      "bce_loss": -0.4650663137435913,
      "epoch": 0.49,
      "step": 98
    },
    {
      "epoch": 0.495,
      "grad_norm": 19.53307342529297,
      "learning_rate": 4.9e-05,
      "loss": 6.1448,
      "step": 99
    },
    {
      "bce_eff_w": 0.1,
      "bce_loss": -0.4654577076435089,
      "epoch": 0.495,
      "step": 99
    },
    {
      "epoch": 0.5,
      "grad_norm": 23.43376350402832,
      "learning_rate": 4.9500000000000004e-05,
      "loss": 4.2605,
      "step": 100
    }
  ],
  "logging_steps": 1,
  "max_steps": 100,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 1,
  "save_steps": 1000,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 6493483980.0,
  "train_batch_size": 1,
  "trial_name": null,
  "trial_params": null
}
