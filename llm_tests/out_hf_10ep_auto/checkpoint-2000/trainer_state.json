{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 10.0,
  "eval_steps": 500,
  "global_step": 2000,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "bce_eff_w": 0.0005,
      "bce_loss": -0.4642454981803894,
      "epoch": 0,
      "step": 0
    },
    {
      "epoch": 0.005,
      "grad_norm": 85.84219360351562,
      "learning_rate": 0.0,
      "loss": 10.7678,
      "step": 1
    },
    {
      "bce_eff_w": 0.001,
      "bce_loss": -0.4629226326942444,
      "epoch": 0.005,
      "step": 1
    },
    {
      "epoch": 0.01,
      "grad_norm": 104.53201293945312,
      "learning_rate": 1.2500000000000002e-07,
      "loss": 11.7058,
      "step": 2
    },
    {
      "bce_eff_w": 0.0015,
      "bce_loss": -0.4621935486793518,
      "epoch": 0.01,
      "step": 2
    },
    {
      "epoch": 0.015,
      "grad_norm": 82.55461120605469,
      "learning_rate": 2.5000000000000004e-07,
      "loss": 11.7044,
      "step": 3
    },
    {
      "bce_eff_w": 0.002,
      "bce_loss": -0.463157057762146,
      "epoch": 0.015,
      "step": 3
    },
    {
      "epoch": 0.02,
      "grad_norm": 80.74925994873047,
      "learning_rate": 3.75e-07,
      "loss": 12.2697,
      "step": 4
    },
    {
      "bce_eff_w": 0.0025000000000000005,
      "bce_loss": -0.46157756447792053,
      "epoch": 0.02,
      "step": 4
    },
    {
      "epoch": 0.025,
      "grad_norm": 86.90525817871094,
      "learning_rate": 5.000000000000001e-07,
      "loss": 11.7551,
      "step": 5
    },
    {
      "bce_eff_w": 0.003,
      "bce_loss": -0.45471861958503723,
      "epoch": 0.025,
      "step": 5
    },
    {
      "epoch": 0.03,
      "grad_norm": 73.8931884765625,
      "learning_rate": 6.25e-07,
      "loss": 13.1857,
      "step": 6
    },
    {
      "bce_eff_w": 0.0035000000000000005,
      "bce_loss": -0.46238699555397034,
      "epoch": 0.03,
      "step": 6
    },
    {
      "epoch": 0.035,
      "grad_norm": 80.74256896972656,
      "learning_rate": 7.5e-07,
      "loss": 12.1384,
      "step": 7
    },
    {
      "bce_eff_w": 0.004,
      "bce_loss": -0.460891455411911,
      "epoch": 0.035,
      "step": 7
    },
    {
      "epoch": 0.04,
      "grad_norm": 75.72113800048828,
      "learning_rate": 8.750000000000001e-07,
      "loss": 12.9223,
      "step": 8
    },
    {
      "bce_eff_w": 0.0045,
      "bce_loss": -0.45333927869796753,
      "epoch": 0.04,
      "step": 8
    },
    {
      "epoch": 0.045,
      "grad_norm": 66.41242980957031,
      "learning_rate": 1.0000000000000002e-06,
      "loss": 12.857,
      "step": 9
    },
    {
      "bce_eff_w": 0.005000000000000001,
      "bce_loss": -0.4567180871963501,
      "epoch": 0.045,
      "step": 9
    },
    {
      "epoch": 0.05,
      "grad_norm": 74.34892272949219,
      "learning_rate": 1.125e-06,
      "loss": 12.2378,
      "step": 10
    },
    {
      "bce_eff_w": 0.0055000000000000005,
      "bce_loss": -0.46356797218322754,
      "epoch": 0.05,
      "step": 10
    },
    {
      "epoch": 0.055,
      "grad_norm": 75.91450500488281,
      "learning_rate": 1.25e-06,
      "loss": 11.9393,
      "step": 11
    },
    {
      "bce_eff_w": 0.006,
      "bce_loss": -0.46294161677360535,
      "epoch": 0.055,
      "step": 11
    },
    {
      "epoch": 0.06,
      "grad_norm": 73.05245971679688,
      "learning_rate": 1.3750000000000002e-06,
      "loss": 11.5958,
      "step": 12
    },
    {
      "bce_eff_w": 0.006500000000000001,
      "bce_loss": -0.46399569511413574,
      "epoch": 0.06,
      "step": 12
    },
    {
      "epoch": 0.065,
      "grad_norm": 85.04893493652344,
      "learning_rate": 1.5e-06,
      "loss": 11.6997,
      "step": 13
    },
    {
      "bce_eff_w": 0.007000000000000001,
      "bce_loss": -0.46276721358299255,
      "epoch": 0.065,
      "step": 13
    },
    {
      "epoch": 0.07,
      "grad_norm": 79.7200698852539,
      "learning_rate": 1.6250000000000001e-06,
      "loss": 12.5554,
      "step": 14
    },
    {
      "bce_eff_w": 0.0075,
      "bce_loss": -0.4644526541233063,
      "epoch": 0.07,
      "step": 14
    },
    {
      "epoch": 0.075,
      "grad_norm": 87.14427947998047,
      "learning_rate": 1.7500000000000002e-06,
      "loss": 12.6004,
      "step": 15
    },
    {
      "bce_eff_w": 0.008,
      "bce_loss": -0.4636283218860626,
      "epoch": 0.075,
      "step": 15
    },
    {
      "epoch": 0.08,
      "grad_norm": 87.37216186523438,
      "learning_rate": 1.875e-06,
      "loss": 12.1047,
      "step": 16
    },
    {
      "bce_eff_w": 0.0085,
      "bce_loss": -0.4570363759994507,
      "epoch": 0.08,
      "step": 16
    },
    {
      "epoch": 0.085,
      "grad_norm": 68.55889129638672,
      "learning_rate": 2.0000000000000003e-06,
      "loss": 11.3187,
      "step": 17
    },
    {
      "bce_eff_w": 0.009,
      "bce_loss": -0.46178925037384033,
      "epoch": 0.085,
      "step": 17
    },
    {
      "epoch": 0.09,
      "grad_norm": 94.26811218261719,
      "learning_rate": 2.1250000000000004e-06,
      "loss": 12.5144,
      "step": 18
    },
    {
      "bce_eff_w": 0.009500000000000001,
      "bce_loss": -0.46254870295524597,
      "epoch": 0.09,
      "step": 18
    },
    {
      "epoch": 0.095,
      "grad_norm": 64.98574829101562,
      "learning_rate": 2.25e-06,
      "loss": 11.685,
      "step": 19
    },
    {
      "bce_eff_w": 0.010000000000000002,
      "bce_loss": -0.45992615818977356,
      "epoch": 0.095,
      "step": 19
    },
    {
      "epoch": 0.1,
      "grad_norm": 73.39107513427734,
      "learning_rate": 2.375e-06,
      "loss": 11.4568,
      "step": 20
    },
    {
      "bce_eff_w": 0.0105,
      "bce_loss": -0.45521360635757446,
      "epoch": 0.1,
      "step": 20
    },
    {
      "epoch": 0.105,
      "grad_norm": 80.08700561523438,
      "learning_rate": 2.5e-06,
      "loss": 12.0808,
      "step": 21
    },
    {
      "bce_eff_w": 0.011000000000000001,
      "bce_loss": -0.45598798990249634,
      "epoch": 0.105,
      "step": 21
    },
    {
      "epoch": 0.11,
      "grad_norm": 73.51981353759766,
      "learning_rate": 2.625e-06,
      "loss": 11.4,
      "step": 22
    },
    {
      "bce_eff_w": 0.011500000000000002,
      "bce_loss": -0.4621700048446655,
      "epoch": 0.11,
      "step": 22
    },
    {
      "epoch": 0.115,
      "grad_norm": 73.35059356689453,
      "learning_rate": 2.7500000000000004e-06,
      "loss": 13.2684,
      "step": 23
    },
    {
      "bce_eff_w": 0.012,
      "bce_loss": -0.46110662817955017,
      "epoch": 0.115,
      "step": 23
    },
    {
      "epoch": 0.12,
      "grad_norm": 73.85403442382812,
      "learning_rate": 2.8750000000000004e-06,
      "loss": 13.0353,
      "step": 24
    },
    {
      "bce_eff_w": 0.0125,
      "bce_loss": -0.4539836049079895,
      "epoch": 0.12,
      "step": 24
    },
    {
      "epoch": 0.125,
      "grad_norm": 84.20223999023438,
      "learning_rate": 3e-06,
      "loss": 11.1823,
      "step": 25
    },
    {
      "bce_eff_w": 0.013000000000000001,
      "bce_loss": -0.45934006571769714,
      "epoch": 0.125,
      "step": 25
    },
    {
      "epoch": 0.13,
      "grad_norm": 64.91201782226562,
      "learning_rate": 3.125e-06,
      "loss": 13.1254,
      "step": 26
    },
    {
      "bce_eff_w": 0.013500000000000002,
      "bce_loss": -0.46402522921562195,
      "epoch": 0.13,
      "step": 26
    },
    {
      "epoch": 0.135,
      "grad_norm": 96.98448181152344,
      "learning_rate": 3.2500000000000002e-06,
      "loss": 11.7448,
      "step": 27
    },
    {
      "bce_eff_w": 0.014000000000000002,
      "bce_loss": -0.4643944203853607,
      "epoch": 0.135,
      "step": 27
    },
    {
      "epoch": 0.14,
      "grad_norm": 81.29939270019531,
      "learning_rate": 3.3750000000000003e-06,
      "loss": 11.3432,
      "step": 28
    },
    {
      "bce_eff_w": 0.014499999999999999,
      "bce_loss": -0.46324196457862854,
      "epoch": 0.14,
      "step": 28
    },
    {
      "epoch": 0.145,
      "grad_norm": 96.53822326660156,
      "learning_rate": 3.5000000000000004e-06,
      "loss": 11.3771,
      "step": 29
    },
    {
      "bce_eff_w": 0.015,
      "bce_loss": -0.46324875950813293,
      "epoch": 0.145,
      "step": 29
    },
    {
      "epoch": 0.15,
      "grad_norm": 105.13235473632812,
      "learning_rate": 3.625e-06,
      "loss": 10.2995,
      "step": 30
    },
    {
      "bce_eff_w": 0.0155,
      "bce_loss": -0.45219600200653076,
      "epoch": 0.15,
      "step": 30
    },
    {
      "epoch": 0.155,
      "grad_norm": 67.23070526123047,
      "learning_rate": 3.75e-06,
      "loss": 12.0051,
      "step": 31
    },
    {
      "bce_eff_w": 0.016,
      "bce_loss": -0.4621044099330902,
      "epoch": 0.155,
      "step": 31
    },
    {
      "epoch": 0.16,
      "grad_norm": 65.5751953125,
      "learning_rate": 3.875e-06,
      "loss": 12.6193,
      "step": 32
    },
    {
      "bce_eff_w": 0.0165,
      "bce_loss": -0.46381300687789917,
      "epoch": 0.16,
      "step": 32
    },
    {
      "epoch": 0.165,
      "grad_norm": 78.67259216308594,
      "learning_rate": 4.000000000000001e-06,
      "loss": 13.8249,
      "step": 33
    },
    {
      "bce_eff_w": 0.017,
      "bce_loss": -0.4629085958003998,
      "epoch": 0.165,
      "step": 33
    },
    {
      "epoch": 0.17,
      "grad_norm": 66.6431655883789,
      "learning_rate": 4.125e-06,
      "loss": 13.7413,
      "step": 34
    },
    {
      "bce_eff_w": 0.017499999999999998,
      "bce_loss": -0.4622683525085449,
      "epoch": 0.17,
      "step": 34
    },
    {
      "epoch": 0.175,
      "grad_norm": 84.42593383789062,
      "learning_rate": 4.250000000000001e-06,
      "loss": 13.0313,
      "step": 35
    },
    {
      "bce_eff_w": 0.018,
      "bce_loss": -0.46480879187583923,
      "epoch": 0.175,
      "step": 35
    },
    {
      "epoch": 0.18,
      "grad_norm": 75.4310531616211,
      "learning_rate": 4.375e-06,
      "loss": 11.4915,
      "step": 36
    },
    {
      "bce_eff_w": 0.0185,
      "bce_loss": -0.4648837745189667,
      "epoch": 0.18,
      "step": 36
    },
    {
      "epoch": 0.185,
      "grad_norm": 77.5410385131836,
      "learning_rate": 4.5e-06,
      "loss": 11.1991,
      "step": 37
    },
    {
      "bce_eff_w": 0.019000000000000003,
      "bce_loss": -0.4623277485370636,
      "epoch": 0.185,
      "step": 37
    },
    {
      "epoch": 0.19,
      "grad_norm": 80.47073364257812,
      "learning_rate": 4.625e-06,
      "loss": 12.144,
      "step": 38
    },
    {
      "bce_eff_w": 0.019500000000000003,
      "bce_loss": -0.4645787179470062,
      "epoch": 0.19,
      "step": 38
    },
    {
      "epoch": 0.195,
      "grad_norm": 63.399253845214844,
      "learning_rate": 4.75e-06,
      "loss": 11.3362,
      "step": 39
    },
    {
      "bce_eff_w": 0.020000000000000004,
      "bce_loss": -0.46148207783699036,
      "epoch": 0.195,
      "step": 39
    },
    {
      "epoch": 0.2,
      "grad_norm": 87.61561584472656,
      "learning_rate": 4.875000000000001e-06,
      "loss": 11.5823,
      "step": 40
    },
    {
      "bce_eff_w": 0.0205,
      "bce_loss": -0.4619758725166321,
      "epoch": 0.2,
      "step": 40
    },
    {
      "epoch": 0.205,
      "grad_norm": 69.1026382446289,
      "learning_rate": 5e-06,
      "loss": 10.777,
      "step": 41
    },
    {
      "bce_eff_w": 0.021,
      "bce_loss": -0.4568440318107605,
      "epoch": 0.205,
      "step": 41
    },
    {
      "epoch": 0.21,
      "grad_norm": 74.10343933105469,
      "learning_rate": 5.125e-06,
      "loss": 13.1653,
      "step": 42
    },
    {
      "bce_eff_w": 0.021500000000000002,
      "bce_loss": -0.4645569920539856,
      "epoch": 0.21,
      "step": 42
    },
    {
      "epoch": 0.215,
      "grad_norm": 95.73321533203125,
      "learning_rate": 5.25e-06,
      "loss": 12.223,
      "step": 43
    },
    {
      "bce_eff_w": 0.022000000000000002,
      "bce_loss": -0.45969656109809875,
      "epoch": 0.215,
      "step": 43
    },
    {
      "epoch": 0.22,
      "grad_norm": 69.20557403564453,
      "learning_rate": 5.375e-06,
      "loss": 11.6155,
      "step": 44
    },
    {
      "bce_eff_w": 0.022500000000000003,
      "bce_loss": -0.46334436535835266,
      "epoch": 0.22,
      "step": 44
    },
    {
      "epoch": 0.225,
      "grad_norm": 70.05427551269531,
      "learning_rate": 5.500000000000001e-06,
      "loss": 12.0253,
      "step": 45
    },
    {
      "bce_eff_w": 0.023000000000000003,
      "bce_loss": -0.46003618836402893,
      "epoch": 0.225,
      "step": 45
    },
    {
      "epoch": 0.23,
      "grad_norm": 61.17974853515625,
      "learning_rate": 5.625e-06,
      "loss": 10.9824,
      "step": 46
    },
    {
      "bce_eff_w": 0.0235,
      "bce_loss": -0.46088358759880066,
      "epoch": 0.23,
      "step": 46
    },
    {
      "epoch": 0.235,
      "grad_norm": 56.614437103271484,
      "learning_rate": 5.750000000000001e-06,
      "loss": 11.6955,
      "step": 47
    },
    {
      "bce_eff_w": 0.024,
      "bce_loss": -0.4623553156852722,
      "epoch": 0.235,
      "step": 47
    },
    {
      "epoch": 0.24,
      "grad_norm": 78.15635681152344,
      "learning_rate": 5.875e-06,
      "loss": 14.1668,
      "step": 48
    },
    {
      "bce_eff_w": 0.0245,
      "bce_loss": -0.4647798538208008,
      "epoch": 0.24,
      "step": 48
    },
    {
      "epoch": 0.245,
      "grad_norm": 93.78659057617188,
      "learning_rate": 6e-06,
      "loss": 13.2251,
      "step": 49
    },
    {
      "bce_eff_w": 0.025,
      "bce_loss": -0.46270468831062317,
      "epoch": 0.245,
      "step": 49
    },
    {
      "epoch": 0.25,
      "grad_norm": 69.47052764892578,
      "learning_rate": 6.125e-06,
      "loss": 10.9306,
      "step": 50
    },
    {
      "bce_eff_w": 0.025500000000000002,
      "bce_loss": -0.46391311287879944,
      "epoch": 0.25,
      "step": 50
    },
    {
      "epoch": 0.255,
      "grad_norm": 68.94725036621094,
      "learning_rate": 6.25e-06,
      "loss": 11.2628,
      "step": 51
    },
    {
      "bce_eff_w": 0.026000000000000002,
      "bce_loss": -0.4522249102592468,
      "epoch": 0.255,
      "step": 51
    },
    {
      "epoch": 0.26,
      "grad_norm": 67.34236145019531,
      "learning_rate": 6.375000000000001e-06,
      "loss": 11.5813,
      "step": 52
    },
    {
      "bce_eff_w": 0.026500000000000003,
      "bce_loss": -0.465262770652771,
      "epoch": 0.26,
      "step": 52
    },
    {
      "epoch": 0.265,
      "grad_norm": 90.82621765136719,
      "learning_rate": 6.5000000000000004e-06,
      "loss": 12.299,
      "step": 53
    },
    {
      "bce_eff_w": 0.027000000000000003,
      "bce_loss": -0.46453914046287537,
      "epoch": 0.265,
      "step": 53
    },
    {
      "epoch": 0.27,
      "grad_norm": 70.37382507324219,
      "learning_rate": 6.625000000000001e-06,
      "loss": 11.089,
      "step": 54
    },
    {
      "bce_eff_w": 0.027500000000000004,
      "bce_loss": -0.46351733803749084,
      "epoch": 0.27,
      "step": 54
    },
    {
      "epoch": 0.275,
      "grad_norm": 67.73320770263672,
      "learning_rate": 6.750000000000001e-06,
      "loss": 10.9277,
      "step": 55
    },
    {
      "bce_eff_w": 0.028000000000000004,
      "bce_loss": -0.4649254083633423,
      "epoch": 0.275,
      "step": 55
    },
    {
      "epoch": 0.28,
      "grad_norm": 71.37574768066406,
      "learning_rate": 6.875000000000001e-06,
      "loss": 10.0482,
      "step": 56
    },
    {
      "bce_eff_w": 0.028499999999999998,
      "bce_loss": -0.46406930685043335,
      "epoch": 0.28,
      "step": 56
    },
    {
      "epoch": 0.285,
      "grad_norm": 60.938018798828125,
      "learning_rate": 7.000000000000001e-06,
      "loss": 10.4935,
      "step": 57
    },
    {
      "bce_eff_w": 0.028999999999999998,
      "bce_loss": -0.4638941287994385,
      "epoch": 0.285,
      "step": 57
    },
    {
      "epoch": 0.29,
      "grad_norm": 64.05599975585938,
      "learning_rate": 7.1249999999999995e-06,
      "loss": 10.7486,
      "step": 58
    },
    {
      "bce_eff_w": 0.0295,
      "bce_loss": -0.4630584418773651,
      "epoch": 0.29,
      "step": 58
    },
    {
      "epoch": 0.295,
      "grad_norm": 80.4647445678711,
      "learning_rate": 7.25e-06,
      "loss": 11.2576,
      "step": 59
    },
    {
      "bce_eff_w": 0.03,
      "bce_loss": -0.46429672837257385,
      "epoch": 0.295,
      "step": 59
    },
    {
      "epoch": 0.3,
      "grad_norm": 83.30785369873047,
      "learning_rate": 7.375e-06,
      "loss": 10.8698,
      "step": 60
    },
    {
      "bce_eff_w": 0.0305,
      "bce_loss": -0.4622052311897278,
      "epoch": 0.3,
      "step": 60
    },
    {
      "epoch": 0.305,
      "grad_norm": 89.71240997314453,
      "learning_rate": 7.5e-06,
      "loss": 12.9046,
      "step": 61
    },
    {
      "bce_eff_w": 0.031,
      "bce_loss": -0.4630052149295807,
      "epoch": 0.305,
      "step": 61
    },
    {
      "epoch": 0.31,
      "grad_norm": 66.94534301757812,
      "learning_rate": 7.625e-06,
      "loss": 12.5378,
      "step": 62
    },
    {
      "bce_eff_w": 0.0315,
      "bce_loss": -0.46135520935058594,
      "epoch": 0.31,
      "step": 62
    },
    {
      "epoch": 0.315,
      "grad_norm": 70.3182144165039,
      "learning_rate": 7.75e-06,
      "loss": 10.9139,
      "step": 63
    },
    {
      "bce_eff_w": 0.032,
      "bce_loss": -0.4591519236564636,
      "epoch": 0.315,
      "step": 63
    },
    {
      "epoch": 0.32,
      "grad_norm": 73.53337097167969,
      "learning_rate": 7.875e-06,
      "loss": 11.3134,
      "step": 64
    },
    {
      "bce_eff_w": 0.0325,
      "bce_loss": -0.45977678894996643,
      "epoch": 0.32,
      "step": 64
    },
    {
      "epoch": 0.325,
      "grad_norm": 61.42216110229492,
      "learning_rate": 8.000000000000001e-06,
      "loss": 11.8883,
      "step": 65
    },
    {
      "bce_eff_w": 0.033,
      "bce_loss": -0.46502190828323364,
      "epoch": 0.325,
      "step": 65
    },
    {
      "epoch": 0.33,
      "grad_norm": 80.43852996826172,
      "learning_rate": 8.125000000000001e-06,
      "loss": 11.1462,
      "step": 66
    },
    {
      "bce_eff_w": 0.0335,
      "bce_loss": -0.46498170495033264,
      "epoch": 0.33,
      "step": 66
    },
    {
      "epoch": 0.335,
      "grad_norm": 71.64945220947266,
      "learning_rate": 8.25e-06,
      "loss": 11.5157,
      "step": 67
    },
    {
      "bce_eff_w": 0.034,
      "bce_loss": -0.4646504521369934,
      "epoch": 0.335,
      "step": 67
    },
    {
      "epoch": 0.34,
      "grad_norm": 87.28618621826172,
      "learning_rate": 8.375e-06,
      "loss": 10.757,
      "step": 68
    },
    {
      "bce_eff_w": 0.034499999999999996,
      "bce_loss": -0.46427279710769653,
      "epoch": 0.34,
      "step": 68
    },
    {
      "epoch": 0.345,
      "grad_norm": 87.90367889404297,
      "learning_rate": 8.500000000000002e-06,
      "loss": 11.296,
      "step": 69
    },
    {
      "bce_eff_w": 0.034999999999999996,
      "bce_loss": -0.46231311559677124,
      "epoch": 0.345,
      "step": 69
    },
    {
      "epoch": 0.35,
      "grad_norm": 54.00718688964844,
      "learning_rate": 8.625e-06,
      "loss": 9.752,
      "step": 70
    },
    {
      "bce_eff_w": 0.0355,
      "bce_loss": -0.46510353684425354,
      "epoch": 0.35,
      "step": 70
    },
    {
      "epoch": 0.355,
      "grad_norm": 68.23587799072266,
      "learning_rate": 8.75e-06,
      "loss": 9.2741,
      "step": 71
    },
    {
      "bce_eff_w": 0.036,
      "bce_loss": -0.4636786878108978,
      "epoch": 0.355,
      "step": 71
    },
    {
      "epoch": 0.36,
      "grad_norm": 65.70463562011719,
      "learning_rate": 8.875e-06,
      "loss": 12.2541,
      "step": 72
    },
    {
      "bce_eff_w": 0.0365,
      "bce_loss": -0.4649604856967926,
      "epoch": 0.36,
      "step": 72
    },
    {
      "epoch": 0.365,
      "grad_norm": 73.22380065917969,
      "learning_rate": 9e-06,
      "loss": 10.5196,
      "step": 73
    },
    {
      "bce_eff_w": 0.037,
      "bce_loss": -0.4639410972595215,
      "epoch": 0.365,
      "step": 73
    },
    {
      "epoch": 0.37,
      "grad_norm": 60.187767028808594,
      "learning_rate": 9.125e-06,
      "loss": 10.5235,
      "step": 74
    },
    {
      "bce_eff_w": 0.037500000000000006,
      "bce_loss": -0.4641088843345642,
      "epoch": 0.37,
      "step": 74
    },
    {
      "epoch": 0.375,
      "grad_norm": 74.3308334350586,
      "learning_rate": 9.25e-06,
      "loss": 12.1081,
      "step": 75
    },
    {
      "bce_eff_w": 0.038000000000000006,
      "bce_loss": -0.4619172215461731,
      "epoch": 0.375,
      "step": 75
    },
    {
      "epoch": 0.38,
      "grad_norm": 58.08497619628906,
      "learning_rate": 9.375000000000001e-06,
      "loss": 9.9945,
      "step": 76
    },
    {
      "bce_eff_w": 0.038500000000000006,
      "bce_loss": -0.46481677889823914,
      "epoch": 0.38,
      "step": 76
    },
    {
      "epoch": 0.385,
      "grad_norm": 65.18840789794922,
      "learning_rate": 9.5e-06,
      "loss": 10.5803,
      "step": 77
    },
    {
      "bce_eff_w": 0.03900000000000001,
      "bce_loss": -0.4646616578102112,
      "epoch": 0.385,
      "step": 77
    },
    {
      "epoch": 0.39,
      "grad_norm": 67.98514556884766,
      "learning_rate": 9.625e-06,
      "loss": 11.1422,
      "step": 78
    },
    {
      "bce_eff_w": 0.03950000000000001,
      "bce_loss": -0.4638308584690094,
      "epoch": 0.39,
      "step": 78
    },
    {
      "epoch": 0.395,
      "grad_norm": 57.4843864440918,
      "learning_rate": 9.750000000000002e-06,
      "loss": 9.8779,
      "step": 79
    },
    {
      "bce_eff_w": 0.04000000000000001,
      "bce_loss": -0.46479862928390503,
      "epoch": 0.395,
      "step": 79
    },
    {
      "epoch": 0.4,
      "grad_norm": 66.67782592773438,
      "learning_rate": 9.875000000000001e-06,
      "loss": 8.6035,
      "step": 80
    },
    {
      "bce_eff_w": 0.04050000000000001,
      "bce_loss": -0.4613906443119049,
      "epoch": 0.4,
      "step": 80
    },
    {
      "epoch": 0.405,
      "grad_norm": 62.86677551269531,
      "learning_rate": 1e-05,
      "loss": 9.8971,
      "step": 81
    },
    {
      "bce_eff_w": 0.041,
      "bce_loss": -0.46197935938835144,
      "epoch": 0.405,
      "step": 81
    },
    {
      "epoch": 0.41,
      "grad_norm": 68.00482940673828,
      "learning_rate": 1.0125e-05,
      "loss": 10.7214,
      "step": 82
    },
    {
      "bce_eff_w": 0.0415,
      "bce_loss": -0.4602997601032257,
      "epoch": 0.41,
      "step": 82
    },
    {
      "epoch": 0.415,
      "grad_norm": 56.63899230957031,
      "learning_rate": 1.025e-05,
      "loss": 10.4389,
      "step": 83
    },
    {
      "bce_eff_w": 0.042,
      "bce_loss": -0.4624604880809784,
      "epoch": 0.415,
      "step": 83
    },
    {
      "epoch": 0.42,
      "grad_norm": 54.78959274291992,
      "learning_rate": 1.0375e-05,
      "loss": 11.0724,
      "step": 84
    },
    {
      "bce_eff_w": 0.0425,
      "bce_loss": -0.4646940231323242,
      "epoch": 0.42,
      "step": 84
    },
    {
      "epoch": 0.425,
      "grad_norm": 57.02690124511719,
      "learning_rate": 1.05e-05,
      "loss": 9.0107,
      "step": 85
    },
    {
      "bce_eff_w": 0.043000000000000003,
      "bce_loss": -0.4632320702075958,
      "epoch": 0.425,
      "step": 85
    },
    {
      "epoch": 0.43,
      "grad_norm": 73.80619812011719,
      "learning_rate": 1.0625e-05,
      "loss": 12.8853,
      "step": 86
    },
    {
      "bce_eff_w": 0.043500000000000004,
      "bce_loss": -0.4643074572086334,
      "epoch": 0.43,
      "step": 86
    },
    {
      "epoch": 0.435,
      "grad_norm": 61.38444519042969,
      "learning_rate": 1.075e-05,
      "loss": 9.5765,
      "step": 87
    },
    {
      "bce_eff_w": 0.044000000000000004,
      "bce_loss": -0.4630521535873413,
      "epoch": 0.435,
      "step": 87
    },
    {
      "epoch": 0.44,
      "grad_norm": 52.41750717163086,
      "learning_rate": 1.0875e-05,
      "loss": 11.1904,
      "step": 88
    },
    {
      "bce_eff_w": 0.044500000000000005,
      "bce_loss": -0.4641951322555542,
      "epoch": 0.44,
      "step": 88
    },
    {
      "epoch": 0.445,
      "grad_norm": 67.63813781738281,
      "learning_rate": 1.1000000000000001e-05,
      "loss": 10.8719,
      "step": 89
    },
    {
      "bce_eff_w": 0.045000000000000005,
      "bce_loss": -0.4640274941921234,
      "epoch": 0.445,
      "step": 89
    },
    {
      "epoch": 0.45,
      "grad_norm": 56.526126861572266,
      "learning_rate": 1.1125000000000001e-05,
      "loss": 9.7981,
      "step": 90
    },
    {
      "bce_eff_w": 0.045500000000000006,
      "bce_loss": -0.46438780426979065,
      "epoch": 0.45,
      "step": 90
    },
    {
      "epoch": 0.455,
      "grad_norm": 58.84091567993164,
      "learning_rate": 1.125e-05,
      "loss": 10.563,
      "step": 91
    },
    {
      "bce_eff_w": 0.046000000000000006,
      "bce_loss": -0.46242043375968933,
      "epoch": 0.455,
      "step": 91
    },
    {
      "epoch": 0.46,
      "grad_norm": 57.50535202026367,
      "learning_rate": 1.1375e-05,
      "loss": 9.532,
      "step": 92
    },
    {
      "bce_eff_w": 0.04650000000000001,
      "bce_loss": -0.46183842420578003,
      "epoch": 0.46,
      "step": 92
    },
    {
      "epoch": 0.465,
      "grad_norm": 51.990631103515625,
      "learning_rate": 1.1500000000000002e-05,
      "loss": 10.708,
      "step": 93
    },
    {
      "bce_eff_w": 0.047,
      "bce_loss": -0.4622124433517456,
      "epoch": 0.465,
      "step": 93
    },
    {
      "epoch": 0.47,
      "grad_norm": 52.95343780517578,
      "learning_rate": 1.1625000000000001e-05,
      "loss": 8.8918,
      "step": 94
    },
    {
      "bce_eff_w": 0.0475,
      "bce_loss": -0.46242716908454895,
      "epoch": 0.47,
      "step": 94
    },
    {
      "epoch": 0.475,
      "grad_norm": 46.93238830566406,
      "learning_rate": 1.175e-05,
      "loss": 9.0365,
      "step": 95
    },
    {
      "bce_eff_w": 0.048,
      "bce_loss": -0.46465739607810974,
      "epoch": 0.475,
      "step": 95
    },
    {
      "epoch": 0.48,
      "grad_norm": 66.32672882080078,
      "learning_rate": 1.1875e-05,
      "loss": 10.0998,
      "step": 96
    },
    {
      "bce_eff_w": 0.0485,
      "bce_loss": -0.46436500549316406,
      "epoch": 0.48,
      "step": 96
    },
    {
      "epoch": 0.485,
      "grad_norm": 50.59001922607422,
      "learning_rate": 1.2e-05,
      "loss": 9.4385,
      "step": 97
    },
    {
      "bce_eff_w": 0.049,
      "bce_loss": -0.4648324251174927,
      "epoch": 0.485,
      "step": 97
    },
    {
      "epoch": 0.49,
      "grad_norm": 67.90401458740234,
      "learning_rate": 1.2125e-05,
      "loss": 11.3571,
      "step": 98
    },
    {
      "bce_eff_w": 0.0495,
      "bce_loss": -0.46318238973617554,
      "epoch": 0.49,
      "step": 98
    },
    {
      "epoch": 0.495,
      "grad_norm": 46.12901306152344,
      "learning_rate": 1.225e-05,
      "loss": 9.2154,
      "step": 99
    },
    {
      "bce_eff_w": 0.05,
      "bce_loss": -0.46471574902534485,
      "epoch": 0.495,
      "step": 99
    },
    {
      "epoch": 0.5,
      "grad_norm": 53.37900924682617,
      "learning_rate": 1.2375000000000001e-05,
      "loss": 7.3192,
      "step": 100
    },
    {
      "bce_eff_w": 0.0505,
      "bce_loss": -0.4646366834640503,
      "epoch": 0.5,
      "step": 100
    },
    {
      "epoch": 0.505,
      "grad_norm": 64.5044174194336,
      "learning_rate": 1.25e-05,
      "loss": 8.6824,
      "step": 101
    },
    {
      "bce_eff_w": 0.051000000000000004,
      "bce_loss": -0.4649508595466614,
      "epoch": 0.505,
      "step": 101
    },
    {
      "epoch": 0.51,
      "grad_norm": 63.18211364746094,
      "learning_rate": 1.2625e-05,
      "loss": 9.1057,
      "step": 102
    },
    {
      "bce_eff_w": 0.051500000000000004,
      "bce_loss": -0.46180006861686707,
      "epoch": 0.51,
      "step": 102
    },
    {
      "epoch": 0.515,
      "grad_norm": 46.51227951049805,
      "learning_rate": 1.2750000000000002e-05,
      "loss": 8.4713,
      "step": 103
    },
    {
      "bce_eff_w": 0.052000000000000005,
      "bce_loss": -0.463383287191391,
      "epoch": 0.515,
      "step": 103
    },
    {
      "epoch": 0.52,
      "grad_norm": 54.65254592895508,
      "learning_rate": 1.2875000000000001e-05,
      "loss": 9.5038,
      "step": 104
    },
    {
      "bce_eff_w": 0.052500000000000005,
      "bce_loss": -0.4637046158313751,
      "epoch": 0.52,
      "step": 104
    },
    {
      "epoch": 0.525,
      "grad_norm": 48.3548583984375,
      "learning_rate": 1.3000000000000001e-05,
      "loss": 9.8699,
      "step": 105
    },
    {
      "bce_eff_w": 0.053000000000000005,
      "bce_loss": -0.4637077748775482,
      "epoch": 0.525,
      "step": 105
    },
    {
      "epoch": 0.53,
      "grad_norm": 47.68805694580078,
      "learning_rate": 1.3125e-05,
      "loss": 9.4074,
      "step": 106
    },
    {
      "bce_eff_w": 0.053500000000000006,
      "bce_loss": -0.46235474944114685,
      "epoch": 0.53,
      "step": 106
    },
    {
      "epoch": 0.535,
      "grad_norm": 56.699920654296875,
      "learning_rate": 1.3250000000000002e-05,
      "loss": 10.7677,
      "step": 107
    },
    {
      "bce_eff_w": 0.054000000000000006,
      "bce_loss": -0.46525809168815613,
      "epoch": 0.535,
      "step": 107
    },
    {
      "epoch": 0.54,
      "grad_norm": 56.748470306396484,
      "learning_rate": 1.3375000000000002e-05,
      "loss": 8.3996,
      "step": 108
    },
    {
      "bce_eff_w": 0.05450000000000001,
      "bce_loss": -0.46414244174957275,
      "epoch": 0.54,
      "step": 108
    },
    {
      "epoch": 0.545,
      "grad_norm": 44.38187789916992,
      "learning_rate": 1.3500000000000001e-05,
      "loss": 8.8141,
      "step": 109
    },
    {
      "bce_eff_w": 0.05500000000000001,
      "bce_loss": -0.4650443196296692,
      "epoch": 0.545,
      "step": 109
    },
    {
      "epoch": 0.55,
      "grad_norm": 60.4393310546875,
      "learning_rate": 1.3625e-05,
      "loss": 8.9699,
      "step": 110
    },
    {
      "bce_eff_w": 0.05550000000000001,
      "bce_loss": -0.46081793308258057,
      "epoch": 0.55,
      "step": 110
    },
    {
      "epoch": 0.555,
      "grad_norm": 41.40561294555664,
      "learning_rate": 1.3750000000000002e-05,
      "loss": 8.9332,
      "step": 111
    },
    {
      "bce_eff_w": 0.05600000000000001,
      "bce_loss": -0.46523556113243103,
      "epoch": 0.555,
      "step": 111
    },
    {
      "epoch": 0.56,
      "grad_norm": 51.71089553833008,
      "learning_rate": 1.3875000000000002e-05,
      "loss": 7.2267,
      "step": 112
    },
    {
      "bce_eff_w": 0.056499999999999995,
      "bce_loss": -0.4646672308444977,
      "epoch": 0.56,
      "step": 112
    },
    {
      "epoch": 0.565,
      "grad_norm": 54.6438102722168,
      "learning_rate": 1.4000000000000001e-05,
      "loss": 8.8635,
      "step": 113
    },
    {
      "bce_eff_w": 0.056999999999999995,
      "bce_loss": -0.4649803340435028,
      "epoch": 0.565,
      "step": 113
    },
    {
      "epoch": 0.57,
      "grad_norm": 63.47549057006836,
      "learning_rate": 1.4125e-05,
      "loss": 8.7067,
      "step": 114
    },
    {
      "bce_eff_w": 0.057499999999999996,
      "bce_loss": -0.4647313952445984,
      "epoch": 0.57,
      "step": 114
    },
    {
      "epoch": 0.575,
      "grad_norm": 50.47797775268555,
      "learning_rate": 1.4249999999999999e-05,
      "loss": 8.655,
      "step": 115
    },
    {
      "bce_eff_w": 0.057999999999999996,
      "bce_loss": -0.46501296758651733,
      "epoch": 0.575,
      "step": 115
    },
    {
      "epoch": 0.58,
      "grad_norm": 50.73628616333008,
      "learning_rate": 1.4374999999999999e-05,
      "loss": 8.6707,
      "step": 116
    },
    {
      "bce_eff_w": 0.058499999999999996,
      "bce_loss": -0.46258631348609924,
      "epoch": 0.58,
      "step": 116
    },
    {
      "epoch": 0.585,
      "grad_norm": 44.77275848388672,
      "learning_rate": 1.45e-05,
      "loss": 8.9803,
      "step": 117
    },
    {
      "bce_eff_w": 0.059,
      "bce_loss": -0.4645646810531616,
      "epoch": 0.585,
      "step": 117
    },
    {
      "epoch": 0.59,
      "grad_norm": 48.21491622924805,
      "learning_rate": 1.4625e-05,
      "loss": 8.5659,
      "step": 118
    },
    {
      "bce_eff_w": 0.0595,
      "bce_loss": -0.4648820757865906,
      "epoch": 0.59,
      "step": 118
    },
    {
      "epoch": 0.595,
      "grad_norm": 46.0783805847168,
      "learning_rate": 1.475e-05,
      "loss": 7.8414,
      "step": 119
    },
    {
      "bce_eff_w": 0.06,
      "bce_loss": -0.46456262469291687,
      "epoch": 0.595,
      "step": 119
    },
    {
      "epoch": 0.6,
      "grad_norm": 53.508846282958984,
      "learning_rate": 1.4875e-05,
      "loss": 9.6237,
      "step": 120
    },
    {
      "bce_eff_w": 0.0605,
      "bce_loss": -0.4651379883289337,
      "epoch": 0.6,
      "step": 120
    },
    {
      "epoch": 0.605,
      "grad_norm": 70.06456756591797,
      "learning_rate": 1.5e-05,
      "loss": 8.4764,
      "step": 121
    },
    {
      "bce_eff_w": 0.061,
      "bce_loss": -0.46157214045524597,
      "epoch": 0.605,
      "step": 121
    },
    {
      "epoch": 0.61,
      "grad_norm": 38.876319885253906,
      "learning_rate": 1.5125e-05,
      "loss": 8.6364,
      "step": 122
    },
    {
      "bce_eff_w": 0.0615,
      "bce_loss": -0.4652634263038635,
      "epoch": 0.61,
      "step": 122
    },
    {
      "epoch": 0.615,
      "grad_norm": 47.732948303222656,
      "learning_rate": 1.525e-05,
      "loss": 7.6735,
      "step": 123
    },
    {
      "bce_eff_w": 0.062,
      "bce_loss": -0.4625225365161896,
      "epoch": 0.615,
      "step": 123
    },
    {
      "epoch": 0.62,
      "grad_norm": 38.566917419433594,
      "learning_rate": 1.5375e-05,
      "loss": 8.4903,
      "step": 124
    },
    {
      "bce_eff_w": 0.0625,
      "bce_loss": -0.4633859097957611,
      "epoch": 0.62,
      "step": 124
    },
    {
      "epoch": 0.625,
      "grad_norm": 32.538822174072266,
      "learning_rate": 1.55e-05,
      "loss": 8.0779,
      "step": 125
    },
    {
      "bce_eff_w": 0.063,
      "bce_loss": -0.46459636092185974,
      "epoch": 0.625,
      "step": 125
    },
    {
      "epoch": 0.63,
      "grad_norm": 49.88463592529297,
      "learning_rate": 1.5625e-05,
      "loss": 8.9737,
      "step": 126
    },
    {
      "bce_eff_w": 0.0635,
      "bce_loss": -0.46415042877197266,
      "epoch": 0.63,
      "step": 126
    },
    {
      "epoch": 0.635,
      "grad_norm": 43.19935607910156,
      "learning_rate": 1.575e-05,
      "loss": 7.7897,
      "step": 127
    },
    {
      "bce_eff_w": 0.064,
      "bce_loss": -0.4648018181324005,
      "epoch": 0.635,
      "step": 127
    },
    {
      "epoch": 0.64,
      "grad_norm": 53.60526657104492,
      "learning_rate": 1.5875e-05,
      "loss": 8.6294,
      "step": 128
    },
    {
      "bce_eff_w": 0.0645,
      "bce_loss": -0.46474751830101013,
      "epoch": 0.64,
      "step": 128
    },
    {
      "epoch": 0.645,
      "grad_norm": 39.25738525390625,
      "learning_rate": 1.6000000000000003e-05,
      "loss": 7.3888,
      "step": 129
    },
    {
      "bce_eff_w": 0.065,
      "bce_loss": -0.46510985493659973,
      "epoch": 0.645,
      "step": 129
    },
    {
      "epoch": 0.65,
      "grad_norm": 42.50278091430664,
      "learning_rate": 1.6125000000000002e-05,
      "loss": 8.6918,
      "step": 130
    },
    {
      "bce_eff_w": 0.0655,
      "bce_loss": -0.46486547589302063,
      "epoch": 0.65,
      "step": 130
    },
    {
      "epoch": 0.655,
      "grad_norm": 64.91773223876953,
      "learning_rate": 1.6250000000000002e-05,
      "loss": 9.2177,
      "step": 131
    },
    {
      "bce_eff_w": 0.066,
      "bce_loss": -0.46431541442871094,
      "epoch": 0.655,
      "step": 131
    },
    {
      "epoch": 0.66,
      "grad_norm": 59.03307342529297,
      "learning_rate": 1.6375e-05,
      "loss": 8.047,
      "step": 132
    },
    {
      "bce_eff_w": 0.0665,
      "bce_loss": -0.46224212646484375,
      "epoch": 0.66,
      "step": 132
    },
    {
      "epoch": 0.665,
      "grad_norm": 33.3810920715332,
      "learning_rate": 1.65e-05,
      "loss": 8.8435,
      "step": 133
    },
    {
      "bce_eff_w": 0.067,
      "bce_loss": -0.465175986289978,
      "epoch": 0.665,
      "step": 133
    },
    {
      "epoch": 0.67,
      "grad_norm": 53.683712005615234,
      "learning_rate": 1.6625e-05,
      "loss": 6.8509,
      "step": 134
    },
    {
      "bce_eff_w": 0.0675,
      "bce_loss": -0.4639795124530792,
      "epoch": 0.67,
      "step": 134
    },
    {
      "epoch": 0.675,
      "grad_norm": 47.11360549926758,
      "learning_rate": 1.675e-05,
      "loss": 7.3624,
      "step": 135
    },
    {
      "bce_eff_w": 0.068,
      "bce_loss": -0.4639940857887268,
      "epoch": 0.675,
      "step": 135
    },
    {
      "epoch": 0.68,
      "grad_norm": 34.322792053222656,
      "learning_rate": 1.6875000000000004e-05,
      "loss": 8.9721,
      "step": 136
    },
    {
      "bce_eff_w": 0.0685,
      "bce_loss": -0.4652080535888672,
      "epoch": 0.68,
      "step": 136
    },
    {
      "epoch": 0.685,
      "grad_norm": 47.299530029296875,
      "learning_rate": 1.7000000000000003e-05,
      "loss": 7.9641,
      "step": 137
    },
    {
      "bce_eff_w": 0.06899999999999999,
      "bce_loss": -0.4632855951786041,
      "epoch": 0.685,
      "step": 137
    },
    {
      "epoch": 0.69,
      "grad_norm": 34.72751998901367,
      "learning_rate": 1.7125000000000003e-05,
      "loss": 7.7919,
      "step": 138
    },
    {
      "bce_eff_w": 0.06949999999999999,
      "bce_loss": -0.4637623429298401,
      "epoch": 0.69,
      "step": 138
    },
    {
      "epoch": 0.695,
      "grad_norm": 37.143760681152344,
      "learning_rate": 1.725e-05,
      "loss": 8.1407,
      "step": 139
    },
    {
      "bce_eff_w": 0.06999999999999999,
      "bce_loss": -0.46490392088890076,
      "epoch": 0.695,
      "step": 139
    },
    {
      "epoch": 0.7,
      "grad_norm": 45.26401138305664,
      "learning_rate": 1.7375e-05,
      "loss": 8.0769,
      "step": 140
    },
    {
      "bce_eff_w": 0.0705,
      "bce_loss": -0.46475088596343994,
      "epoch": 0.7,
      "step": 140
    },
    {
      "epoch": 0.705,
      "grad_norm": 49.261295318603516,
      "learning_rate": 1.75e-05,
      "loss": 7.7316,
      "step": 141
    },
    {
      "bce_eff_w": 0.071,
      "bce_loss": -0.46516621112823486,
      "epoch": 0.705,
      "step": 141
    },
    {
      "epoch": 0.71,
      "grad_norm": 39.722450256347656,
      "learning_rate": 1.7625e-05,
      "loss": 7.7789,
      "step": 142
    },
    {
      "bce_eff_w": 0.0715,
      "bce_loss": -0.46511220932006836,
      "epoch": 0.71,
      "step": 142
    },
    {
      "epoch": 0.715,
      "grad_norm": 35.586910247802734,
      "learning_rate": 1.775e-05,
      "loss": 5.743,
      "step": 143
    },
    {
      "bce_eff_w": 0.072,
      "bce_loss": -0.4641256630420685,
      "epoch": 0.715,
      "step": 143
    },
    {
      "epoch": 0.72,
      "grad_norm": 27.157930374145508,
      "learning_rate": 1.7875e-05,
      "loss": 7.7097,
      "step": 144
    },
    {
      "bce_eff_w": 0.0725,
      "bce_loss": -0.46535375714302063,
      "epoch": 0.72,
      "step": 144
    },
    {
      "epoch": 0.725,
      "grad_norm": 33.76722717285156,
      "learning_rate": 1.8e-05,
      "loss": 6.2308,
      "step": 145
    },
    {
      "bce_eff_w": 0.073,
      "bce_loss": -0.4651789367198944,
      "epoch": 0.725,
      "step": 145
    },
    {
      "epoch": 0.73,
      "grad_norm": 44.966068267822266,
      "learning_rate": 1.8125e-05,
      "loss": 7.7753,
      "step": 146
    },
    {
      "bce_eff_w": 0.0735,
      "bce_loss": -0.46524930000305176,
      "epoch": 0.73,
      "step": 146
    },
    {
      "epoch": 0.735,
      "grad_norm": 41.0178108215332,
      "learning_rate": 1.825e-05,
      "loss": 6.616,
      "step": 147
    },
    {
      "bce_eff_w": 0.074,
      "bce_loss": -0.4653513431549072,
      "epoch": 0.735,
      "step": 147
    },
    {
      "epoch": 0.74,
      "grad_norm": 40.8757209777832,
      "learning_rate": 1.8375e-05,
      "loss": 6.7773,
      "step": 148
    },
    {
      "bce_eff_w": 0.0745,
      "bce_loss": -0.46521908044815063,
      "epoch": 0.74,
      "step": 148
    },
    {
      "epoch": 0.745,
      "grad_norm": 30.586565017700195,
      "learning_rate": 1.85e-05,
      "loss": 6.3987,
      "step": 149
    },
    {
      "bce_eff_w": 0.07500000000000001,
      "bce_loss": -0.4652109742164612,
      "epoch": 0.745,
      "step": 149
    },
    {
      "epoch": 0.75,
      "grad_norm": 40.9041748046875,
      "learning_rate": 1.8625000000000002e-05,
      "loss": 8.1543,
      "step": 150
    },
    {
      "bce_eff_w": 0.07550000000000001,
      "bce_loss": -0.464022696018219,
      "epoch": 0.75,
      "step": 150
    },
    {
      "epoch": 0.755,
      "grad_norm": 24.68842124938965,
      "learning_rate": 1.8750000000000002e-05,
      "loss": 7.322,
      "step": 151
    },
    {
      "bce_eff_w": 0.07600000000000001,
      "bce_loss": -0.4636939764022827,
      "epoch": 0.755,
      "step": 151
    },
    {
      "epoch": 0.76,
      "grad_norm": 32.45808792114258,
      "learning_rate": 1.8875e-05,
      "loss": 7.1135,
      "step": 152
    },
    {
      "bce_eff_w": 0.07650000000000001,
      "bce_loss": -0.46526458859443665,
      "epoch": 0.76,
      "step": 152
    },
    {
      "epoch": 0.765,
      "grad_norm": 26.125076293945312,
      "learning_rate": 1.9e-05,
      "loss": 7.2822,
      "step": 153
    },
    {
      "bce_eff_w": 0.07700000000000001,
      "bce_loss": -0.4650348722934723,
      "epoch": 0.765,
      "step": 153
    },
    {
      "epoch": 0.77,
      "grad_norm": 48.71684265136719,
      "learning_rate": 1.9125e-05,
      "loss": 7.3886,
      "step": 154
    },
    {
      "bce_eff_w": 0.07750000000000001,
      "bce_loss": -0.4651755690574646,
      "epoch": 0.77,
      "step": 154
    },
    {
      "epoch": 0.775,
      "grad_norm": 45.03085708618164,
      "learning_rate": 1.925e-05,
      "loss": 8.0334,
      "step": 155
    },
    {
      "bce_eff_w": 0.07800000000000001,
      "bce_loss": -0.46443888545036316,
      "epoch": 0.775,
      "step": 155
    },
    {
      "epoch": 0.78,
      "grad_norm": 24.743602752685547,
      "learning_rate": 1.9375e-05,
      "loss": 8.2274,
      "step": 156
    },
    {
      "bce_eff_w": 0.07850000000000001,
      "bce_loss": -0.4637763500213623,
      "epoch": 0.78,
      "step": 156
    },
    {
      "epoch": 0.785,
      "grad_norm": 26.276878356933594,
      "learning_rate": 1.9500000000000003e-05,
      "loss": 6.8321,
      "step": 157
    },
    {
      "bce_eff_w": 0.07900000000000001,
      "bce_loss": -0.4653412997722626,
      "epoch": 0.785,
      "step": 157
    },
    {
      "epoch": 0.79,
      "grad_norm": 32.728145599365234,
      "learning_rate": 1.9625000000000003e-05,
      "loss": 6.8865,
      "step": 158
    },
    {
      "bce_eff_w": 0.07950000000000002,
      "bce_loss": -0.4653024971485138,
      "epoch": 0.79,
      "step": 158
    },
    {
      "epoch": 0.795,
      "grad_norm": 36.236244201660156,
      "learning_rate": 1.9750000000000002e-05,
      "loss": 7.6199,
      "step": 159
    },
    {
      "bce_eff_w": 0.08000000000000002,
      "bce_loss": -0.46428173780441284,
      "epoch": 0.795,
      "step": 159
    },
    {
      "epoch": 0.8,
      "grad_norm": 24.63259506225586,
      "learning_rate": 1.9875000000000002e-05,
      "loss": 8.2861,
      "step": 160
    },
    {
      "bce_eff_w": 0.08050000000000002,
      "bce_loss": -0.4647698700428009,
      "epoch": 0.8,
      "step": 160
    },
    {
      "epoch": 0.805,
      "grad_norm": 23.229747772216797,
      "learning_rate": 2e-05,
      "loss": 8.0105,
      "step": 161
    },
    {
      "bce_eff_w": 0.08100000000000002,
      "bce_loss": -0.46541303396224976,
      "epoch": 0.805,
      "step": 161
    },
    {
      "epoch": 0.81,
      "grad_norm": 36.47067642211914,
      "learning_rate": 2.0125e-05,
      "loss": 6.373,
      "step": 162
    },
    {
      "bce_eff_w": 0.0815,
      "bce_loss": -0.4654325842857361,
      "epoch": 0.81,
      "step": 162
    },
    {
      "epoch": 0.815,
      "grad_norm": 30.947561264038086,
      "learning_rate": 2.025e-05,
      "loss": 5.8572,
      "step": 163
    },
    {
      "bce_eff_w": 0.082,
      "bce_loss": -0.46536242961883545,
      "epoch": 0.815,
      "step": 163
    },
    {
      "epoch": 0.82,
      "grad_norm": 32.21134567260742,
      "learning_rate": 2.0375e-05,
      "loss": 5.9233,
      "step": 164
    },
    {
      "bce_eff_w": 0.0825,
      "bce_loss": -0.4644497036933899,
      "epoch": 0.82,
      "step": 164
    },
    {
      "epoch": 0.825,
      "grad_norm": 27.76972007751465,
      "learning_rate": 2.05e-05,
      "loss": 7.0918,
      "step": 165
    },
    {
      "bce_eff_w": 0.083,
      "bce_loss": -0.4652973711490631,
      "epoch": 0.825,
      "step": 165
    },
    {
      "epoch": 0.83,
      "grad_norm": 32.436439514160156,
      "learning_rate": 2.0625e-05,
      "loss": 7.8897,
      "step": 166
    },
    {
      "bce_eff_w": 0.0835,
      "bce_loss": -0.46476253867149353,
      "epoch": 0.83,
      "step": 166
    },
    {
      "epoch": 0.835,
      "grad_norm": 25.51907730102539,
      "learning_rate": 2.075e-05,
      "loss": 6.7827,
      "step": 167
    },
    {
      "bce_eff_w": 0.084,
      "bce_loss": -0.46515804529190063,
      "epoch": 0.835,
      "step": 167
    },
    {
      "epoch": 0.84,
      "grad_norm": 25.191184997558594,
      "learning_rate": 2.0875e-05,
      "loss": 6.426,
      "step": 168
    },
    {
      "bce_eff_w": 0.0845,
      "bce_loss": -0.4639824330806732,
      "epoch": 0.84,
      "step": 168
    },
    {
      "epoch": 0.845,
      "grad_norm": 36.15488815307617,
      "learning_rate": 2.1e-05,
      "loss": 7.8533,
      "step": 169
    },
    {
      "bce_eff_w": 0.085,
      "bce_loss": -0.4649084806442261,
      "epoch": 0.845,
      "step": 169
    },
    {
      "epoch": 0.85,
      "grad_norm": 24.643367767333984,
      "learning_rate": 2.1125000000000002e-05,
      "loss": 6.715,
      "step": 170
    },
    {
      "bce_eff_w": 0.0855,
      "bce_loss": -0.4653628468513489,
      "epoch": 0.85,
      "step": 170
    },
    {
      "epoch": 0.855,
      "grad_norm": 28.089956283569336,
      "learning_rate": 2.125e-05,
      "loss": 6.1232,
      "step": 171
    },
    {
      "bce_eff_w": 0.08600000000000001,
      "bce_loss": -0.4653455913066864,
      "epoch": 0.855,
      "step": 171
    },
    {
      "epoch": 0.86,
      "grad_norm": 40.18937301635742,
      "learning_rate": 2.1375e-05,
      "loss": 5.9669,
      "step": 172
    },
    {
      "bce_eff_w": 0.08650000000000001,
      "bce_loss": -0.46541064977645874,
      "epoch": 0.86,
      "step": 172
    },
    {
      "epoch": 0.865,
      "grad_norm": 31.298433303833008,
      "learning_rate": 2.15e-05,
      "loss": 7.2963,
      "step": 173
    },
    {
      "bce_eff_w": 0.08700000000000001,
      "bce_loss": -0.46486586332321167,
      "epoch": 0.865,
      "step": 173
    },
    {
      "epoch": 0.87,
      "grad_norm": 28.048084259033203,
      "learning_rate": 2.1625e-05,
      "loss": 5.8982,
      "step": 174
    },
    {
      "bce_eff_w": 0.08750000000000001,
      "bce_loss": -0.4654850661754608,
      "epoch": 0.87,
      "step": 174
    },
    {
      "epoch": 0.875,
      "grad_norm": 35.404170989990234,
      "learning_rate": 2.175e-05,
      "loss": 6.7977,
      "step": 175
    },
    {
      "bce_eff_w": 0.08800000000000001,
      "bce_loss": -0.46535971760749817,
      "epoch": 0.875,
      "step": 175
    },
    {
      "epoch": 0.88,
      "grad_norm": 39.42770004272461,
      "learning_rate": 2.1875e-05,
      "loss": 6.6949,
      "step": 176
    },
    {
      "bce_eff_w": 0.08850000000000001,
      "bce_loss": -0.4653153717517853,
      "epoch": 0.88,
      "step": 176
    },
    {
      "epoch": 0.885,
      "grad_norm": 21.10227394104004,
      "learning_rate": 2.2000000000000003e-05,
      "loss": 7.2648,
      "step": 177
    },
    {
      "bce_eff_w": 0.08900000000000001,
      "bce_loss": -0.46524813771247864,
      "epoch": 0.885,
      "step": 177
    },
    {
      "epoch": 0.89,
      "grad_norm": 34.02735137939453,
      "learning_rate": 2.2125000000000002e-05,
      "loss": 6.8613,
      "step": 178
    },
    {
      "bce_eff_w": 0.08950000000000001,
      "bce_loss": -0.4650106728076935,
      "epoch": 0.89,
      "step": 178
    },
    {
      "epoch": 0.895,
      "grad_norm": 26.2396297454834,
      "learning_rate": 2.2250000000000002e-05,
      "loss": 6.5095,
      "step": 179
    },
    {
      "bce_eff_w": 0.09000000000000001,
      "bce_loss": -0.4645770490169525,
      "epoch": 0.895,
      "step": 179
    },
    {
      "epoch": 0.9,
      "grad_norm": 19.548757553100586,
      "learning_rate": 2.2375000000000002e-05,
      "loss": 7.4345,
      "step": 180
    },
    {
      "bce_eff_w": 0.09050000000000001,
      "bce_loss": -0.46549615263938904,
      "epoch": 0.9,
      "step": 180
    },
    {
      "epoch": 0.905,
      "grad_norm": 22.511245727539062,
      "learning_rate": 2.25e-05,
      "loss": 5.0731,
      "step": 181
    },
    {
      "bce_eff_w": 0.09100000000000001,
      "bce_loss": -0.465352863073349,
      "epoch": 0.905,
      "step": 181
    },
    {
      "epoch": 0.91,
      "grad_norm": 19.804000854492188,
      "learning_rate": 2.2625e-05,
      "loss": 5.9574,
      "step": 182
    },
    {
      "bce_eff_w": 0.09150000000000001,
      "bce_loss": -0.46530166268348694,
      "epoch": 0.91,
      "step": 182
    },
    {
      "epoch": 0.915,
      "grad_norm": 32.83204650878906,
      "learning_rate": 2.275e-05,
      "loss": 5.5059,
      "step": 183
    },
    {
      "bce_eff_w": 0.09200000000000001,
      "bce_loss": -0.4648280143737793,
      "epoch": 0.915,
      "step": 183
    },
    {
      "epoch": 0.92,
      "grad_norm": 21.301965713500977,
      "learning_rate": 2.2875e-05,
      "loss": 6.3712,
      "step": 184
    },
    {
      "bce_eff_w": 0.09250000000000001,
      "bce_loss": -0.46542391180992126,
      "epoch": 0.92,
      "step": 184
    },
    {
      "epoch": 0.925,
      "grad_norm": 29.221105575561523,
      "learning_rate": 2.3000000000000003e-05,
      "loss": 6.0756,
      "step": 185
    },
    {
      "bce_eff_w": 0.09300000000000001,
      "bce_loss": -0.46508777141571045,
      "epoch": 0.925,
      "step": 185
    },
    {
      "epoch": 0.93,
      "grad_norm": 19.429363250732422,
      "learning_rate": 2.3125000000000003e-05,
      "loss": 6.6733,
      "step": 186
    },
    {
      "bce_eff_w": 0.09350000000000001,
      "bce_loss": -0.4654500484466553,
      "epoch": 0.93,
      "step": 186
    },
    {
      "epoch": 0.935,
      "grad_norm": 24.802095413208008,
      "learning_rate": 2.3250000000000003e-05,
      "loss": 6.8834,
      "step": 187
    },
    {
      "bce_eff_w": 0.094,
      "bce_loss": -0.46526867151260376,
      "epoch": 0.935,
      "step": 187
    },
    {
      "epoch": 0.94,
      "grad_norm": 26.278461456298828,
      "learning_rate": 2.3375000000000002e-05,
      "loss": 7.5417,
      "step": 188
    },
    {
      "bce_eff_w": 0.0945,
      "bce_loss": -0.46540191769599915,
      "epoch": 0.94,
      "step": 188
    },
    {
      "epoch": 0.945,
      "grad_norm": 17.684534072875977,
      "learning_rate": 2.35e-05,
      "loss": 5.7268,
      "step": 189
    },
    {
      "bce_eff_w": 0.095,
      "bce_loss": -0.4654042720794678,
      "epoch": 0.945,
      "step": 189
    },
    {
      "epoch": 0.95,
      "grad_norm": 30.617815017700195,
      "learning_rate": 2.3624999999999998e-05,
      "loss": 5.7889,
      "step": 190
    },
    {
      "bce_eff_w": 0.0955,
      "bce_loss": -0.46507513523101807,
      "epoch": 0.95,
      "step": 190
    },
    {
      "epoch": 0.955,
      "grad_norm": 20.512170791625977,
      "learning_rate": 2.375e-05,
      "loss": 6.1226,
      "step": 191
    },
    {
      "bce_eff_w": 0.096,
      "bce_loss": -0.46535685658454895,
      "epoch": 0.955,
      "step": 191
    },
    {
      "epoch": 0.96,
      "grad_norm": 27.41116714477539,
      "learning_rate": 2.3875e-05,
      "loss": 7.3303,
      "step": 192
    },
    {
      "bce_eff_w": 0.0965,
      "bce_loss": -0.4653269946575165,
      "epoch": 0.96,
      "step": 192
    },
    {
      "epoch": 0.965,
      "grad_norm": 28.403099060058594,
      "learning_rate": 2.4e-05,
      "loss": 6.7769,
      "step": 193
    },
    {
      "bce_eff_w": 0.097,
      "bce_loss": -0.4640354514122009,
      "epoch": 0.965,
      "step": 193
    },
    {
      "epoch": 0.97,
      "grad_norm": 30.64805793762207,
      "learning_rate": 2.4125e-05,
      "loss": 7.8275,
      "step": 194
    },
    {
      "bce_eff_w": 0.0975,
      "bce_loss": -0.46503183245658875,
      "epoch": 0.97,
      "step": 194
    },
    {
      "epoch": 0.975,
      "grad_norm": 20.91246223449707,
      "learning_rate": 2.425e-05,
      "loss": 6.2036,
      "step": 195
    },
    {
      "bce_eff_w": 0.098,
      "bce_loss": -0.4648284912109375,
      "epoch": 0.975,
      "step": 195
    },
    {
      "epoch": 0.98,
      "grad_norm": 15.831451416015625,
      "learning_rate": 2.4375e-05,
      "loss": 7.4064,
      "step": 196
    },
    {
      "bce_eff_w": 0.0985,
      "bce_loss": -0.46530038118362427,
      "epoch": 0.98,
      "step": 196
    },
    {
      "epoch": 0.985,
      "grad_norm": 33.57976531982422,
      "learning_rate": 2.45e-05,
      "loss": 4.7734,
      "step": 197
    },
    {
      "bce_eff_w": 0.099,
      "bce_loss": -0.46546173095703125,
      "epoch": 0.985,
      "step": 197
    },
    {
      "epoch": 0.99,
      "grad_norm": 18.090978622436523,
      "learning_rate": 2.4625000000000002e-05,
      "loss": 6.4244,
      "step": 198
    },
    {
      "bce_eff_w": 0.0995,
      "bce_loss": -0.4654600918292999,
      "epoch": 0.99,
      "step": 198
    },
    {
      "epoch": 0.995,
      "grad_norm": 29.41700553894043,
      "learning_rate": 2.4750000000000002e-05,
      "loss": 5.1589,
      "step": 199
    },
    {
      "bce_eff_w": 0.1,
      "bce_loss": -0.4654471278190613,
      "epoch": 0.995,
      "step": 199
    },
    {
      "epoch": 1.0,
      "grad_norm": 27.571374893188477,
      "learning_rate": 2.4875e-05,
      "loss": 5.4809,
      "step": 200
    },
    {
      "bce_eff_w": 0.10049999999999999,
      "bce_loss": -0.46539369225502014,
      "epoch": 1.0,
      "step": 200
    },
    {
      "epoch": 1.005,
      "grad_norm": 32.270896911621094,
      "learning_rate": 2.5e-05,
      "loss": 5.7413,
      "step": 201
    },
    {
      "bce_eff_w": 0.101,
      "bce_loss": -0.46541306376457214,
      "epoch": 1.005,
      "step": 201
    },
    {
      "epoch": 1.01,
      "grad_norm": 28.617961883544922,
      "learning_rate": 2.5124999999999997e-05,
      "loss": 5.7259,
      "step": 202
    },
    {
      "bce_eff_w": 0.10149999999999999,
      "bce_loss": -0.46517762541770935,
      "epoch": 1.01,
      "step": 202
    },
    {
      "epoch": 1.015,
      "grad_norm": 28.45734214782715,
      "learning_rate": 2.525e-05,
      "loss": 5.9612,
      "step": 203
    },
    {
      "bce_eff_w": 0.10200000000000001,
      "bce_loss": -0.4651470482349396,
      "epoch": 1.015,
      "step": 203
    },
    {
      "epoch": 1.02,
      "grad_norm": 26.71159553527832,
      "learning_rate": 2.5375e-05,
      "loss": 6.5435,
      "step": 204
    },
    {
      "bce_eff_w": 0.1025,
      "bce_loss": -0.46497997641563416,
      "epoch": 1.02,
      "step": 204
    },
    {
      "epoch": 1.025,
      "grad_norm": 24.66506576538086,
      "learning_rate": 2.5500000000000003e-05,
      "loss": 5.4496,
      "step": 205
    },
    {
      "bce_eff_w": 0.10300000000000001,
      "bce_loss": -0.4650716185569763,
      "epoch": 1.025,
      "step": 205
    },
    {
      "epoch": 1.03,
      "grad_norm": 18.89259147644043,
      "learning_rate": 2.5625e-05,
      "loss": 6.3648,
      "step": 206
    },
    {
      "bce_eff_w": 0.1035,
      "bce_loss": -0.4653087556362152,
      "epoch": 1.03,
      "step": 206
    },
    {
      "epoch": 1.035,
      "grad_norm": 29.510780334472656,
      "learning_rate": 2.5750000000000002e-05,
      "loss": 5.5884,
      "step": 207
    },
    {
      "bce_eff_w": 0.10400000000000001,
      "bce_loss": -0.4655051529407501,
      "epoch": 1.035,
      "step": 207
    },
    {
      "epoch": 1.04,
      "grad_norm": 27.571569442749023,
      "learning_rate": 2.5875e-05,
      "loss": 5.2667,
      "step": 208
    },
    {
      "bce_eff_w": 0.1045,
      "bce_loss": -0.4644837975502014,
      "epoch": 1.04,
      "step": 208
    },
    {
      "epoch": 1.045,
      "grad_norm": 24.202919006347656,
      "learning_rate": 2.6000000000000002e-05,
      "loss": 5.6619,
      "step": 209
    },
    {
      "bce_eff_w": 0.10500000000000001,
      "bce_loss": -0.46546927094459534,
      "epoch": 1.045,
      "step": 209
    },
    {
      "epoch": 1.05,
      "grad_norm": 30.928730010986328,
      "learning_rate": 2.6124999999999998e-05,
      "loss": 5.3611,
      "step": 210
    },
    {
      "bce_eff_w": 0.1055,
      "bce_loss": -0.46545669436454773,
      "epoch": 1.05,
      "step": 210
    },
    {
      "epoch": 1.055,
      "grad_norm": 26.173431396484375,
      "learning_rate": 2.625e-05,
      "loss": 5.4524,
      "step": 211
    },
    {
      "bce_eff_w": 0.10600000000000001,
      "bce_loss": -0.4651934504508972,
      "epoch": 1.055,
      "step": 211
    },
    {
      "epoch": 1.06,
      "grad_norm": 21.759653091430664,
      "learning_rate": 2.6375e-05,
      "loss": 6.9892,
      "step": 212
    },
    {
      "bce_eff_w": 0.1065,
      "bce_loss": -0.4654938280582428,
      "epoch": 1.06,
      "step": 212
    },
    {
      "epoch": 1.065,
      "grad_norm": 21.749317169189453,
      "learning_rate": 2.6500000000000004e-05,
      "loss": 5.5857,
      "step": 213
    },
    {
      "bce_eff_w": 0.10700000000000001,
      "bce_loss": -0.4653974175453186,
      "epoch": 1.065,
      "step": 213
    },
    {
      "epoch": 1.07,
      "grad_norm": 36.931968688964844,
      "learning_rate": 2.6625e-05,
      "loss": 5.5409,
      "step": 214
    },
    {
      "bce_eff_w": 0.1075,
      "bce_loss": -0.46533578634262085,
      "epoch": 1.07,
      "step": 214
    },
    {
      "epoch": 1.075,
      "grad_norm": 19.221853256225586,
      "learning_rate": 2.6750000000000003e-05,
      "loss": 6.0882,
      "step": 215
    },
    {
      "bce_eff_w": 0.10800000000000001,
      "bce_loss": -0.46511006355285645,
      "epoch": 1.075,
      "step": 215
    },
    {
      "epoch": 1.08,
      "grad_norm": 38.10520935058594,
      "learning_rate": 2.6875e-05,
      "loss": 6.6061,
      "step": 216
    },
    {
      "bce_eff_w": 0.1085,
      "bce_loss": -0.46526002883911133,
      "epoch": 1.08,
      "step": 216
    },
    {
      "epoch": 1.085,
      "grad_norm": 23.130237579345703,
      "learning_rate": 2.7000000000000002e-05,
      "loss": 6.1181,
      "step": 217
    },
    {
      "bce_eff_w": 0.10900000000000001,
      "bce_loss": -0.4653128385543823,
      "epoch": 1.085,
      "step": 217
    },
    {
      "epoch": 1.09,
      "grad_norm": 19.503828048706055,
      "learning_rate": 2.7125000000000002e-05,
      "loss": 5.4722,
      "step": 218
    },
    {
      "bce_eff_w": 0.1095,
      "bce_loss": -0.465379923582077,
      "epoch": 1.09,
      "step": 218
    },
    {
      "epoch": 1.095,
      "grad_norm": 31.182523727416992,
      "learning_rate": 2.725e-05,
      "loss": 4.7032,
      "step": 219
    },
    {
      "bce_eff_w": 0.11000000000000001,
      "bce_loss": -0.4654683768749237,
      "epoch": 1.095,
      "step": 219
    },
    {
      "epoch": 1.1,
      "grad_norm": 26.428598403930664,
      "learning_rate": 2.7375e-05,
      "loss": 5.0783,
      "step": 220
    },
    {
      "bce_eff_w": 0.1105,
      "bce_loss": -0.4654862880706787,
      "epoch": 1.1,
      "step": 220
    },
    {
      "epoch": 1.105,
      "grad_norm": 37.16847610473633,
      "learning_rate": 2.7500000000000004e-05,
      "loss": 4.4896,
      "step": 221
    },
    {
      "bce_eff_w": 0.11100000000000002,
      "bce_loss": -0.46525755524635315,
      "epoch": 1.105,
      "step": 221
    },
    {
      "epoch": 1.11,
      "grad_norm": 18.622602462768555,
      "learning_rate": 2.7625e-05,
      "loss": 5.8159,
      "step": 222
    },
    {
      "bce_eff_w": 0.1115,
      "bce_loss": -0.465457558631897,
      "epoch": 1.11,
      "step": 222
    },
    {
      "epoch": 1.115,
      "grad_norm": 23.820091247558594,
      "learning_rate": 2.7750000000000004e-05,
      "loss": 5.635,
      "step": 223
    },
    {
      "bce_eff_w": 0.11200000000000002,
      "bce_loss": -0.46459025144577026,
      "epoch": 1.115,
      "step": 223
    },
    {
      "epoch": 1.12,
      "grad_norm": 22.166034698486328,
      "learning_rate": 2.7875e-05,
      "loss": 6.6965,
      "step": 224
    },
    {
      "bce_eff_w": 0.1125,
      "bce_loss": -0.46478205919265747,
      "epoch": 1.12,
      "step": 224
    },
    {
      "epoch": 1.125,
      "grad_norm": 26.089282989501953,
      "learning_rate": 2.8000000000000003e-05,
      "loss": 5.5054,
      "step": 225
    },
    {
      "bce_eff_w": 0.11299999999999999,
      "bce_loss": -0.4654451608657837,
      "epoch": 1.125,
      "step": 225
    },
    {
      "epoch": 1.13,
      "grad_norm": 33.0383415222168,
      "learning_rate": 2.8125000000000003e-05,
      "loss": 4.703,
      "step": 226
    },
    {
      "bce_eff_w": 0.1135,
      "bce_loss": -0.46454179286956787,
      "epoch": 1.13,
      "step": 226
    },
    {
      "epoch": 1.135,
      "grad_norm": 23.894332885742188,
      "learning_rate": 2.825e-05,
      "loss": 6.7121,
      "step": 227
    },
    {
      "bce_eff_w": 0.11399999999999999,
      "bce_loss": -0.46528229117393494,
      "epoch": 1.135,
      "step": 227
    },
    {
      "epoch": 1.1400000000000001,
      "grad_norm": 34.90788269042969,
      "learning_rate": 2.8375000000000002e-05,
      "loss": 4.9337,
      "step": 228
    },
    {
      "bce_eff_w": 0.1145,
      "bce_loss": -0.4653344750404358,
      "epoch": 1.1400000000000001,
      "step": 228
    },
    {
      "epoch": 1.145,
      "grad_norm": 19.506755828857422,
      "learning_rate": 2.8499999999999998e-05,
      "loss": 5.24,
      "step": 229
    },
    {
      "bce_eff_w": 0.11499999999999999,
      "bce_loss": -0.4651220142841339,
      "epoch": 1.145,
      "step": 229
    },
    {
      "epoch": 1.15,
      "grad_norm": 34.77787780761719,
      "learning_rate": 2.8625e-05,
      "loss": 5.3305,
      "step": 230
    },
    {
      "bce_eff_w": 0.1155,
      "bce_loss": -0.46513375639915466,
      "epoch": 1.15,
      "step": 230
    },
    {
      "epoch": 1.155,
      "grad_norm": 36.750205993652344,
      "learning_rate": 2.8749999999999997e-05,
      "loss": 5.6413,
      "step": 231
    },
    {
      "bce_eff_w": 0.11599999999999999,
      "bce_loss": -0.4643872380256653,
      "epoch": 1.155,
      "step": 231
    },
    {
      "epoch": 1.16,
      "grad_norm": 23.805252075195312,
      "learning_rate": 2.8875e-05,
      "loss": 5.9575,
      "step": 232
    },
    {
      "bce_eff_w": 0.1165,
      "bce_loss": -0.4654479920864105,
      "epoch": 1.16,
      "step": 232
    },
    {
      "epoch": 1.165,
      "grad_norm": 21.278512954711914,
      "learning_rate": 2.9e-05,
      "loss": 6.1502,
      "step": 233
    },
    {
      "bce_eff_w": 0.11699999999999999,
      "bce_loss": -0.46469005942344666,
      "epoch": 1.165,
      "step": 233
    },
    {
      "epoch": 1.17,
      "grad_norm": 25.141483306884766,
      "learning_rate": 2.9125000000000003e-05,
      "loss": 5.3023,
      "step": 234
    },
    {
      "bce_eff_w": 0.11750000000000001,
      "bce_loss": -0.46549445390701294,
      "epoch": 1.17,
      "step": 234
    },
    {
      "epoch": 1.175,
      "grad_norm": 27.434892654418945,
      "learning_rate": 2.925e-05,
      "loss": 3.8563,
      "step": 235
    },
    {
      "bce_eff_w": 0.118,
      "bce_loss": -0.46541351079940796,
      "epoch": 1.175,
      "step": 235
    },
    {
      "epoch": 1.18,
      "grad_norm": 21.9615478515625,
      "learning_rate": 2.9375000000000003e-05,
      "loss": 6.0974,
      "step": 236
    },
    {
      "bce_eff_w": 0.11850000000000001,
      "bce_loss": -0.4655005931854248,
      "epoch": 1.18,
      "step": 236
    },
    {
      "epoch": 1.185,
      "grad_norm": 17.93736457824707,
      "learning_rate": 2.95e-05,
      "loss": 6.0547,
      "step": 237
    },
    {
      "bce_eff_w": 0.119,
      "bce_loss": -0.4654359221458435,
      "epoch": 1.185,
      "step": 237
    },
    {
      "epoch": 1.19,
      "grad_norm": 23.03627586364746,
      "learning_rate": 2.9625000000000002e-05,
      "loss": 3.7082,
      "step": 238
    },
    {
      "bce_eff_w": 0.11950000000000001,
      "bce_loss": -0.4653204381465912,
      "epoch": 1.19,
      "step": 238
    },
    {
      "epoch": 1.195,
      "grad_norm": 29.666677474975586,
      "learning_rate": 2.975e-05,
      "loss": 4.1645,
      "step": 239
    },
    {
      "bce_eff_w": 0.12,
      "bce_loss": -0.46529868245124817,
      "epoch": 1.195,
      "step": 239
    },
    {
      "epoch": 1.2,
      "grad_norm": 21.095489501953125,
      "learning_rate": 2.9875000000000004e-05,
      "loss": 6.4048,
      "step": 240
    },
    {
      "bce_eff_w": 0.12050000000000001,
      "bce_loss": -0.46530821919441223,
      "epoch": 1.2,
      "step": 240
    },
    {
      "epoch": 1.205,
      "grad_norm": 25.554447174072266,
      "learning_rate": 3e-05,
      "loss": 4.9283,
      "step": 241
    },
    {
      "bce_eff_w": 0.121,
      "bce_loss": -0.46534955501556396,
      "epoch": 1.205,
      "step": 241
    },
    {
      "epoch": 1.21,
      "grad_norm": 37.64746856689453,
      "learning_rate": 3.0125000000000004e-05,
      "loss": 3.5451,
      "step": 242
    },
    {
      "bce_eff_w": 0.12150000000000001,
      "bce_loss": -0.46532294154167175,
      "epoch": 1.21,
      "step": 242
    },
    {
      "epoch": 1.215,
      "grad_norm": 27.65874481201172,
      "learning_rate": 3.025e-05,
      "loss": 3.9054,
      "step": 243
    },
    {
      "bce_eff_w": 0.122,
      "bce_loss": -0.46516525745391846,
      "epoch": 1.215,
      "step": 243
    },
    {
      "epoch": 1.22,
      "grad_norm": 18.89794158935547,
      "learning_rate": 3.0375000000000003e-05,
      "loss": 6.2768,
      "step": 244
    },
    {
      "bce_eff_w": 0.12250000000000001,
      "bce_loss": -0.4654252529144287,
      "epoch": 1.22,
      "step": 244
    },
    {
      "epoch": 1.225,
      "grad_norm": 31.50501251220703,
      "learning_rate": 3.05e-05,
      "loss": 4.2043,
      "step": 245
    },
    {
      "bce_eff_w": 0.123,
      "bce_loss": -0.46483609080314636,
      "epoch": 1.225,
      "step": 245
    },
    {
      "epoch": 1.23,
      "grad_norm": 20.825159072875977,
      "learning_rate": 3.0625000000000006e-05,
      "loss": 4.7428,
      "step": 246
    },
    {
      "bce_eff_w": 0.12350000000000001,
      "bce_loss": -0.46535786986351013,
      "epoch": 1.23,
      "step": 246
    },
    {
      "epoch": 1.2349999999999999,
      "grad_norm": 31.615493774414062,
      "learning_rate": 3.075e-05,
      "loss": 4.4456,
      "step": 247
    },
    {
      "bce_eff_w": 0.124,
      "bce_loss": -0.46528974175453186,
      "epoch": 1.2349999999999999,
      "step": 247
    },
    {
      "epoch": 1.24,
      "grad_norm": 30.924272537231445,
      "learning_rate": 3.0875000000000005e-05,
      "loss": 4.8571,
      "step": 248
    },
    {
      "bce_eff_w": 0.12450000000000001,
      "bce_loss": -0.46535831689834595,
      "epoch": 1.24,
      "step": 248
    },
    {
      "epoch": 1.245,
      "grad_norm": 25.927143096923828,
      "learning_rate": 3.1e-05,
      "loss": 4.7735,
      "step": 249
    },
    {
      "bce_eff_w": 0.125,
      "bce_loss": -0.46537861227989197,
      "epoch": 1.245,
      "step": 249
    },
    {
      "epoch": 1.25,
      "grad_norm": 33.86741256713867,
      "learning_rate": 3.1125000000000004e-05,
      "loss": 3.8459,
      "step": 250
    },
    {
      "bce_eff_w": 0.1255,
      "bce_loss": -0.46489226818084717,
      "epoch": 1.25,
      "step": 250
    },
    {
      "epoch": 1.255,
      "grad_norm": 17.510725021362305,
      "learning_rate": 3.125e-05,
      "loss": 5.233,
      "step": 251
    },
    {
      "bce_eff_w": 0.126,
      "bce_loss": -0.46472275257110596,
      "epoch": 1.255,
      "step": 251
    },
    {
      "epoch": 1.26,
      "grad_norm": 24.10572052001953,
      "learning_rate": 3.1375e-05,
      "loss": 4.654,
      "step": 252
    },
    {
      "bce_eff_w": 0.1265,
      "bce_loss": -0.4653651714324951,
      "epoch": 1.26,
      "step": 252
    },
    {
      "epoch": 1.2650000000000001,
      "grad_norm": 23.301401138305664,
      "learning_rate": 3.15e-05,
      "loss": 4.7487,
      "step": 253
    },
    {
      "bce_eff_w": 0.127,
      "bce_loss": -0.4648127257823944,
      "epoch": 1.2650000000000001,
      "step": 253
    },
    {
      "epoch": 1.27,
      "grad_norm": 26.583160400390625,
      "learning_rate": 3.1624999999999996e-05,
      "loss": 6.2717,
      "step": 254
    },
    {
      "bce_eff_w": 0.1275,
      "bce_loss": -0.4655085802078247,
      "epoch": 1.27,
      "step": 254
    },
    {
      "epoch": 1.275,
      "grad_norm": 23.08489227294922,
      "learning_rate": 3.175e-05,
      "loss": 4.1414,
      "step": 255
    },
    {
      "bce_eff_w": 0.128,
      "bce_loss": -0.4634343683719635,
      "epoch": 1.275,
      "step": 255
    },
    {
      "epoch": 1.28,
      "grad_norm": 25.74541664123535,
      "learning_rate": 3.1875e-05,
      "loss": 4.6913,
      "step": 256
    },
    {
      "bce_eff_w": 0.1285,
      "bce_loss": -0.46545955538749695,
      "epoch": 1.28,
      "step": 256
    },
    {
      "epoch": 1.285,
      "grad_norm": 22.8374080657959,
      "learning_rate": 3.2000000000000005e-05,
      "loss": 4.1029,
      "step": 257
    },
    {
      "bce_eff_w": 0.129,
      "bce_loss": -0.464824378490448,
      "epoch": 1.285,
      "step": 257
    },
    {
      "epoch": 1.29,
      "grad_norm": 18.409809112548828,
      "learning_rate": 3.2125e-05,
      "loss": 6.8878,
      "step": 258
    },
    {
      "bce_eff_w": 0.1295,
      "bce_loss": -0.46539291739463806,
      "epoch": 1.29,
      "step": 258
    },
    {
      "epoch": 1.295,
      "grad_norm": 22.524255752563477,
      "learning_rate": 3.2250000000000005e-05,
      "loss": 4.2487,
      "step": 259
    },
    {
      "bce_eff_w": 0.13,
      "bce_loss": -0.4648902416229248,
      "epoch": 1.295,
      "step": 259
    },
    {
      "epoch": 1.3,
      "grad_norm": 18.544769287109375,
      "learning_rate": 3.2375e-05,
      "loss": 4.5711,
      "step": 260
    },
    {
      "bce_eff_w": 0.1305,
      "bce_loss": -0.46550774574279785,
      "epoch": 1.3,
      "step": 260
    },
    {
      "epoch": 1.305,
      "grad_norm": 23.416196823120117,
      "learning_rate": 3.2500000000000004e-05,
      "loss": 4.1828,
      "step": 261
    },
    {
      "bce_eff_w": 0.131,
      "bce_loss": -0.46432557702064514,
      "epoch": 1.305,
      "step": 261
    },
    {
      "epoch": 1.31,
      "grad_norm": 22.82514190673828,
      "learning_rate": 3.2625e-05,
      "loss": 4.6837,
      "step": 262
    },
    {
      "bce_eff_w": 0.1315,
      "bce_loss": -0.46472492814064026,
      "epoch": 1.31,
      "step": 262
    },
    {
      "epoch": 1.315,
      "grad_norm": 17.654857635498047,
      "learning_rate": 3.275e-05,
      "loss": 5.1308,
      "step": 263
    },
    {
      "bce_eff_w": 0.132,
      "bce_loss": -0.46488016843795776,
      "epoch": 1.315,
      "step": 263
    },
    {
      "epoch": 1.32,
      "grad_norm": 38.641971588134766,
      "learning_rate": 3.2875e-05,
      "loss": 3.6863,
      "step": 264
    },
    {
      "bce_eff_w": 0.1325,
      "bce_loss": -0.46540752053260803,
      "epoch": 1.32,
      "step": 264
    },
    {
      "epoch": 1.325,
      "grad_norm": 18.63744354248047,
      "learning_rate": 3.3e-05,
      "loss": 4.5398,
      "step": 265
    },
    {
      "bce_eff_w": 0.133,
      "bce_loss": -0.46533671021461487,
      "epoch": 1.325,
      "step": 265
    },
    {
      "epoch": 1.33,
      "grad_norm": 24.729095458984375,
      "learning_rate": 3.3125e-05,
      "loss": 4.8207,
      "step": 266
    },
    {
      "bce_eff_w": 0.1335,
      "bce_loss": -0.465234637260437,
      "epoch": 1.33,
      "step": 266
    },
    {
      "epoch": 1.335,
      "grad_norm": 18.844966888427734,
      "learning_rate": 3.325e-05,
      "loss": 5.6818,
      "step": 267
    },
    {
      "bce_eff_w": 0.134,
      "bce_loss": -0.4655004143714905,
      "epoch": 1.335,
      "step": 267
    },
    {
      "epoch": 1.34,
      "grad_norm": 29.17711639404297,
      "learning_rate": 3.3375e-05,
      "loss": 4.4519,
      "step": 268
    },
    {
      "bce_eff_w": 0.1345,
      "bce_loss": -0.4648720920085907,
      "epoch": 1.34,
      "step": 268
    },
    {
      "epoch": 1.345,
      "grad_norm": 31.622453689575195,
      "learning_rate": 3.35e-05,
      "loss": 3.6056,
      "step": 269
    },
    {
      "bce_eff_w": 0.135,
      "bce_loss": -0.4653538763523102,
      "epoch": 1.345,
      "step": 269
    },
    {
      "epoch": 1.35,
      "grad_norm": 26.52191925048828,
      "learning_rate": 3.3625000000000004e-05,
      "loss": 2.8344,
      "step": 270
    },
    {
      "bce_eff_w": 0.1355,
      "bce_loss": -0.46543869376182556,
      "epoch": 1.35,
      "step": 270
    },
    {
      "epoch": 1.355,
      "grad_norm": 23.30450439453125,
      "learning_rate": 3.375000000000001e-05,
      "loss": 4.8713,
      "step": 271
    },
    {
      "bce_eff_w": 0.136,
      "bce_loss": -0.46516183018684387,
      "epoch": 1.355,
      "step": 271
    },
    {
      "epoch": 1.3599999999999999,
      "grad_norm": 24.353254318237305,
      "learning_rate": 3.3875000000000003e-05,
      "loss": 5.0114,
      "step": 272
    },
    {
      "bce_eff_w": 0.1365,
      "bce_loss": -0.46532273292541504,
      "epoch": 1.3599999999999999,
      "step": 272
    },
    {
      "epoch": 1.365,
      "grad_norm": 24.99665069580078,
      "learning_rate": 3.4000000000000007e-05,
      "loss": 6.2388,
      "step": 273
    },
    {
      "bce_eff_w": 0.137,
      "bce_loss": -0.46378636360168457,
      "epoch": 1.365,
      "step": 273
    },
    {
      "epoch": 1.37,
      "grad_norm": 21.411230087280273,
      "learning_rate": 3.4125e-05,
      "loss": 5.1309,
      "step": 274
    },
    {
      "bce_eff_w": 0.1375,
      "bce_loss": -0.4654015004634857,
      "epoch": 1.37,
      "step": 274
    },
    {
      "epoch": 1.375,
      "grad_norm": 27.579544067382812,
      "learning_rate": 3.4250000000000006e-05,
      "loss": 3.2893,
      "step": 275
    },
    {
      "bce_eff_w": 0.13799999999999998,
      "bce_loss": -0.46534261107444763,
      "epoch": 1.375,
      "step": 275
    },
    {
      "epoch": 1.38,
      "grad_norm": 30.765914916992188,
      "learning_rate": 3.4375e-05,
      "loss": 3.4522,
      "step": 276
    },
    {
      "bce_eff_w": 0.1385,
      "bce_loss": -0.4648519456386566,
      "epoch": 1.38,
      "step": 276
    },
    {
      "epoch": 1.385,
      "grad_norm": 25.602991104125977,
      "learning_rate": 3.45e-05,
      "loss": 5.2177,
      "step": 277
    },
    {
      "bce_eff_w": 0.13899999999999998,
      "bce_loss": -0.46520188450813293,
      "epoch": 1.385,
      "step": 277
    },
    {
      "epoch": 1.3900000000000001,
      "grad_norm": 29.258434295654297,
      "learning_rate": 3.4625e-05,
      "loss": 3.1114,
      "step": 278
    },
    {
      "bce_eff_w": 0.1395,
      "bce_loss": -0.4653150737285614,
      "epoch": 1.3900000000000001,
      "step": 278
    },
    {
      "epoch": 1.395,
      "grad_norm": 20.211275100708008,
      "learning_rate": 3.475e-05,
      "loss": 5.8706,
      "step": 279
    },
    {
      "bce_eff_w": 0.13999999999999999,
      "bce_loss": -0.46515795588493347,
      "epoch": 1.395,
      "step": 279
    },
    {
      "epoch": 1.4,
      "grad_norm": 25.150527954101562,
      "learning_rate": 3.4875e-05,
      "loss": 3.7331,
      "step": 280
    },
    {
      "bce_eff_w": 0.1405,
      "bce_loss": -0.46465498208999634,
      "epoch": 1.4,
      "step": 280
    },
    {
      "epoch": 1.405,
      "grad_norm": 25.926349639892578,
      "learning_rate": 3.5e-05,
      "loss": 4.7801,
      "step": 281
    },
    {
      "bce_eff_w": 0.141,
      "bce_loss": -0.4654272198677063,
      "epoch": 1.405,
      "step": 281
    },
    {
      "epoch": 1.41,
      "grad_norm": 18.033903121948242,
      "learning_rate": 3.5125e-05,
      "loss": 5.5661,
      "step": 282
    },
    {
      "bce_eff_w": 0.14150000000000001,
      "bce_loss": -0.46548929810523987,
      "epoch": 1.41,
      "step": 282
    },
    {
      "epoch": 1.415,
      "grad_norm": 27.397212982177734,
      "learning_rate": 3.525e-05,
      "loss": 2.6331,
      "step": 283
    },
    {
      "bce_eff_w": 0.142,
      "bce_loss": -0.4653893709182739,
      "epoch": 1.415,
      "step": 283
    },
    {
      "epoch": 1.42,
      "grad_norm": 29.294607162475586,
      "learning_rate": 3.5375e-05,
      "loss": 3.2536,
      "step": 284
    },
    {
      "bce_eff_w": 0.14250000000000002,
      "bce_loss": -0.46490082144737244,
      "epoch": 1.42,
      "step": 284
    },
    {
      "epoch": 1.425,
      "grad_norm": 29.705354690551758,
      "learning_rate": 3.55e-05,
      "loss": 3.8821,
      "step": 285
    },
    {
      "bce_eff_w": 0.143,
      "bce_loss": -0.4653111696243286,
      "epoch": 1.425,
      "step": 285
    },
    {
      "epoch": 1.43,
      "grad_norm": 33.2081184387207,
      "learning_rate": 3.5625000000000005e-05,
      "loss": 3.666,
      "step": 286
    },
    {
      "bce_eff_w": 0.14350000000000002,
      "bce_loss": -0.46538251638412476,
      "epoch": 1.43,
      "step": 286
    },
    {
      "epoch": 1.435,
      "grad_norm": 21.10721206665039,
      "learning_rate": 3.575e-05,
      "loss": 4.9217,
      "step": 287
    },
    {
      "bce_eff_w": 0.144,
      "bce_loss": -0.46493539214134216,
      "epoch": 1.435,
      "step": 287
    },
    {
      "epoch": 1.44,
      "grad_norm": 15.697203636169434,
      "learning_rate": 3.5875000000000005e-05,
      "loss": 6.3884,
      "step": 288
    },
    {
      "bce_eff_w": 0.14450000000000002,
      "bce_loss": -0.46515902876853943,
      "epoch": 1.44,
      "step": 288
    },
    {
      "epoch": 1.445,
      "grad_norm": 34.04167938232422,
      "learning_rate": 3.6e-05,
      "loss": 2.2123,
      "step": 289
    },
    {
      "bce_eff_w": 0.145,
      "bce_loss": -0.46509477496147156,
      "epoch": 1.445,
      "step": 289
    },
    {
      "epoch": 1.45,
      "grad_norm": 30.307374954223633,
      "learning_rate": 3.6125000000000004e-05,
      "loss": 6.3647,
      "step": 290
    },
    {
      "bce_eff_w": 0.14550000000000002,
      "bce_loss": -0.4653908312320709,
      "epoch": 1.45,
      "step": 290
    },
    {
      "epoch": 1.455,
      "grad_norm": 20.735363006591797,
      "learning_rate": 3.625e-05,
      "loss": 4.8955,
      "step": 291
    },
    {
      "bce_eff_w": 0.146,
      "bce_loss": -0.46544182300567627,
      "epoch": 1.455,
      "step": 291
    },
    {
      "epoch": 1.46,
      "grad_norm": 22.95737075805664,
      "learning_rate": 3.6375e-05,
      "loss": 2.3253,
      "step": 292
    },
    {
      "bce_eff_w": 0.14650000000000002,
      "bce_loss": -0.46540793776512146,
      "epoch": 1.46,
      "step": 292
    },
    {
      "epoch": 1.465,
      "grad_norm": 22.96851921081543,
      "learning_rate": 3.65e-05,
      "loss": 5.4051,
      "step": 293
    },
    {
      "bce_eff_w": 0.147,
      "bce_loss": -0.46432945132255554,
      "epoch": 1.465,
      "step": 293
    },
    {
      "epoch": 1.47,
      "grad_norm": 18.08442497253418,
      "learning_rate": 3.6625e-05,
      "loss": 4.4407,
      "step": 294
    },
    {
      "bce_eff_w": 0.14750000000000002,
      "bce_loss": -0.46424707770347595,
      "epoch": 1.47,
      "step": 294
    },
    {
      "epoch": 1.475,
      "grad_norm": 20.173377990722656,
      "learning_rate": 3.675e-05,
      "loss": 4.2009,
      "step": 295
    },
    {
      "bce_eff_w": 0.148,
      "bce_loss": -0.4650178849697113,
      "epoch": 1.475,
      "step": 295
    },
    {
      "epoch": 1.48,
      "grad_norm": 19.996503829956055,
      "learning_rate": 3.6875e-05,
      "loss": 4.1861,
      "step": 296
    },
    {
      "bce_eff_w": 0.14850000000000002,
      "bce_loss": -0.46537262201309204,
      "epoch": 1.48,
      "step": 296
    },
    {
      "epoch": 1.4849999999999999,
      "grad_norm": 38.552093505859375,
      "learning_rate": 3.7e-05,
      "loss": 3.8766,
      "step": 297
    },
    {
      "bce_eff_w": 0.149,
      "bce_loss": -0.4654339849948883,
      "epoch": 1.4849999999999999,
      "step": 297
    },
    {
      "epoch": 1.49,
      "grad_norm": 30.41377830505371,
      "learning_rate": 3.7125e-05,
      "loss": 3.5591,
      "step": 298
    },
    {
      "bce_eff_w": 0.14950000000000002,
      "bce_loss": -0.46533846855163574,
      "epoch": 1.49,
      "step": 298
    },
    {
      "epoch": 1.495,
      "grad_norm": 20.15133285522461,
      "learning_rate": 3.7250000000000004e-05,
      "loss": 4.737,
      "step": 299
    },
    {
      "bce_eff_w": 0.15000000000000002,
      "bce_loss": -0.4647447466850281,
      "epoch": 1.495,
      "step": 299
    },
    {
      "epoch": 1.5,
      "grad_norm": 13.597869873046875,
      "learning_rate": 3.737500000000001e-05,
      "loss": 6.0709,
      "step": 300
    },
    {
      "bce_eff_w": 0.1505,
      "bce_loss": -0.46540239453315735,
      "epoch": 1.5,
      "step": 300
    },
    {
      "epoch": 1.505,
      "grad_norm": 24.91572380065918,
      "learning_rate": 3.7500000000000003e-05,
      "loss": 3.1193,
      "step": 301
    },
    {
      "bce_eff_w": 0.15100000000000002,
      "bce_loss": -0.46512505412101746,
      "epoch": 1.505,
      "step": 301
    },
    {
      "epoch": 1.51,
      "grad_norm": 36.4630241394043,
      "learning_rate": 3.7625e-05,
      "loss": 3.8613,
      "step": 302
    },
    {
      "bce_eff_w": 0.1515,
      "bce_loss": -0.4654644727706909,
      "epoch": 1.51,
      "step": 302
    },
    {
      "epoch": 1.5150000000000001,
      "grad_norm": 19.667236328125,
      "learning_rate": 3.775e-05,
      "loss": 5.1926,
      "step": 303
    },
    {
      "bce_eff_w": 0.15200000000000002,
      "bce_loss": -0.46444857120513916,
      "epoch": 1.5150000000000001,
      "step": 303
    },
    {
      "epoch": 1.52,
      "grad_norm": 14.419787406921387,
      "learning_rate": 3.7875e-05,
      "loss": 5.9241,
      "step": 304
    },
    {
      "bce_eff_w": 0.1525,
      "bce_loss": -0.4645536243915558,
      "epoch": 1.52,
      "step": 304
    },
    {
      "epoch": 1.525,
      "grad_norm": 22.79183578491211,
      "learning_rate": 3.8e-05,
      "loss": 3.7963,
      "step": 305
    },
    {
      "bce_eff_w": 0.15300000000000002,
      "bce_loss": -0.4654006361961365,
      "epoch": 1.525,
      "step": 305
    },
    {
      "epoch": 1.53,
      "grad_norm": 19.939990997314453,
      "learning_rate": 3.8125e-05,
      "loss": 4.6021,
      "step": 306
    },
    {
      "bce_eff_w": 0.1535,
      "bce_loss": -0.46500006318092346,
      "epoch": 1.53,
      "step": 306
    },
    {
      "epoch": 1.5350000000000001,
      "grad_norm": 26.720670700073242,
      "learning_rate": 3.825e-05,
      "loss": 1.9125,
      "step": 307
    },
    {
      "bce_eff_w": 0.15400000000000003,
      "bce_loss": -0.4647410809993744,
      "epoch": 1.5350000000000001,
      "step": 307
    },
    {
      "epoch": 1.54,
      "grad_norm": 27.021249771118164,
      "learning_rate": 3.8375e-05,
      "loss": 5.2195,
      "step": 308
    },
    {
      "bce_eff_w": 0.1545,
      "bce_loss": -0.4653771221637726,
      "epoch": 1.54,
      "step": 308
    },
    {
      "epoch": 1.545,
      "grad_norm": 17.404525756835938,
      "learning_rate": 3.85e-05,
      "loss": 4.6867,
      "step": 309
    },
    {
      "bce_eff_w": 0.15500000000000003,
      "bce_loss": -0.46500518918037415,
      "epoch": 1.545,
      "step": 309
    },
    {
      "epoch": 1.55,
      "grad_norm": 21.20182228088379,
      "learning_rate": 3.8625e-05,
      "loss": 3.2962,
      "step": 310
    },
    {
      "bce_eff_w": 0.1555,
      "bce_loss": -0.46481603384017944,
      "epoch": 1.55,
      "step": 310
    },
    {
      "epoch": 1.5550000000000002,
      "grad_norm": 20.58338737487793,
      "learning_rate": 3.875e-05,
      "loss": 4.0079,
      "step": 311
    },
    {
      "bce_eff_w": 0.15600000000000003,
      "bce_loss": -0.4644995331764221,
      "epoch": 1.5550000000000002,
      "step": 311
    },
    {
      "epoch": 1.56,
      "grad_norm": 16.117883682250977,
      "learning_rate": 3.8875e-05,
      "loss": 6.2601,
      "step": 312
    },
    {
      "bce_eff_w": 0.1565,
      "bce_loss": -0.46510767936706543,
      "epoch": 1.56,
      "step": 312
    },
    {
      "epoch": 1.565,
      "grad_norm": 21.062610626220703,
      "learning_rate": 3.9000000000000006e-05,
      "loss": 4.2146,
      "step": 313
    },
    {
      "bce_eff_w": 0.15700000000000003,
      "bce_loss": -0.46540531516075134,
      "epoch": 1.565,
      "step": 313
    },
    {
      "epoch": 1.5699999999999998,
      "grad_norm": 31.077693939208984,
      "learning_rate": 3.9125e-05,
      "loss": 2.3619,
      "step": 314
    },
    {
      "bce_eff_w": 0.1575,
      "bce_loss": -0.4653918445110321,
      "epoch": 1.5699999999999998,
      "step": 314
    },
    {
      "epoch": 1.575,
      "grad_norm": 27.872079849243164,
      "learning_rate": 3.9250000000000005e-05,
      "loss": 3.6689,
      "step": 315
    },
    {
      "bce_eff_w": 0.15800000000000003,
      "bce_loss": -0.46397385001182556,
      "epoch": 1.575,
      "step": 315
    },
    {
      "epoch": 1.58,
      "grad_norm": 19.025827407836914,
      "learning_rate": 3.9375e-05,
      "loss": 3.6334,
      "step": 316
    },
    {
      "bce_eff_w": 0.1585,
      "bce_loss": -0.46544283628463745,
      "epoch": 1.58,
      "step": 316
    },
    {
      "epoch": 1.585,
      "grad_norm": 19.200056076049805,
      "learning_rate": 3.9500000000000005e-05,
      "loss": 5.4257,
      "step": 317
    },
    {
      "bce_eff_w": 0.15900000000000003,
      "bce_loss": -0.46478861570358276,
      "epoch": 1.585,
      "step": 317
    },
    {
      "epoch": 1.5899999999999999,
      "grad_norm": 19.648269653320312,
      "learning_rate": 3.9625e-05,
      "loss": 3.8482,
      "step": 318
    },
    {
      "bce_eff_w": 0.1595,
      "bce_loss": -0.4648369550704956,
      "epoch": 1.5899999999999999,
      "step": 318
    },
    {
      "epoch": 1.595,
      "grad_norm": 19.29697608947754,
      "learning_rate": 3.9750000000000004e-05,
      "loss": 4.5805,
      "step": 319
    },
    {
      "bce_eff_w": 0.16000000000000003,
      "bce_loss": -0.46438029408454895,
      "epoch": 1.595,
      "step": 319
    },
    {
      "epoch": 1.6,
      "grad_norm": 13.475461959838867,
      "learning_rate": 3.9875e-05,
      "loss": 6.0531,
      "step": 320
    },
    {
      "bce_eff_w": 0.1605,
      "bce_loss": -0.46485957503318787,
      "epoch": 1.6,
      "step": 320
    },
    {
      "epoch": 1.605,
      "grad_norm": 24.701030731201172,
      "learning_rate": 4e-05,
      "loss": 3.3975,
      "step": 321
    },
    {
      "bce_eff_w": 0.16100000000000003,
      "bce_loss": -0.46547892689704895,
      "epoch": 1.605,
      "step": 321
    },
    {
      "epoch": 1.6099999999999999,
      "grad_norm": 27.799240112304688,
      "learning_rate": 4.0125e-05,
      "loss": 2.8084,
      "step": 322
    },
    {
      "bce_eff_w": 0.1615,
      "bce_loss": -0.4644296169281006,
      "epoch": 1.6099999999999999,
      "step": 322
    },
    {
      "epoch": 1.615,
      "grad_norm": 19.385934829711914,
      "learning_rate": 4.025e-05,
      "loss": 5.2634,
      "step": 323
    },
    {
      "bce_eff_w": 0.16200000000000003,
      "bce_loss": -0.4653349220752716,
      "epoch": 1.615,
      "step": 323
    },
    {
      "epoch": 1.62,
      "grad_norm": 27.713489532470703,
      "learning_rate": 4.0375e-05,
      "loss": 3.6372,
      "step": 324
    },
    {
      "bce_eff_w": 0.1625,
      "bce_loss": -0.465019166469574,
      "epoch": 1.62,
      "step": 324
    },
    {
      "epoch": 1.625,
      "grad_norm": 36.6516227722168,
      "learning_rate": 4.05e-05,
      "loss": 2.8122,
      "step": 325
    },
    {
      "bce_eff_w": 0.163,
      "bce_loss": -0.46523669362068176,
      "epoch": 1.625,
      "step": 325
    },
    {
      "epoch": 1.63,
      "grad_norm": 16.184045791625977,
      "learning_rate": 4.0625000000000005e-05,
      "loss": 4.8976,
      "step": 326
    },
    {
      "bce_eff_w": 0.1635,
      "bce_loss": -0.46551600098609924,
      "epoch": 1.63,
      "step": 326
    },
    {
      "epoch": 1.635,
      "grad_norm": 15.827122688293457,
      "learning_rate": 4.075e-05,
      "loss": 4.8948,
      "step": 327
    },
    {
      "bce_eff_w": 0.164,
      "bce_loss": -0.46464627981185913,
      "epoch": 1.635,
      "step": 327
    },
    {
      "epoch": 1.6400000000000001,
      "grad_norm": 12.834779739379883,
      "learning_rate": 4.0875000000000004e-05,
      "loss": 6.103,
      "step": 328
    },
    {
      "bce_eff_w": 0.1645,
      "bce_loss": -0.46514344215393066,
      "epoch": 1.6400000000000001,
      "step": 328
    },
    {
      "epoch": 1.645,
      "grad_norm": 23.937793731689453,
      "learning_rate": 4.1e-05,
      "loss": 3.9793,
      "step": 329
    },
    {
      "bce_eff_w": 0.165,
      "bce_loss": -0.4646380841732025,
      "epoch": 1.645,
      "step": 329
    },
    {
      "epoch": 1.65,
      "grad_norm": 20.258216857910156,
      "learning_rate": 4.1125000000000004e-05,
      "loss": 3.7543,
      "step": 330
    },
    {
      "bce_eff_w": 0.1655,
      "bce_loss": -0.46544620394706726,
      "epoch": 1.65,
      "step": 330
    },
    {
      "epoch": 1.655,
      "grad_norm": 22.937803268432617,
      "learning_rate": 4.125e-05,
      "loss": 3.6204,
      "step": 331
    },
    {
      "bce_eff_w": 0.166,
      "bce_loss": -0.4645216166973114,
      "epoch": 1.655,
      "step": 331
    },
    {
      "epoch": 1.6600000000000001,
      "grad_norm": 12.45897388458252,
      "learning_rate": 4.1375e-05,
      "loss": 5.7825,
      "step": 332
    },
    {
      "bce_eff_w": 0.1665,
      "bce_loss": -0.46511051058769226,
      "epoch": 1.6600000000000001,
      "step": 332
    },
    {
      "epoch": 1.665,
      "grad_norm": 22.351648330688477,
      "learning_rate": 4.15e-05,
      "loss": 4.0481,
      "step": 333
    },
    {
      "bce_eff_w": 0.167,
      "bce_loss": -0.46510979533195496,
      "epoch": 1.665,
      "step": 333
    },
    {
      "epoch": 1.67,
      "grad_norm": 29.896183013916016,
      "learning_rate": 4.1625e-05,
      "loss": 3.7962,
      "step": 334
    },
    {
      "bce_eff_w": 0.1675,
      "bce_loss": -0.46548089385032654,
      "epoch": 1.67,
      "step": 334
    },
    {
      "epoch": 1.675,
      "grad_norm": 25.180055618286133,
      "learning_rate": 4.175e-05,
      "loss": 4.6326,
      "step": 335
    },
    {
      "bce_eff_w": 0.168,
      "bce_loss": -0.465323805809021,
      "epoch": 1.675,
      "step": 335
    },
    {
      "epoch": 1.6800000000000002,
      "grad_norm": 29.356739044189453,
      "learning_rate": 4.1875e-05,
      "loss": 2.5535,
      "step": 336
    },
    {
      "bce_eff_w": 0.1685,
      "bce_loss": -0.46538031101226807,
      "epoch": 1.6800000000000002,
      "step": 336
    },
    {
      "epoch": 1.685,
      "grad_norm": 14.723274230957031,
      "learning_rate": 4.2e-05,
      "loss": 5.32,
      "step": 337
    },
    {
      "bce_eff_w": 0.169,
      "bce_loss": -0.465254545211792,
      "epoch": 1.685,
      "step": 337
    },
    {
      "epoch": 1.69,
      "grad_norm": 22.878007888793945,
      "learning_rate": 4.2125e-05,
      "loss": 2.0768,
      "step": 338
    },
    {
      "bce_eff_w": 0.1695,
      "bce_loss": -0.4646029770374298,
      "epoch": 1.69,
      "step": 338
    },
    {
      "epoch": 1.6949999999999998,
      "grad_norm": 12.545848846435547,
      "learning_rate": 4.2250000000000004e-05,
      "loss": 5.7901,
      "step": 339
    },
    {
      "bce_eff_w": 0.17,
      "bce_loss": -0.46438920497894287,
      "epoch": 1.6949999999999998,
      "step": 339
    },
    {
      "epoch": 1.7,
      "grad_norm": 21.343053817749023,
      "learning_rate": 4.237500000000001e-05,
      "loss": 3.1689,
      "step": 340
    },
    {
      "bce_eff_w": 0.1705,
      "bce_loss": -0.4647005498409271,
      "epoch": 1.7,
      "step": 340
    },
    {
      "epoch": 1.705,
      "grad_norm": 19.594181060791016,
      "learning_rate": 4.25e-05,
      "loss": 3.0199,
      "step": 341
    },
    {
      "bce_eff_w": 0.171,
      "bce_loss": -0.4652271866798401,
      "epoch": 1.705,
      "step": 341
    },
    {
      "epoch": 1.71,
      "grad_norm": 25.538827896118164,
      "learning_rate": 4.2625000000000006e-05,
      "loss": 1.9376,
      "step": 342
    },
    {
      "bce_eff_w": 0.1715,
      "bce_loss": -0.46497124433517456,
      "epoch": 1.71,
      "step": 342
    },
    {
      "epoch": 1.7149999999999999,
      "grad_norm": 35.33733367919922,
      "learning_rate": 4.275e-05,
      "loss": 3.5171,
      "step": 343
    },
    {
      "bce_eff_w": 0.17200000000000001,
      "bce_loss": -0.4653560221195221,
      "epoch": 1.7149999999999999,
      "step": 343
    },
    {
      "epoch": 1.72,
      "grad_norm": 23.652050018310547,
      "learning_rate": 4.2875000000000005e-05,
      "loss": 3.4695,
      "step": 344
    },
    {
      "bce_eff_w": 0.17250000000000001,
      "bce_loss": -0.46536964178085327,
      "epoch": 1.72,
      "step": 344
    },
    {
      "epoch": 1.725,
      "grad_norm": 45.07368850708008,
      "learning_rate": 4.3e-05,
      "loss": 3.4832,
      "step": 345
    },
    {
      "bce_eff_w": 0.17300000000000001,
      "bce_loss": -0.4646291732788086,
      "epoch": 1.725,
      "step": 345
    },
    {
      "epoch": 1.73,
      "grad_norm": 20.78664207458496,
      "learning_rate": 4.3125000000000005e-05,
      "loss": 3.4084,
      "step": 346
    },
    {
      "bce_eff_w": 0.17350000000000002,
      "bce_loss": -0.4650375247001648,
      "epoch": 1.73,
      "step": 346
    },
    {
      "epoch": 1.7349999999999999,
      "grad_norm": 19.15228843688965,
      "learning_rate": 4.325e-05,
      "loss": 4.6779,
      "step": 347
    },
    {
      "bce_eff_w": 0.17400000000000002,
      "bce_loss": -0.46507999300956726,
      "epoch": 1.7349999999999999,
      "step": 347
    },
    {
      "epoch": 1.74,
      "grad_norm": 24.860271453857422,
      "learning_rate": 4.3375000000000004e-05,
      "loss": 2.0271,
      "step": 348
    },
    {
      "bce_eff_w": 0.17450000000000002,
      "bce_loss": -0.4650293290615082,
      "epoch": 1.74,
      "step": 348
    },
    {
      "epoch": 1.745,
      "grad_norm": 30.818418502807617,
      "learning_rate": 4.35e-05,
      "loss": 3.2201,
      "step": 349
    },
    {
      "bce_eff_w": 0.17500000000000002,
      "bce_loss": -0.4653792083263397,
      "epoch": 1.745,
      "step": 349
    },
    {
      "epoch": 1.75,
      "grad_norm": 25.935386657714844,
      "learning_rate": 4.3625e-05,
      "loss": 3.234,
      "step": 350
    },
    {
      "bce_eff_w": 0.1755,
      "bce_loss": -0.46524563431739807,
      "epoch": 1.75,
      "step": 350
    },
    {
      "epoch": 1.755,
      "grad_norm": 25.777585983276367,
      "learning_rate": 4.375e-05,
      "loss": 2.9454,
      "step": 351
    },
    {
      "bce_eff_w": 0.17600000000000002,
      "bce_loss": -0.46526652574539185,
      "epoch": 1.755,
      "step": 351
    },
    {
      "epoch": 1.76,
      "grad_norm": 18.060007095336914,
      "learning_rate": 4.3875e-05,
      "loss": 5.0378,
      "step": 352
    },
    {
      "bce_eff_w": 0.1765,
      "bce_loss": -0.46514564752578735,
      "epoch": 1.76,
      "step": 352
    },
    {
      "epoch": 1.7650000000000001,
      "grad_norm": 25.968685150146484,
      "learning_rate": 4.4000000000000006e-05,
      "loss": 2.4508,
      "step": 353
    },
    {
      "bce_eff_w": 0.17700000000000002,
      "bce_loss": -0.46434685587882996,
      "epoch": 1.7650000000000001,
      "step": 353
    },
    {
      "epoch": 1.77,
      "grad_norm": 21.2624568939209,
      "learning_rate": 4.4125e-05,
      "loss": 2.951,
      "step": 354
    },
    {
      "bce_eff_w": 0.1775,
      "bce_loss": -0.4653729498386383,
      "epoch": 1.77,
      "step": 354
    },
    {
      "epoch": 1.775,
      "grad_norm": 27.346485137939453,
      "learning_rate": 4.4250000000000005e-05,
      "loss": 3.5064,
      "step": 355
    },
    {
      "bce_eff_w": 0.17800000000000002,
      "bce_loss": -0.4652723968029022,
      "epoch": 1.775,
      "step": 355
    },
    {
      "epoch": 1.78,
      "grad_norm": 31.859973907470703,
      "learning_rate": 4.4375e-05,
      "loss": 1.5649,
      "step": 356
    },
    {
      "bce_eff_w": 0.1785,
      "bce_loss": -0.46537303924560547,
      "epoch": 1.78,
      "step": 356
    },
    {
      "epoch": 1.7850000000000001,
      "grad_norm": 25.097183227539062,
      "learning_rate": 4.4500000000000004e-05,
      "loss": 2.5492,
      "step": 357
    },
    {
      "bce_eff_w": 0.17900000000000002,
      "bce_loss": -0.46490949392318726,
      "epoch": 1.7850000000000001,
      "step": 357
    },
    {
      "epoch": 1.79,
      "grad_norm": 18.728708267211914,
      "learning_rate": 4.4625e-05,
      "loss": 5.1583,
      "step": 358
    },
    {
      "bce_eff_w": 0.1795,
      "bce_loss": -0.4644683599472046,
      "epoch": 1.79,
      "step": 358
    },
    {
      "epoch": 1.795,
      "grad_norm": 25.00967788696289,
      "learning_rate": 4.4750000000000004e-05,
      "loss": 2.4899,
      "step": 359
    },
    {
      "bce_eff_w": 0.18000000000000002,
      "bce_loss": -0.4652426838874817,
      "epoch": 1.795,
      "step": 359
    },
    {
      "epoch": 1.8,
      "grad_norm": 26.712074279785156,
      "learning_rate": 4.4875e-05,
      "loss": 1.5141,
      "step": 360
    },
    {
      "bce_eff_w": 0.1805,
      "bce_loss": -0.465224951505661,
      "epoch": 1.8,
      "step": 360
    },
    {
      "epoch": 1.8050000000000002,
      "grad_norm": 32.81074523925781,
      "learning_rate": 4.5e-05,
      "loss": 3.3749,
      "step": 361
    },
    {
      "bce_eff_w": 0.18100000000000002,
      "bce_loss": -0.4643028676509857,
      "epoch": 1.8050000000000002,
      "step": 361
    },
    {
      "epoch": 1.81,
      "grad_norm": 12.184688568115234,
      "learning_rate": 4.5125e-05,
      "loss": 5.5607,
      "step": 362
    },
    {
      "bce_eff_w": 0.1815,
      "bce_loss": -0.4653644263744354,
      "epoch": 1.81,
      "step": 362
    },
    {
      "epoch": 1.815,
      "grad_norm": 24.936059951782227,
      "learning_rate": 4.525e-05,
      "loss": 2.3121,
      "step": 363
    },
    {
      "bce_eff_w": 0.18200000000000002,
      "bce_loss": -0.46488410234451294,
      "epoch": 1.815,
      "step": 363
    },
    {
      "epoch": 1.8199999999999998,
      "grad_norm": 27.731555938720703,
      "learning_rate": 4.5375e-05,
      "loss": 2.1718,
      "step": 364
    },
    {
      "bce_eff_w": 0.1825,
      "bce_loss": -0.4644760191440582,
      "epoch": 1.8199999999999998,
      "step": 364
    },
    {
      "epoch": 1.825,
      "grad_norm": 31.16444206237793,
      "learning_rate": 4.55e-05,
      "loss": 4.7581,
      "step": 365
    },
    {
      "bce_eff_w": 0.18300000000000002,
      "bce_loss": -0.4653710126876831,
      "epoch": 1.825,
      "step": 365
    },
    {
      "epoch": 1.83,
      "grad_norm": 16.59871482849121,
      "learning_rate": 4.5625e-05,
      "loss": 4.009,
      "step": 366
    },
    {
      "bce_eff_w": 0.1835,
      "bce_loss": -0.46474045515060425,
      "epoch": 1.83,
      "step": 366
    },
    {
      "epoch": 1.835,
      "grad_norm": 17.561458587646484,
      "learning_rate": 4.575e-05,
      "loss": 5.0412,
      "step": 367
    },
    {
      "bce_eff_w": 0.18400000000000002,
      "bce_loss": -0.4651150703430176,
      "epoch": 1.835,
      "step": 367
    },
    {
      "epoch": 1.8399999999999999,
      "grad_norm": 28.793062210083008,
      "learning_rate": 4.5875000000000004e-05,
      "loss": 3.5887,
      "step": 368
    },
    {
      "bce_eff_w": 0.1845,
      "bce_loss": -0.465341180562973,
      "epoch": 1.8399999999999999,
      "step": 368
    },
    {
      "epoch": 1.845,
      "grad_norm": 26.811540603637695,
      "learning_rate": 4.600000000000001e-05,
      "loss": 1.8127,
      "step": 369
    },
    {
      "bce_eff_w": 0.18500000000000003,
      "bce_loss": -0.46538591384887695,
      "epoch": 1.845,
      "step": 369
    },
    {
      "epoch": 1.85,
      "grad_norm": 28.516376495361328,
      "learning_rate": 4.6125e-05,
      "loss": 3.1219,
      "step": 370
    },
    {
      "bce_eff_w": 0.1855,
      "bce_loss": -0.465229332447052,
      "epoch": 1.85,
      "step": 370
    },
    {
      "epoch": 1.855,
      "grad_norm": 30.089582443237305,
      "learning_rate": 4.6250000000000006e-05,
      "loss": 2.2673,
      "step": 371
    },
    {
      "bce_eff_w": 0.18600000000000003,
      "bce_loss": -0.464997261762619,
      "epoch": 1.855,
      "step": 371
    },
    {
      "epoch": 1.8599999999999999,
      "grad_norm": 26.9233455657959,
      "learning_rate": 4.6375e-05,
      "loss": 2.6958,
      "step": 372
    },
    {
      "bce_eff_w": 0.1865,
      "bce_loss": -0.4653021991252899,
      "epoch": 1.8599999999999999,
      "step": 372
    },
    {
      "epoch": 1.865,
      "grad_norm": 27.83464813232422,
      "learning_rate": 4.6500000000000005e-05,
      "loss": 4.7987,
      "step": 373
    },
    {
      "bce_eff_w": 0.18700000000000003,
      "bce_loss": -0.46543988585472107,
      "epoch": 1.865,
      "step": 373
    },
    {
      "epoch": 1.87,
      "grad_norm": 23.70186424255371,
      "learning_rate": 4.6625e-05,
      "loss": 3.4123,
      "step": 374
    },
    {
      "bce_eff_w": 0.1875,
      "bce_loss": -0.46547403931617737,
      "epoch": 1.87,
      "step": 374
    },
    {
      "epoch": 1.875,
      "grad_norm": 36.473358154296875,
      "learning_rate": 4.6750000000000005e-05,
      "loss": 3.5635,
      "step": 375
    },
    {
      "bce_eff_w": 0.188,
      "bce_loss": -0.4638841152191162,
      "epoch": 1.875,
      "step": 375
    },
    {
      "epoch": 1.88,
      "grad_norm": 18.4928035736084,
      "learning_rate": 4.6875e-05,
      "loss": 2.2979,
      "step": 376
    },
    {
      "bce_eff_w": 0.1885,
      "bce_loss": -0.4626108705997467,
      "epoch": 1.88,
      "step": 376
    },
    {
      "epoch": 1.885,
      "grad_norm": 22.801549911499023,
      "learning_rate": 4.7e-05,
      "loss": 2.8957,
      "step": 377
    },
    {
      "bce_eff_w": 0.189,
      "bce_loss": -0.4652440845966339,
      "epoch": 1.885,
      "step": 377
    },
    {
      "epoch": 1.8900000000000001,
      "grad_norm": 26.306947708129883,
      "learning_rate": 4.7125e-05,
      "loss": 2.0529,
      "step": 378
    },
    {
      "bce_eff_w": 0.1895,
      "bce_loss": -0.46497562527656555,
      "epoch": 1.8900000000000001,
      "step": 378
    },
    {
      "epoch": 1.895,
      "grad_norm": 28.801061630249023,
      "learning_rate": 4.7249999999999997e-05,
      "loss": 3.3844,
      "step": 379
    },
    {
      "bce_eff_w": 0.19,
      "bce_loss": -0.4653811454772949,
      "epoch": 1.895,
      "step": 379
    },
    {
      "epoch": 1.9,
      "grad_norm": 27.77256202697754,
      "learning_rate": 4.7375e-05,
      "loss": 2.4633,
      "step": 380
    },
    {
      "bce_eff_w": 0.1905,
      "bce_loss": -0.4653913080692291,
      "epoch": 1.9,
      "step": 380
    },
    {
      "epoch": 1.905,
      "grad_norm": 30.195146560668945,
      "learning_rate": 4.75e-05,
      "loss": 2.085,
      "step": 381
    },
    {
      "bce_eff_w": 0.191,
      "bce_loss": -0.46503961086273193,
      "epoch": 1.905,
      "step": 381
    },
    {
      "epoch": 1.9100000000000001,
      "grad_norm": 30.423343658447266,
      "learning_rate": 4.7625000000000006e-05,
      "loss": 3.22,
      "step": 382
    },
    {
      "bce_eff_w": 0.1915,
      "bce_loss": -0.46539306640625,
      "epoch": 1.9100000000000001,
      "step": 382
    },
    {
      "epoch": 1.915,
      "grad_norm": 25.457504272460938,
      "learning_rate": 4.775e-05,
      "loss": 1.2166,
      "step": 383
    },
    {
      "bce_eff_w": 0.192,
      "bce_loss": -0.4652981162071228,
      "epoch": 1.915,
      "step": 383
    },
    {
      "epoch": 1.92,
      "grad_norm": 23.935314178466797,
      "learning_rate": 4.7875000000000005e-05,
      "loss": 1.8688,
      "step": 384
    },
    {
      "bce_eff_w": 0.1925,
      "bce_loss": -0.46472862362861633,
      "epoch": 1.92,
      "step": 384
    },
    {
      "epoch": 1.925,
      "grad_norm": 37.55683517456055,
      "learning_rate": 4.8e-05,
      "loss": 1.5527,
      "step": 385
    },
    {
      "bce_eff_w": 0.193,
      "bce_loss": -0.46420061588287354,
      "epoch": 1.925,
      "step": 385
    },
    {
      "epoch": 1.9300000000000002,
      "grad_norm": 20.444141387939453,
      "learning_rate": 4.8125000000000004e-05,
      "loss": 2.5064,
      "step": 386
    },
    {
      "bce_eff_w": 0.1935,
      "bce_loss": -0.46301016211509705,
      "epoch": 1.9300000000000002,
      "step": 386
    },
    {
      "epoch": 1.935,
      "grad_norm": 22.201828002929688,
      "learning_rate": 4.825e-05,
      "loss": 4.8092,
      "step": 387
    },
    {
      "bce_eff_w": 0.194,
      "bce_loss": -0.46547213196754456,
      "epoch": 1.935,
      "step": 387
    },
    {
      "epoch": 1.94,
      "grad_norm": 20.324295043945312,
      "learning_rate": 4.8375000000000004e-05,
      "loss": 4.3398,
      "step": 388
    },
    {
      "bce_eff_w": 0.1945,
      "bce_loss": -0.4652888774871826,
      "epoch": 1.94,
      "step": 388
    },
    {
      "epoch": 1.9449999999999998,
      "grad_norm": 30.073955535888672,
      "learning_rate": 4.85e-05,
      "loss": 1.9476,
      "step": 389
    },
    {
      "bce_eff_w": 0.195,
      "bce_loss": -0.46541866660118103,
      "epoch": 1.9449999999999998,
      "step": 389
    },
    {
      "epoch": 1.95,
      "grad_norm": 20.04509162902832,
      "learning_rate": 4.8625e-05,
      "loss": 4.8108,
      "step": 390
    },
    {
      "bce_eff_w": 0.1955,
      "bce_loss": -0.4653654992580414,
      "epoch": 1.95,
      "step": 390
    },
    {
      "epoch": 1.955,
      "grad_norm": 38.52975845336914,
      "learning_rate": 4.875e-05,
      "loss": 2.7792,
      "step": 391
    },
    {
      "bce_eff_w": 0.196,
      "bce_loss": -0.46371370553970337,
      "epoch": 1.955,
      "step": 391
    },
    {
      "epoch": 1.96,
      "grad_norm": 26.080036163330078,
      "learning_rate": 4.8875e-05,
      "loss": 3.6386,
      "step": 392
    },
    {
      "bce_eff_w": 0.1965,
      "bce_loss": -0.46527573466300964,
      "epoch": 1.96,
      "step": 392
    },
    {
      "epoch": 1.9649999999999999,
      "grad_norm": 20.641042709350586,
      "learning_rate": 4.9e-05,
      "loss": 1.877,
      "step": 393
    },
    {
      "bce_eff_w": 0.197,
      "bce_loss": -0.4652210474014282,
      "epoch": 1.9649999999999999,
      "step": 393
    },
    {
      "epoch": 1.97,
      "grad_norm": 28.673498153686523,
      "learning_rate": 4.9125e-05,
      "loss": 3.2855,
      "step": 394
    },
    {
      "bce_eff_w": 0.1975,
      "bce_loss": -0.46530425548553467,
      "epoch": 1.97,
      "step": 394
    },
    {
      "epoch": 1.975,
      "grad_norm": 29.259654998779297,
      "learning_rate": 4.9250000000000004e-05,
      "loss": 3.2672,
      "step": 395
    },
    {
      "bce_eff_w": 0.198,
      "bce_loss": -0.46459245681762695,
      "epoch": 1.975,
      "step": 395
    },
    {
      "epoch": 1.98,
      "grad_norm": 30.25040626525879,
      "learning_rate": 4.937500000000001e-05,
      "loss": 4.4825,
      "step": 396
    },
    {
      "bce_eff_w": 0.1985,
      "bce_loss": -0.4654204249382019,
      "epoch": 1.98,
      "step": 396
    },
    {
      "epoch": 1.9849999999999999,
      "grad_norm": 21.8668155670166,
      "learning_rate": 4.9500000000000004e-05,
      "loss": 3.2379,
      "step": 397
    },
    {
      "bce_eff_w": 0.199,
      "bce_loss": -0.4639120399951935,
      "epoch": 1.9849999999999999,
      "step": 397
    },
    {
      "epoch": 1.99,
      "grad_norm": 21.161806106567383,
      "learning_rate": 4.962500000000001e-05,
      "loss": 3.0393,
      "step": 398
    },
    {
      "bce_eff_w": 0.1995,
      "bce_loss": -0.4646475911140442,
      "epoch": 1.99,
      "step": 398
    },
    {
      "epoch": 1.995,
      "grad_norm": 38.0291633605957,
      "learning_rate": 4.975e-05,
      "loss": 3.1684,
      "step": 399
    },
    {
      "bce_eff_w": 0.2,
      "bce_loss": -0.46503692865371704,
      "epoch": 1.995,
      "step": 399
    },
    {
      "epoch": 2.0,
      "grad_norm": 32.66402816772461,
      "learning_rate": 4.9875000000000006e-05,
      "loss": 2.8085,
      "step": 400
    },
    {
      "bce_eff_w": 0.20015000000000002,
      "bce_loss": -0.46412134170532227,
      "epoch": 2.0,
      "step": 400
    },
    {
      "epoch": 2.005,
      "grad_norm": 15.615931510925293,
      "learning_rate": 5e-05,
      "loss": 3.4506,
      "step": 401
    },
    {
      "bce_eff_w": 0.2003,
      "bce_loss": -0.4652916193008423,
      "epoch": 2.005,
      "step": 401
    },
    {
      "epoch": 2.01,
      "grad_norm": 23.286422729492188,
      "learning_rate": 4.9968750000000005e-05,
      "loss": 2.7708,
      "step": 402
    },
    {
      "bce_eff_w": 0.20045000000000002,
      "bce_loss": -0.46439722180366516,
      "epoch": 2.01,
      "step": 402
    },
    {
      "epoch": 2.015,
      "grad_norm": 22.407835006713867,
      "learning_rate": 4.99375e-05,
      "loss": 5.3234,
      "step": 403
    },
    {
      "bce_eff_w": 0.2006,
      "bce_loss": -0.46507641673088074,
      "epoch": 2.015,
      "step": 403
    },
    {
      "epoch": 2.02,
      "grad_norm": 22.24904441833496,
      "learning_rate": 4.9906250000000004e-05,
      "loss": 3.8882,
      "step": 404
    },
    {
      "bce_eff_w": 0.20075,
      "bce_loss": -0.46401986479759216,
      "epoch": 2.02,
      "step": 404
    },
    {
      "epoch": 2.025,
      "grad_norm": 17.33360481262207,
      "learning_rate": 4.9875000000000006e-05,
      "loss": 2.9772,
      "step": 405
    },
    {
      "bce_eff_w": 0.20090000000000002,
      "bce_loss": -0.4651729166507721,
      "epoch": 2.025,
      "step": 405
    },
    {
      "epoch": 2.03,
      "grad_norm": 30.8370361328125,
      "learning_rate": 4.984375e-05,
      "loss": 2.0601,
      "step": 406
    },
    {
      "bce_eff_w": 0.20105,
      "bce_loss": -0.46519720554351807,
      "epoch": 2.03,
      "step": 406
    },
    {
      "epoch": 2.035,
      "grad_norm": 35.95887756347656,
      "learning_rate": 4.98125e-05,
      "loss": 0.7649,
      "step": 407
    },
    {
      "bce_eff_w": 0.20120000000000002,
      "bce_loss": -0.46504148840904236,
      "epoch": 2.035,
      "step": 407
    },
    {
      "epoch": 2.04,
      "grad_norm": 36.829124450683594,
      "learning_rate": 4.978125e-05,
      "loss": 2.1233,
      "step": 408
    },
    {
      "bce_eff_w": 0.20135,
      "bce_loss": -0.46506837010383606,
      "epoch": 2.04,
      "step": 408
    },
    {
      "epoch": 2.045,
      "grad_norm": 21.492172241210938,
      "learning_rate": 4.975e-05,
      "loss": 4.6858,
      "step": 409
    },
    {
      "bce_eff_w": 0.2015,
      "bce_loss": -0.4652943015098572,
      "epoch": 2.045,
      "step": 409
    },
    {
      "epoch": 2.05,
      "grad_norm": 20.936758041381836,
      "learning_rate": 4.9718750000000006e-05,
      "loss": 2.8207,
      "step": 410
    },
    {
      "bce_eff_w": 0.20165000000000002,
      "bce_loss": -0.46353596448898315,
      "epoch": 2.05,
      "step": 410
    },
    {
      "epoch": 2.055,
      "grad_norm": 21.93984603881836,
      "learning_rate": 4.96875e-05,
      "loss": 2.1533,
      "step": 411
    },
    {
      "bce_eff_w": 0.2018,
      "bce_loss": -0.46515390276908875,
      "epoch": 2.055,
      "step": 411
    },
    {
      "epoch": 2.06,
      "grad_norm": 28.867069244384766,
      "learning_rate": 4.9656250000000004e-05,
      "loss": 4.7839,
      "step": 412
    },
    {
      "bce_eff_w": 0.20195000000000002,
      "bce_loss": -0.4647100269794464,
      "epoch": 2.06,
      "step": 412
    },
    {
      "epoch": 2.065,
      "grad_norm": 30.308971405029297,
      "learning_rate": 4.962500000000001e-05,
      "loss": 1.8308,
      "step": 413
    },
    {
      "bce_eff_w": 0.2021,
      "bce_loss": -0.4641945958137512,
      "epoch": 2.065,
      "step": 413
    },
    {
      "epoch": 2.07,
      "grad_norm": 18.6268367767334,
      "learning_rate": 4.959375e-05,
      "loss": 2.5732,
      "step": 414
    },
    {
      "bce_eff_w": 0.20225,
      "bce_loss": -0.4590069055557251,
      "epoch": 2.07,
      "step": 414
    },
    {
      "epoch": 2.075,
      "grad_norm": 21.28029441833496,
      "learning_rate": 4.95625e-05,
      "loss": 2.4757,
      "step": 415
    },
    {
      "bce_eff_w": 0.20240000000000002,
      "bce_loss": -0.46221330761909485,
      "epoch": 2.075,
      "step": 415
    },
    {
      "epoch": 2.08,
      "grad_norm": 17.688480377197266,
      "learning_rate": 4.953125e-05,
      "loss": 3.1804,
      "step": 416
    },
    {
      "bce_eff_w": 0.20255,
      "bce_loss": -0.46503493189811707,
      "epoch": 2.08,
      "step": 416
    },
    {
      "epoch": 2.085,
      "grad_norm": 22.738399505615234,
      "learning_rate": 4.9500000000000004e-05,
      "loss": 3.8581,
      "step": 417
    },
    {
      "bce_eff_w": 0.20270000000000002,
      "bce_loss": -0.46514463424682617,
      "epoch": 2.085,
      "step": 417
    },
    {
      "epoch": 2.09,
      "grad_norm": 21.42011833190918,
      "learning_rate": 4.946875e-05,
      "loss": 4.7119,
      "step": 418
    },
    {
      "bce_eff_w": 0.20285,
      "bce_loss": -0.46490365266799927,
      "epoch": 2.09,
      "step": 418
    },
    {
      "epoch": 2.095,
      "grad_norm": 24.751867294311523,
      "learning_rate": 4.94375e-05,
      "loss": 1.5316,
      "step": 419
    },
    {
      "bce_eff_w": 0.203,
      "bce_loss": -0.46441328525543213,
      "epoch": 2.095,
      "step": 419
    },
    {
      "epoch": 2.1,
      "grad_norm": 35.216400146484375,
      "learning_rate": 4.9406250000000005e-05,
      "loss": 1.8685,
      "step": 420
    },
    {
      "bce_eff_w": 0.20315,
      "bce_loss": -0.46187958121299744,
      "epoch": 2.1,
      "step": 420
    },
    {
      "epoch": 2.105,
      "grad_norm": 22.495046615600586,
      "learning_rate": 4.937500000000001e-05,
      "loss": 2.0394,
      "step": 421
    },
    {
      "bce_eff_w": 0.2033,
      "bce_loss": -0.4630025327205658,
      "epoch": 2.105,
      "step": 421
    },
    {
      "epoch": 2.11,
      "grad_norm": 13.258506774902344,
      "learning_rate": 4.9343749999999997e-05,
      "loss": 5.4128,
      "step": 422
    },
    {
      "bce_eff_w": 0.20345000000000002,
      "bce_loss": -0.46547162532806396,
      "epoch": 2.11,
      "step": 422
    },
    {
      "epoch": 2.115,
      "grad_norm": 23.3631591796875,
      "learning_rate": 4.93125e-05,
      "loss": 3.6899,
      "step": 423
    },
    {
      "bce_eff_w": 0.2036,
      "bce_loss": -0.46501556038856506,
      "epoch": 2.115,
      "step": 423
    },
    {
      "epoch": 2.12,
      "grad_norm": 16.757972717285156,
      "learning_rate": 4.928125e-05,
      "loss": 4.0506,
      "step": 424
    },
    {
      "bce_eff_w": 0.20375000000000001,
      "bce_loss": -0.46418455243110657,
      "epoch": 2.12,
      "step": 424
    },
    {
      "epoch": 2.125,
      "grad_norm": 33.1575927734375,
      "learning_rate": 4.9250000000000004e-05,
      "loss": 4.725,
      "step": 425
    },
    {
      "bce_eff_w": 0.2039,
      "bce_loss": -0.4639117121696472,
      "epoch": 2.125,
      "step": 425
    },
    {
      "epoch": 2.13,
      "grad_norm": 23.027984619140625,
      "learning_rate": 4.921875e-05,
      "loss": 4.3435,
      "step": 426
    },
    {
      "bce_eff_w": 0.20405,
      "bce_loss": -0.4644492268562317,
      "epoch": 2.13,
      "step": 426
    },
    {
      "epoch": 2.135,
      "grad_norm": 17.061336517333984,
      "learning_rate": 4.91875e-05,
      "loss": 2.7696,
      "step": 427
    },
    {
      "bce_eff_w": 0.20420000000000002,
      "bce_loss": -0.46145322918891907,
      "epoch": 2.135,
      "step": 427
    },
    {
      "epoch": 2.14,
      "grad_norm": 17.222253799438477,
      "learning_rate": 4.9156250000000006e-05,
      "loss": 1.7524,
      "step": 428
    },
    {
      "bce_eff_w": 0.20435,
      "bce_loss": -0.4651474356651306,
      "epoch": 2.14,
      "step": 428
    },
    {
      "epoch": 2.145,
      "grad_norm": 27.961719512939453,
      "learning_rate": 4.9125e-05,
      "loss": 2.7736,
      "step": 429
    },
    {
      "bce_eff_w": 0.20450000000000002,
      "bce_loss": -0.4643608629703522,
      "epoch": 2.145,
      "step": 429
    },
    {
      "epoch": 2.15,
      "grad_norm": 32.14550018310547,
      "learning_rate": 4.9093750000000004e-05,
      "loss": 2.7386,
      "step": 430
    },
    {
      "bce_eff_w": 0.20465,
      "bce_loss": -0.46497347950935364,
      "epoch": 2.15,
      "step": 430
    },
    {
      "epoch": 2.155,
      "grad_norm": 23.558788299560547,
      "learning_rate": 4.90625e-05,
      "loss": 1.461,
      "step": 431
    },
    {
      "bce_eff_w": 0.2048,
      "bce_loss": -0.46537843346595764,
      "epoch": 2.155,
      "step": 431
    },
    {
      "epoch": 2.16,
      "grad_norm": 20.906150817871094,
      "learning_rate": 4.903125e-05,
      "loss": 3.1865,
      "step": 432
    },
    {
      "bce_eff_w": 0.20495000000000002,
      "bce_loss": -0.46502161026000977,
      "epoch": 2.16,
      "step": 432
    },
    {
      "epoch": 2.165,
      "grad_norm": 18.252037048339844,
      "learning_rate": 4.9e-05,
      "loss": 3.9616,
      "step": 433
    },
    {
      "bce_eff_w": 0.2051,
      "bce_loss": -0.4652312994003296,
      "epoch": 2.165,
      "step": 433
    },
    {
      "epoch": 2.17,
      "grad_norm": 19.823530197143555,
      "learning_rate": 4.896875e-05,
      "loss": 4.0456,
      "step": 434
    },
    {
      "bce_eff_w": 0.20525000000000002,
      "bce_loss": -0.46510210633277893,
      "epoch": 2.17,
      "step": 434
    },
    {
      "epoch": 2.175,
      "grad_norm": 31.549623489379883,
      "learning_rate": 4.8937500000000004e-05,
      "loss": 2.3945,
      "step": 435
    },
    {
      "bce_eff_w": 0.2054,
      "bce_loss": -0.4641838073730469,
      "epoch": 2.175,
      "step": 435
    },
    {
      "epoch": 2.18,
      "grad_norm": 29.489212036132812,
      "learning_rate": 4.8906250000000006e-05,
      "loss": 1.3651,
      "step": 436
    },
    {
      "bce_eff_w": 0.20555,
      "bce_loss": -0.46307697892189026,
      "epoch": 2.18,
      "step": 436
    },
    {
      "epoch": 2.185,
      "grad_norm": 18.132678985595703,
      "learning_rate": 4.8875e-05,
      "loss": 1.8292,
      "step": 437
    },
    {
      "bce_eff_w": 0.20570000000000002,
      "bce_loss": -0.4651753604412079,
      "epoch": 2.185,
      "step": 437
    },
    {
      "epoch": 2.19,
      "grad_norm": 39.957313537597656,
      "learning_rate": 4.8843750000000005e-05,
      "loss": 1.9611,
      "step": 438
    },
    {
      "bce_eff_w": 0.20585,
      "bce_loss": -0.4646415412425995,
      "epoch": 2.19,
      "step": 438
    },
    {
      "epoch": 2.195,
      "grad_norm": 34.66191864013672,
      "learning_rate": 4.88125e-05,
      "loss": 4.3464,
      "step": 439
    },
    {
      "bce_eff_w": 0.20600000000000002,
      "bce_loss": -0.46498027443885803,
      "epoch": 2.195,
      "step": 439
    },
    {
      "epoch": 2.2,
      "grad_norm": 33.66637420654297,
      "learning_rate": 4.878125e-05,
      "loss": 1.8437,
      "step": 440
    },
    {
      "bce_eff_w": 0.20615,
      "bce_loss": -0.4651542603969574,
      "epoch": 2.2,
      "step": 440
    },
    {
      "epoch": 2.205,
      "grad_norm": 18.932655334472656,
      "learning_rate": 4.875e-05,
      "loss": 1.0904,
      "step": 441
    },
    {
      "bce_eff_w": 0.2063,
      "bce_loss": -0.46408963203430176,
      "epoch": 2.205,
      "step": 441
    },
    {
      "epoch": 2.21,
      "grad_norm": 14.76801586151123,
      "learning_rate": 4.871875e-05,
      "loss": 5.0662,
      "step": 442
    },
    {
      "bce_eff_w": 0.20645000000000002,
      "bce_loss": -0.46363601088523865,
      "epoch": 2.21,
      "step": 442
    },
    {
      "epoch": 2.215,
      "grad_norm": 22.166637420654297,
      "learning_rate": 4.8687500000000004e-05,
      "loss": 2.4941,
      "step": 443
    },
    {
      "bce_eff_w": 0.2066,
      "bce_loss": -0.46472862362861633,
      "epoch": 2.215,
      "step": 443
    },
    {
      "epoch": 2.22,
      "grad_norm": 27.554718017578125,
      "learning_rate": 4.865625e-05,
      "loss": 1.1951,
      "step": 444
    },
    {
      "bce_eff_w": 0.20675000000000002,
      "bce_loss": -0.4637032449245453,
      "epoch": 2.22,
      "step": 444
    },
    {
      "epoch": 2.225,
      "grad_norm": 23.746063232421875,
      "learning_rate": 4.8625e-05,
      "loss": 2.2776,
      "step": 445
    },
    {
      "bce_eff_w": 0.2069,
      "bce_loss": -0.46536985039711,
      "epoch": 2.225,
      "step": 445
    },
    {
      "epoch": 2.23,
      "grad_norm": 17.1087646484375,
      "learning_rate": 4.8593750000000005e-05,
      "loss": 3.7845,
      "step": 446
    },
    {
      "bce_eff_w": 0.20705,
      "bce_loss": -0.4650166630744934,
      "epoch": 2.23,
      "step": 446
    },
    {
      "epoch": 2.235,
      "grad_norm": 18.707029342651367,
      "learning_rate": 4.85625e-05,
      "loss": 3.7519,
      "step": 447
    },
    {
      "bce_eff_w": 0.20720000000000002,
      "bce_loss": -0.4651087522506714,
      "epoch": 2.235,
      "step": 447
    },
    {
      "epoch": 2.24,
      "grad_norm": 27.834800720214844,
      "learning_rate": 4.853125e-05,
      "loss": 1.6535,
      "step": 448
    },
    {
      "bce_eff_w": 0.20735,
      "bce_loss": -0.4652242064476013,
      "epoch": 2.24,
      "step": 448
    },
    {
      "epoch": 2.245,
      "grad_norm": 22.309213638305664,
      "learning_rate": 4.85e-05,
      "loss": 3.853,
      "step": 449
    },
    {
      "bce_eff_w": 0.20750000000000002,
      "bce_loss": -0.4647815525531769,
      "epoch": 2.245,
      "step": 449
    },
    {
      "epoch": 2.25,
      "grad_norm": 25.67880630493164,
      "learning_rate": 4.846875e-05,
      "loss": 1.8864,
      "step": 450
    },
    {
      "bce_eff_w": 0.20765,
      "bce_loss": -0.463859498500824,
      "epoch": 2.25,
      "step": 450
    },
    {
      "epoch": 2.255,
      "grad_norm": 25.122549057006836,
      "learning_rate": 4.8437500000000005e-05,
      "loss": 2.7088,
      "step": 451
    },
    {
      "bce_eff_w": 0.2078,
      "bce_loss": -0.4642128050327301,
      "epoch": 2.255,
      "step": 451
    },
    {
      "epoch": 2.26,
      "grad_norm": 33.528343200683594,
      "learning_rate": 4.840625e-05,
      "loss": 1.397,
      "step": 452
    },
    {
      "bce_eff_w": 0.20795000000000002,
      "bce_loss": -0.464143842458725,
      "epoch": 2.26,
      "step": 452
    },
    {
      "epoch": 2.265,
      "grad_norm": 32.8027229309082,
      "learning_rate": 4.8375000000000004e-05,
      "loss": 3.0669,
      "step": 453
    },
    {
      "bce_eff_w": 0.2081,
      "bce_loss": -0.46532100439071655,
      "epoch": 2.265,
      "step": 453
    },
    {
      "epoch": 2.27,
      "grad_norm": 26.03266143798828,
      "learning_rate": 4.8343750000000006e-05,
      "loss": 2.1792,
      "step": 454
    },
    {
      "bce_eff_w": 0.20825000000000002,
      "bce_loss": -0.46454325318336487,
      "epoch": 2.27,
      "step": 454
    },
    {
      "epoch": 2.275,
      "grad_norm": 30.354890823364258,
      "learning_rate": 4.83125e-05,
      "loss": 1.1006,
      "step": 455
    },
    {
      "bce_eff_w": 0.2084,
      "bce_loss": -0.46498680114746094,
      "epoch": 2.275,
      "step": 455
    },
    {
      "epoch": 2.2800000000000002,
      "grad_norm": 20.543724060058594,
      "learning_rate": 4.828125e-05,
      "loss": 1.2227,
      "step": 456
    },
    {
      "bce_eff_w": 0.20855,
      "bce_loss": -0.4638095498085022,
      "epoch": 2.2800000000000002,
      "step": 456
    },
    {
      "epoch": 2.285,
      "grad_norm": 30.273178100585938,
      "learning_rate": 4.825e-05,
      "loss": 2.0679,
      "step": 457
    },
    {
      "bce_eff_w": 0.2087,
      "bce_loss": -0.465177983045578,
      "epoch": 2.285,
      "step": 457
    },
    {
      "epoch": 2.29,
      "grad_norm": 24.20322036743164,
      "learning_rate": 4.821875e-05,
      "loss": 0.7209,
      "step": 458
    },
    {
      "bce_eff_w": 0.20885,
      "bce_loss": -0.46523556113243103,
      "epoch": 2.29,
      "step": 458
    },
    {
      "epoch": 2.295,
      "grad_norm": 21.641572952270508,
      "learning_rate": 4.81875e-05,
      "loss": 1.3699,
      "step": 459
    },
    {
      "bce_eff_w": 0.20900000000000002,
      "bce_loss": -0.4653163552284241,
      "epoch": 2.295,
      "step": 459
    },
    {
      "epoch": 2.3,
      "grad_norm": 31.06055450439453,
      "learning_rate": 4.815625e-05,
      "loss": 2.7439,
      "step": 460
    },
    {
      "bce_eff_w": 0.20915,
      "bce_loss": -0.4649282693862915,
      "epoch": 2.3,
      "step": 460
    },
    {
      "epoch": 2.305,
      "grad_norm": 26.41661262512207,
      "learning_rate": 4.8125000000000004e-05,
      "loss": 1.7337,
      "step": 461
    },
    {
      "bce_eff_w": 0.2093,
      "bce_loss": -0.46467235684394836,
      "epoch": 2.305,
      "step": 461
    },
    {
      "epoch": 2.31,
      "grad_norm": 27.880535125732422,
      "learning_rate": 4.809375000000001e-05,
      "loss": 3.9896,
      "step": 462
    },
    {
      "bce_eff_w": 0.20945000000000003,
      "bce_loss": -0.46482858061790466,
      "epoch": 2.31,
      "step": 462
    },
    {
      "epoch": 2.315,
      "grad_norm": 31.17556381225586,
      "learning_rate": 4.80625e-05,
      "loss": 2.159,
      "step": 463
    },
    {
      "bce_eff_w": 0.2096,
      "bce_loss": -0.4647132456302643,
      "epoch": 2.315,
      "step": 463
    },
    {
      "epoch": 2.32,
      "grad_norm": 25.655536651611328,
      "learning_rate": 4.803125e-05,
      "loss": 2.2045,
      "step": 464
    },
    {
      "bce_eff_w": 0.20975000000000002,
      "bce_loss": -0.4611480236053467,
      "epoch": 2.32,
      "step": 464
    },
    {
      "epoch": 2.325,
      "grad_norm": 22.193010330200195,
      "learning_rate": 4.8e-05,
      "loss": 1.826,
      "step": 465
    },
    {
      "bce_eff_w": 0.2099,
      "bce_loss": -0.4650726318359375,
      "epoch": 2.325,
      "step": 465
    },
    {
      "epoch": 2.33,
      "grad_norm": 30.420366287231445,
      "learning_rate": 4.7968750000000004e-05,
      "loss": 2.4548,
      "step": 466
    },
    {
      "bce_eff_w": 0.21005000000000001,
      "bce_loss": -0.4653153419494629,
      "epoch": 2.33,
      "step": 466
    },
    {
      "epoch": 2.335,
      "grad_norm": 20.74159812927246,
      "learning_rate": 4.79375e-05,
      "loss": 4.1825,
      "step": 467
    },
    {
      "bce_eff_w": 0.2102,
      "bce_loss": -0.4645271301269531,
      "epoch": 2.335,
      "step": 467
    },
    {
      "epoch": 2.34,
      "grad_norm": 23.597434997558594,
      "learning_rate": 4.790625e-05,
      "loss": 3.3902,
      "step": 468
    },
    {
      "bce_eff_w": 0.21035,
      "bce_loss": -0.4652591049671173,
      "epoch": 2.34,
      "step": 468
    },
    {
      "epoch": 2.3449999999999998,
      "grad_norm": 35.83097839355469,
      "learning_rate": 4.7875000000000005e-05,
      "loss": 1.6919,
      "step": 469
    },
    {
      "bce_eff_w": 0.21050000000000002,
      "bce_loss": -0.4613092839717865,
      "epoch": 2.3449999999999998,
      "step": 469
    },
    {
      "epoch": 2.35,
      "grad_norm": 22.759815216064453,
      "learning_rate": 4.784375e-05,
      "loss": 2.5156,
      "step": 470
    },
    {
      "bce_eff_w": 0.21065,
      "bce_loss": -0.4648856818675995,
      "epoch": 2.35,
      "step": 470
    },
    {
      "epoch": 2.355,
      "grad_norm": 31.66248893737793,
      "learning_rate": 4.7812500000000003e-05,
      "loss": 1.872,
      "step": 471
    },
    {
      "bce_eff_w": 0.21080000000000002,
      "bce_loss": -0.4648386240005493,
      "epoch": 2.355,
      "step": 471
    },
    {
      "epoch": 2.36,
      "grad_norm": 33.99718475341797,
      "learning_rate": 4.778125e-05,
      "loss": 3.256,
      "step": 472
    },
    {
      "bce_eff_w": 0.21095,
      "bce_loss": -0.46403199434280396,
      "epoch": 2.36,
      "step": 472
    },
    {
      "epoch": 2.365,
      "grad_norm": 29.614593505859375,
      "learning_rate": 4.775e-05,
      "loss": 3.5586,
      "step": 473
    },
    {
      "bce_eff_w": 0.2111,
      "bce_loss": -0.4649823009967804,
      "epoch": 2.365,
      "step": 473
    },
    {
      "epoch": 2.37,
      "grad_norm": 18.84546661376953,
      "learning_rate": 4.771875e-05,
      "loss": 3.4074,
      "step": 474
    },
    {
      "bce_eff_w": 0.21125000000000002,
      "bce_loss": -0.4643462598323822,
      "epoch": 2.37,
      "step": 474
    },
    {
      "epoch": 2.375,
      "grad_norm": 17.07967758178711,
      "learning_rate": 4.76875e-05,
      "loss": 4.2753,
      "step": 475
    },
    {
      "bce_eff_w": 0.2114,
      "bce_loss": -0.46438542008399963,
      "epoch": 2.375,
      "step": 475
    },
    {
      "epoch": 2.38,
      "grad_norm": 41.663787841796875,
      "learning_rate": 4.765625e-05,
      "loss": 2.0682,
      "step": 476
    },
    {
      "bce_eff_w": 0.21155000000000002,
      "bce_loss": -0.4642183184623718,
      "epoch": 2.38,
      "step": 476
    },
    {
      "epoch": 2.385,
      "grad_norm": 47.454376220703125,
      "learning_rate": 4.7625000000000006e-05,
      "loss": 1.4469,
      "step": 477
    },
    {
      "bce_eff_w": 0.2117,
      "bce_loss": -0.4650995135307312,
      "epoch": 2.385,
      "step": 477
    },
    {
      "epoch": 2.39,
      "grad_norm": 34.922908782958984,
      "learning_rate": 4.759375e-05,
      "loss": 2.5057,
      "step": 478
    },
    {
      "bce_eff_w": 0.21185,
      "bce_loss": -0.46482542157173157,
      "epoch": 2.39,
      "step": 478
    },
    {
      "epoch": 2.395,
      "grad_norm": 22.745548248291016,
      "learning_rate": 4.7562500000000004e-05,
      "loss": 4.1883,
      "step": 479
    },
    {
      "bce_eff_w": 0.21200000000000002,
      "bce_loss": -0.46462446451187134,
      "epoch": 2.395,
      "step": 479
    },
    {
      "epoch": 2.4,
      "grad_norm": 31.382110595703125,
      "learning_rate": 4.753125000000001e-05,
      "loss": 1.7499,
      "step": 480
    },
    {
      "bce_eff_w": 0.21215,
      "bce_loss": -0.4645305275917053,
      "epoch": 2.4,
      "step": 480
    },
    {
      "epoch": 2.4050000000000002,
      "grad_norm": 23.618803024291992,
      "learning_rate": 4.75e-05,
      "loss": 1.4024,
      "step": 481
    },
    {
      "bce_eff_w": 0.21230000000000002,
      "bce_loss": -0.46206170320510864,
      "epoch": 2.4050000000000002,
      "step": 481
    },
    {
      "epoch": 2.41,
      "grad_norm": 21.624025344848633,
      "learning_rate": 4.746875e-05,
      "loss": 2.3036,
      "step": 482
    },
    {
      "bce_eff_w": 0.21245,
      "bce_loss": -0.4653191566467285,
      "epoch": 2.41,
      "step": 482
    },
    {
      "epoch": 2.415,
      "grad_norm": 23.11307144165039,
      "learning_rate": 4.74375e-05,
      "loss": 2.7455,
      "step": 483
    },
    {
      "bce_eff_w": 0.2126,
      "bce_loss": -0.4642396569252014,
      "epoch": 2.415,
      "step": 483
    },
    {
      "epoch": 2.42,
      "grad_norm": 29.972288131713867,
      "learning_rate": 4.7406250000000004e-05,
      "loss": 1.7036,
      "step": 484
    },
    {
      "bce_eff_w": 0.21275000000000002,
      "bce_loss": -0.4634760022163391,
      "epoch": 2.42,
      "step": 484
    },
    {
      "epoch": 2.425,
      "grad_norm": 20.86471939086914,
      "learning_rate": 4.7375e-05,
      "loss": 1.7792,
      "step": 485
    },
    {
      "bce_eff_w": 0.2129,
      "bce_loss": -0.4631577432155609,
      "epoch": 2.425,
      "step": 485
    },
    {
      "epoch": 2.43,
      "grad_norm": 19.12697982788086,
      "learning_rate": 4.734375e-05,
      "loss": 2.93,
      "step": 486
    },
    {
      "bce_eff_w": 0.21305000000000002,
      "bce_loss": -0.46386969089508057,
      "epoch": 2.43,
      "step": 486
    },
    {
      "epoch": 2.435,
      "grad_norm": 14.73940372467041,
      "learning_rate": 4.7312500000000005e-05,
      "loss": 5.1121,
      "step": 487
    },
    {
      "bce_eff_w": 0.2132,
      "bce_loss": -0.4646141231060028,
      "epoch": 2.435,
      "step": 487
    },
    {
      "epoch": 2.44,
      "grad_norm": 38.4787712097168,
      "learning_rate": 4.728125000000001e-05,
      "loss": 1.3495,
      "step": 488
    },
    {
      "bce_eff_w": 0.21335,
      "bce_loss": -0.46475058794021606,
      "epoch": 2.44,
      "step": 488
    },
    {
      "epoch": 2.445,
      "grad_norm": 23.554027557373047,
      "learning_rate": 4.7249999999999997e-05,
      "loss": 3.1813,
      "step": 489
    },
    {
      "bce_eff_w": 0.21350000000000002,
      "bce_loss": -0.46518877148628235,
      "epoch": 2.445,
      "step": 489
    },
    {
      "epoch": 2.45,
      "grad_norm": 18.459264755249023,
      "learning_rate": 4.721875e-05,
      "loss": 3.5925,
      "step": 490
    },
    {
      "bce_eff_w": 0.21365,
      "bce_loss": -0.46379080414772034,
      "epoch": 2.45,
      "step": 490
    },
    {
      "epoch": 2.455,
      "grad_norm": 31.701318740844727,
      "learning_rate": 4.71875e-05,
      "loss": 4.0576,
      "step": 491
    },
    {
      "bce_eff_w": 0.21380000000000002,
      "bce_loss": -0.465253084897995,
      "epoch": 2.455,
      "step": 491
    },
    {
      "epoch": 2.46,
      "grad_norm": 27.17164421081543,
      "learning_rate": 4.7156250000000004e-05,
      "loss": 1.52,
      "step": 492
    },
    {
      "bce_eff_w": 0.21395,
      "bce_loss": -0.4651702642440796,
      "epoch": 2.46,
      "step": 492
    },
    {
      "epoch": 2.465,
      "grad_norm": 23.13445281982422,
      "learning_rate": 4.7125e-05,
      "loss": 2.4706,
      "step": 493
    },
    {
      "bce_eff_w": 0.2141,
      "bce_loss": -0.4651012122631073,
      "epoch": 2.465,
      "step": 493
    },
    {
      "epoch": 2.4699999999999998,
      "grad_norm": 16.57803726196289,
      "learning_rate": 4.709375e-05,
      "loss": 4.4122,
      "step": 494
    },
    {
      "bce_eff_w": 0.21425,
      "bce_loss": -0.4603438377380371,
      "epoch": 2.4699999999999998,
      "step": 494
    },
    {
      "epoch": 2.475,
      "grad_norm": 19.843597412109375,
      "learning_rate": 4.7062500000000006e-05,
      "loss": 1.5835,
      "step": 495
    },
    {
      "bce_eff_w": 0.2144,
      "bce_loss": -0.46521899104118347,
      "epoch": 2.475,
      "step": 495
    },
    {
      "epoch": 2.48,
      "grad_norm": 20.65886688232422,
      "learning_rate": 4.703125e-05,
      "loss": 2.7943,
      "step": 496
    },
    {
      "bce_eff_w": 0.21455000000000002,
      "bce_loss": -0.463400661945343,
      "epoch": 2.48,
      "step": 496
    },
    {
      "epoch": 2.485,
      "grad_norm": 17.28240203857422,
      "learning_rate": 4.7e-05,
      "loss": 4.781,
      "step": 497
    },
    {
      "bce_eff_w": 0.2147,
      "bce_loss": -0.46481987833976746,
      "epoch": 2.485,
      "step": 497
    },
    {
      "epoch": 2.49,
      "grad_norm": 25.80494499206543,
      "learning_rate": 4.696875e-05,
      "loss": 3.3666,
      "step": 498
    },
    {
      "bce_eff_w": 0.21485,
      "bce_loss": -0.46420174837112427,
      "epoch": 2.49,
      "step": 498
    },
    {
      "epoch": 2.495,
      "grad_norm": 12.828569412231445,
      "learning_rate": 4.69375e-05,
      "loss": 4.7783,
      "step": 499
    },
    {
      "bce_eff_w": 0.21500000000000002,
      "bce_loss": -0.46305325627326965,
      "epoch": 2.495,
      "step": 499
    },
    {
      "epoch": 2.5,
      "grad_norm": 25.599163055419922,
      "learning_rate": 4.690625e-05,
      "loss": 4.3831,
      "step": 500
    },
    {
      "bce_eff_w": 0.21515,
      "bce_loss": -0.4648476839065552,
      "epoch": 2.5,
      "step": 500
    },
    {
      "epoch": 2.505,
      "grad_norm": 28.594318389892578,
      "learning_rate": 4.6875e-05,
      "loss": 1.6055,
      "step": 501
    },
    {
      "bce_eff_w": 0.21530000000000002,
      "bce_loss": -0.46474531292915344,
      "epoch": 2.505,
      "step": 501
    },
    {
      "epoch": 2.51,
      "grad_norm": 19.613964080810547,
      "learning_rate": 4.6843750000000004e-05,
      "loss": 3.1281,
      "step": 502
    },
    {
      "bce_eff_w": 0.21545,
      "bce_loss": -0.46300262212753296,
      "epoch": 2.51,
      "step": 502
    },
    {
      "epoch": 2.515,
      "grad_norm": 15.162940979003906,
      "learning_rate": 4.6812500000000006e-05,
      "loss": 4.4869,
      "step": 503
    },
    {
      "bce_eff_w": 0.2156,
      "bce_loss": -0.4626528024673462,
      "epoch": 2.515,
      "step": 503
    },
    {
      "epoch": 2.52,
      "grad_norm": 27.832443237304688,
      "learning_rate": 4.678125e-05,
      "loss": 2.1527,
      "step": 504
    },
    {
      "bce_eff_w": 0.21575,
      "bce_loss": -0.4649800658226013,
      "epoch": 2.52,
      "step": 504
    },
    {
      "epoch": 2.525,
      "grad_norm": 25.832277297973633,
      "learning_rate": 4.6750000000000005e-05,
      "loss": 2.2627,
      "step": 505
    },
    {
      "bce_eff_w": 0.2159,
      "bce_loss": -0.4620366394519806,
      "epoch": 2.525,
      "step": 505
    },
    {
      "epoch": 2.5300000000000002,
      "grad_norm": 20.44835090637207,
      "learning_rate": 4.671875e-05,
      "loss": 2.8848,
      "step": 506
    },
    {
      "bce_eff_w": 0.21605000000000002,
      "bce_loss": -0.4644124507904053,
      "epoch": 2.5300000000000002,
      "step": 506
    },
    {
      "epoch": 2.535,
      "grad_norm": 22.447357177734375,
      "learning_rate": 4.66875e-05,
      "loss": 0.5886,
      "step": 507
    },
    {
      "bce_eff_w": 0.2162,
      "bce_loss": -0.4649764597415924,
      "epoch": 2.535,
      "step": 507
    },
    {
      "epoch": 2.54,
      "grad_norm": 22.504301071166992,
      "learning_rate": 4.665625e-05,
      "loss": 2.6598,
      "step": 508
    },
    {
      "bce_eff_w": 0.21635000000000001,
      "bce_loss": -0.46490171551704407,
      "epoch": 2.54,
      "step": 508
    },
    {
      "epoch": 2.545,
      "grad_norm": 26.2481746673584,
      "learning_rate": 4.6625e-05,
      "loss": 1.3829,
      "step": 509
    },
    {
      "bce_eff_w": 0.21650000000000003,
      "bce_loss": -0.46466517448425293,
      "epoch": 2.545,
      "step": 509
    },
    {
      "epoch": 2.55,
      "grad_norm": 23.271255493164062,
      "learning_rate": 4.6593750000000004e-05,
      "loss": 1.1796,
      "step": 510
    },
    {
      "bce_eff_w": 0.21665,
      "bce_loss": -0.4654143452644348,
      "epoch": 2.55,
      "step": 510
    },
    {
      "epoch": 2.555,
      "grad_norm": 17.227191925048828,
      "learning_rate": 4.65625e-05,
      "loss": 3.3925,
      "step": 511
    },
    {
      "bce_eff_w": 0.21680000000000002,
      "bce_loss": -0.4629509150981903,
      "epoch": 2.555,
      "step": 511
    },
    {
      "epoch": 2.56,
      "grad_norm": 15.647930145263672,
      "learning_rate": 4.653125e-05,
      "loss": 4.7289,
      "step": 512
    },
    {
      "bce_eff_w": 0.21695,
      "bce_loss": -0.46503889560699463,
      "epoch": 2.56,
      "step": 512
    },
    {
      "epoch": 2.565,
      "grad_norm": 29.946369171142578,
      "learning_rate": 4.6500000000000005e-05,
      "loss": 2.9048,
      "step": 513
    },
    {
      "bce_eff_w": 0.21710000000000002,
      "bce_loss": -0.46370160579681396,
      "epoch": 2.565,
      "step": 513
    },
    {
      "epoch": 2.57,
      "grad_norm": 20.62873077392578,
      "learning_rate": 4.646875e-05,
      "loss": 2.5148,
      "step": 514
    },
    {
      "bce_eff_w": 0.21725,
      "bce_loss": -0.46469712257385254,
      "epoch": 2.57,
      "step": 514
    },
    {
      "epoch": 2.575,
      "grad_norm": 34.728790283203125,
      "learning_rate": 4.64375e-05,
      "loss": 2.1323,
      "step": 515
    },
    {
      "bce_eff_w": 0.2174,
      "bce_loss": -0.46306270360946655,
      "epoch": 2.575,
      "step": 515
    },
    {
      "epoch": 2.58,
      "grad_norm": 18.5741024017334,
      "learning_rate": 4.640625e-05,
      "loss": 1.312,
      "step": 516
    },
    {
      "bce_eff_w": 0.21755000000000002,
      "bce_loss": -0.4649317264556885,
      "epoch": 2.58,
      "step": 516
    },
    {
      "epoch": 2.585,
      "grad_norm": 26.845178604125977,
      "learning_rate": 4.6375e-05,
      "loss": 1.8974,
      "step": 517
    },
    {
      "bce_eff_w": 0.2177,
      "bce_loss": -0.4646206498146057,
      "epoch": 2.585,
      "step": 517
    },
    {
      "epoch": 2.59,
      "grad_norm": 23.147294998168945,
      "learning_rate": 4.6343750000000005e-05,
      "loss": 1.2434,
      "step": 518
    },
    {
      "bce_eff_w": 0.21785000000000002,
      "bce_loss": -0.464731901884079,
      "epoch": 2.59,
      "step": 518
    },
    {
      "epoch": 2.5949999999999998,
      "grad_norm": 25.988033294677734,
      "learning_rate": 4.63125e-05,
      "loss": 2.2147,
      "step": 519
    },
    {
      "bce_eff_w": 0.218,
      "bce_loss": -0.4636370837688446,
      "epoch": 2.5949999999999998,
      "step": 519
    },
    {
      "epoch": 2.6,
      "grad_norm": 11.904361724853516,
      "learning_rate": 4.6281250000000003e-05,
      "loss": 4.5399,
      "step": 520
    },
    {
      "bce_eff_w": 0.21815,
      "bce_loss": -0.45944851636886597,
      "epoch": 2.6,
      "step": 520
    },
    {
      "epoch": 2.605,
      "grad_norm": 19.08548927307129,
      "learning_rate": 4.6250000000000006e-05,
      "loss": 1.6734,
      "step": 521
    },
    {
      "bce_eff_w": 0.21830000000000002,
      "bce_loss": -0.4651746153831482,
      "epoch": 2.605,
      "step": 521
    },
    {
      "epoch": 2.61,
      "grad_norm": 31.116884231567383,
      "learning_rate": 4.621875e-05,
      "loss": 2.8371,
      "step": 522
    },
    {
      "bce_eff_w": 0.21845,
      "bce_loss": -0.46399009227752686,
      "epoch": 2.61,
      "step": 522
    },
    {
      "epoch": 2.615,
      "grad_norm": 26.218753814697266,
      "learning_rate": 4.61875e-05,
      "loss": 2.3337,
      "step": 523
    },
    {
      "bce_eff_w": 0.21860000000000002,
      "bce_loss": -0.46193695068359375,
      "epoch": 2.615,
      "step": 523
    },
    {
      "epoch": 2.62,
      "grad_norm": 28.49825668334961,
      "learning_rate": 4.615625e-05,
      "loss": 1.6316,
      "step": 524
    },
    {
      "bce_eff_w": 0.21875,
      "bce_loss": -0.4651731252670288,
      "epoch": 2.62,
      "step": 524
    },
    {
      "epoch": 2.625,
      "grad_norm": 23.643386840820312,
      "learning_rate": 4.6125e-05,
      "loss": 1.8475,
      "step": 525
    },
    {
      "bce_eff_w": 0.2189,
      "bce_loss": -0.4654572010040283,
      "epoch": 2.625,
      "step": 525
    },
    {
      "epoch": 2.63,
      "grad_norm": 19.0080509185791,
      "learning_rate": 4.609375e-05,
      "loss": 4.0678,
      "step": 526
    },
    {
      "bce_eff_w": 0.21905000000000002,
      "bce_loss": -0.4624303877353668,
      "epoch": 2.63,
      "step": 526
    },
    {
      "epoch": 2.635,
      "grad_norm": 21.6160945892334,
      "learning_rate": 4.60625e-05,
      "loss": 1.7109,
      "step": 527
    },
    {
      "bce_eff_w": 0.2192,
      "bce_loss": -0.46412134170532227,
      "epoch": 2.635,
      "step": 527
    },
    {
      "epoch": 2.64,
      "grad_norm": 39.5313835144043,
      "learning_rate": 4.6031250000000004e-05,
      "loss": 2.6084,
      "step": 528
    },
    {
      "bce_eff_w": 0.21935000000000002,
      "bce_loss": -0.46483299136161804,
      "epoch": 2.64,
      "step": 528
    },
    {
      "epoch": 2.645,
      "grad_norm": 30.500762939453125,
      "learning_rate": 4.600000000000001e-05,
      "loss": 1.7098,
      "step": 529
    },
    {
      "bce_eff_w": 0.2195,
      "bce_loss": -0.46495410799980164,
      "epoch": 2.645,
      "step": 529
    },
    {
      "epoch": 2.65,
      "grad_norm": 27.5810489654541,
      "learning_rate": 4.596875e-05,
      "loss": 0.7057,
      "step": 530
    },
    {
      "bce_eff_w": 0.21965,
      "bce_loss": -0.46526819467544556,
      "epoch": 2.65,
      "step": 530
    },
    {
      "epoch": 2.6550000000000002,
      "grad_norm": 25.331344604492188,
      "learning_rate": 4.59375e-05,
      "loss": 3.9472,
      "step": 531
    },
    {
      "bce_eff_w": 0.21980000000000002,
      "bce_loss": -0.4632990062236786,
      "epoch": 2.6550000000000002,
      "step": 531
    },
    {
      "epoch": 2.66,
      "grad_norm": 28.44891357421875,
      "learning_rate": 4.590625e-05,
      "loss": 1.8752,
      "step": 532
    },
    {
      "bce_eff_w": 0.21995,
      "bce_loss": -0.464690238237381,
      "epoch": 2.66,
      "step": 532
    },
    {
      "epoch": 2.665,
      "grad_norm": 25.954626083374023,
      "learning_rate": 4.5875000000000004e-05,
      "loss": 1.2229,
      "step": 533
    },
    {
      "bce_eff_w": 0.22010000000000002,
      "bce_loss": -0.4641616642475128,
      "epoch": 2.665,
      "step": 533
    },
    {
      "epoch": 2.67,
      "grad_norm": 28.848106384277344,
      "learning_rate": 4.584375e-05,
      "loss": 2.1654,
      "step": 534
    },
    {
      "bce_eff_w": 0.22025,
      "bce_loss": -0.4629691541194916,
      "epoch": 2.67,
      "step": 534
    },
    {
      "epoch": 2.675,
      "grad_norm": 16.4453125,
      "learning_rate": 4.58125e-05,
      "loss": 2.0645,
      "step": 535
    },
    {
      "bce_eff_w": 0.2204,
      "bce_loss": -0.46516168117523193,
      "epoch": 2.675,
      "step": 535
    },
    {
      "epoch": 2.68,
      "grad_norm": 21.59776496887207,
      "learning_rate": 4.5781250000000005e-05,
      "loss": 2.3517,
      "step": 536
    },
    {
      "bce_eff_w": 0.22055000000000002,
      "bce_loss": -0.46462902426719666,
      "epoch": 2.68,
      "step": 536
    },
    {
      "epoch": 2.685,
      "grad_norm": 24.543371200561523,
      "learning_rate": 4.575e-05,
      "loss": 1.1062,
      "step": 537
    },
    {
      "bce_eff_w": 0.2207,
      "bce_loss": -0.46477237343788147,
      "epoch": 2.685,
      "step": 537
    },
    {
      "epoch": 2.69,
      "grad_norm": 20.430103302001953,
      "learning_rate": 4.571875e-05,
      "loss": 3.8904,
      "step": 538
    },
    {
      "bce_eff_w": 0.22085000000000002,
      "bce_loss": -0.462662935256958,
      "epoch": 2.69,
      "step": 538
    },
    {
      "epoch": 2.695,
      "grad_norm": 23.01313018798828,
      "learning_rate": 4.56875e-05,
      "loss": 2.0739,
      "step": 539
    },
    {
      "bce_eff_w": 0.221,
      "bce_loss": -0.4645170569419861,
      "epoch": 2.695,
      "step": 539
    },
    {
      "epoch": 2.7,
      "grad_norm": 25.476505279541016,
      "learning_rate": 4.565625e-05,
      "loss": 3.5517,
      "step": 540
    },
    {
      "bce_eff_w": 0.22115,
      "bce_loss": -0.4651588797569275,
      "epoch": 2.7,
      "step": 540
    },
    {
      "epoch": 2.705,
      "grad_norm": 27.433189392089844,
      "learning_rate": 4.5625e-05,
      "loss": 2.3088,
      "step": 541
    },
    {
      "bce_eff_w": 0.2213,
      "bce_loss": -0.46499085426330566,
      "epoch": 2.705,
      "step": 541
    },
    {
      "epoch": 2.71,
      "grad_norm": 33.27272033691406,
      "learning_rate": 4.559375e-05,
      "loss": 2.5671,
      "step": 542
    },
    {
      "bce_eff_w": 0.22145,
      "bce_loss": -0.4635637700557709,
      "epoch": 2.71,
      "step": 542
    },
    {
      "epoch": 2.715,
      "grad_norm": 29.256567001342773,
      "learning_rate": 4.55625e-05,
      "loss": 1.4284,
      "step": 543
    },
    {
      "bce_eff_w": 0.22160000000000002,
      "bce_loss": -0.46486636996269226,
      "epoch": 2.715,
      "step": 543
    },
    {
      "epoch": 2.7199999999999998,
      "grad_norm": 43.30868911743164,
      "learning_rate": 4.5531250000000006e-05,
      "loss": 2.4309,
      "step": 544
    },
    {
      "bce_eff_w": 0.22175,
      "bce_loss": -0.45909780263900757,
      "epoch": 2.7199999999999998,
      "step": 544
    },
    {
      "epoch": 2.725,
      "grad_norm": 21.365816116333008,
      "learning_rate": 4.55e-05,
      "loss": 1.3198,
      "step": 545
    },
    {
      "bce_eff_w": 0.22190000000000001,
      "bce_loss": -0.46509233117103577,
      "epoch": 2.725,
      "step": 545
    },
    {
      "epoch": 2.73,
      "grad_norm": 18.950904846191406,
      "learning_rate": 4.5468750000000004e-05,
      "loss": 0.4408,
      "step": 546
    },
    {
      "bce_eff_w": 0.22205,
      "bce_loss": -0.4652717113494873,
      "epoch": 2.73,
      "step": 546
    },
    {
      "epoch": 2.735,
      "grad_norm": 27.62392234802246,
      "learning_rate": 4.54375e-05,
      "loss": 2.1041,
      "step": 547
    },
    {
      "bce_eff_w": 0.2222,
      "bce_loss": -0.46382761001586914,
      "epoch": 2.735,
      "step": 547
    },
    {
      "epoch": 2.74,
      "grad_norm": 32.08408737182617,
      "learning_rate": 4.540625e-05,
      "loss": 2.1497,
      "step": 548
    },
    {
      "bce_eff_w": 0.22235000000000002,
      "bce_loss": -0.4598369896411896,
      "epoch": 2.74,
      "step": 548
    },
    {
      "epoch": 2.745,
      "grad_norm": 19.926559448242188,
      "learning_rate": 4.5375e-05,
      "loss": 3.9647,
      "step": 549
    },
    {
      "bce_eff_w": 0.2225,
      "bce_loss": -0.4651739299297333,
      "epoch": 2.745,
      "step": 549
    },
    {
      "epoch": 2.75,
      "grad_norm": 18.756664276123047,
      "learning_rate": 4.534375e-05,
      "loss": 2.9869,
      "step": 550
    },
    {
      "bce_eff_w": 0.22265000000000001,
      "bce_loss": -0.46426743268966675,
      "epoch": 2.75,
      "step": 550
    },
    {
      "epoch": 2.755,
      "grad_norm": 27.89464569091797,
      "learning_rate": 4.5312500000000004e-05,
      "loss": 2.2579,
      "step": 551
    },
    {
      "bce_eff_w": 0.2228,
      "bce_loss": -0.4640480577945709,
      "epoch": 2.755,
      "step": 551
    },
    {
      "epoch": 2.76,
      "grad_norm": 24.881290435791016,
      "learning_rate": 4.528125e-05,
      "loss": 2.8319,
      "step": 552
    },
    {
      "bce_eff_w": 0.22295,
      "bce_loss": -0.4644980728626251,
      "epoch": 2.76,
      "step": 552
    },
    {
      "epoch": 2.765,
      "grad_norm": 23.66984748840332,
      "learning_rate": 4.525e-05,
      "loss": 2.6215,
      "step": 553
    },
    {
      "bce_eff_w": 0.22310000000000002,
      "bce_loss": -0.4636322259902954,
      "epoch": 2.765,
      "step": 553
    },
    {
      "epoch": 2.77,
      "grad_norm": 28.972002029418945,
      "learning_rate": 4.5218750000000005e-05,
      "loss": 3.5114,
      "step": 554
    },
    {
      "bce_eff_w": 0.22325,
      "bce_loss": -0.46533656120300293,
      "epoch": 2.77,
      "step": 554
    },
    {
      "epoch": 2.775,
      "grad_norm": 28.034975051879883,
      "learning_rate": 4.518750000000001e-05,
      "loss": 2.2671,
      "step": 555
    },
    {
      "bce_eff_w": 0.22340000000000002,
      "bce_loss": -0.46304789185523987,
      "epoch": 2.775,
      "step": 555
    },
    {
      "epoch": 2.7800000000000002,
      "grad_norm": 21.284805297851562,
      "learning_rate": 4.515625e-05,
      "loss": 2.6394,
      "step": 556
    },
    {
      "bce_eff_w": 0.22355,
      "bce_loss": -0.4645147919654846,
      "epoch": 2.7800000000000002,
      "step": 556
    },
    {
      "epoch": 2.785,
      "grad_norm": 18.023672103881836,
      "learning_rate": 4.5125e-05,
      "loss": 3.75,
      "step": 557
    },
    {
      "bce_eff_w": 0.2237,
      "bce_loss": -0.4649820029735565,
      "epoch": 2.785,
      "step": 557
    },
    {
      "epoch": 2.79,
      "grad_norm": 22.847293853759766,
      "learning_rate": 4.509375e-05,
      "loss": 0.692,
      "step": 558
    },
    {
      "bce_eff_w": 0.22385000000000002,
      "bce_loss": -0.46512943506240845,
      "epoch": 2.79,
      "step": 558
    },
    {
      "epoch": 2.795,
      "grad_norm": 22.16692543029785,
      "learning_rate": 4.5062500000000004e-05,
      "loss": 2.4324,
      "step": 559
    },
    {
      "bce_eff_w": 0.224,
      "bce_loss": -0.46400895714759827,
      "epoch": 2.795,
      "step": 559
    },
    {
      "epoch": 2.8,
      "grad_norm": 25.44426918029785,
      "learning_rate": 4.503125e-05,
      "loss": 1.831,
      "step": 560
    },
    {
      "bce_eff_w": 0.22415000000000002,
      "bce_loss": -0.4639216959476471,
      "epoch": 2.8,
      "step": 560
    },
    {
      "epoch": 2.805,
      "grad_norm": 14.005127906799316,
      "learning_rate": 4.5e-05,
      "loss": 4.8018,
      "step": 561
    },
    {
      "bce_eff_w": 0.2243,
      "bce_loss": -0.4648919701576233,
      "epoch": 2.805,
      "step": 561
    },
    {
      "epoch": 2.81,
      "grad_norm": 29.49741554260254,
      "learning_rate": 4.4968750000000005e-05,
      "loss": 2.5055,
      "step": 562
    },
    {
      "bce_eff_w": 0.22445,
      "bce_loss": -0.464963436126709,
      "epoch": 2.81,
      "step": 562
    },
    {
      "epoch": 2.815,
      "grad_norm": 21.605125427246094,
      "learning_rate": 4.49375e-05,
      "loss": 2.5057,
      "step": 563
    },
    {
      "bce_eff_w": 0.22460000000000002,
      "bce_loss": -0.4652867019176483,
      "epoch": 2.815,
      "step": 563
    },
    {
      "epoch": 2.82,
      "grad_norm": 25.204513549804688,
      "learning_rate": 4.490625e-05,
      "loss": 1.6275,
      "step": 564
    },
    {
      "bce_eff_w": 0.22475,
      "bce_loss": -0.46498003602027893,
      "epoch": 2.82,
      "step": 564
    },
    {
      "epoch": 2.825,
      "grad_norm": 23.712289810180664,
      "learning_rate": 4.4875e-05,
      "loss": 2.7701,
      "step": 565
    },
    {
      "bce_eff_w": 0.22490000000000002,
      "bce_loss": -0.464720755815506,
      "epoch": 2.825,
      "step": 565
    },
    {
      "epoch": 2.83,
      "grad_norm": 30.711708068847656,
      "learning_rate": 4.484375e-05,
      "loss": 1.5747,
      "step": 566
    },
    {
      "bce_eff_w": 0.22505,
      "bce_loss": -0.4639201760292053,
      "epoch": 2.83,
      "step": 566
    },
    {
      "epoch": 2.835,
      "grad_norm": 32.35291290283203,
      "learning_rate": 4.4812500000000005e-05,
      "loss": 2.1503,
      "step": 567
    },
    {
      "bce_eff_w": 0.2252,
      "bce_loss": -0.46427592635154724,
      "epoch": 2.835,
      "step": 567
    },
    {
      "epoch": 2.84,
      "grad_norm": 40.91944122314453,
      "learning_rate": 4.478125e-05,
      "loss": 1.8641,
      "step": 568
    },
    {
      "bce_eff_w": 0.22535000000000002,
      "bce_loss": -0.46533364057540894,
      "epoch": 2.84,
      "step": 568
    },
    {
      "epoch": 2.8449999999999998,
      "grad_norm": 24.826242446899414,
      "learning_rate": 4.4750000000000004e-05,
      "loss": 1.0386,
      "step": 569
    },
    {
      "bce_eff_w": 0.2255,
      "bce_loss": -0.4644891023635864,
      "epoch": 2.8449999999999998,
      "step": 569
    },
    {
      "epoch": 2.85,
      "grad_norm": 18.963970184326172,
      "learning_rate": 4.4718750000000006e-05,
      "loss": 3.5433,
      "step": 570
    },
    {
      "bce_eff_w": 0.22565000000000002,
      "bce_loss": -0.4642498195171356,
      "epoch": 2.85,
      "step": 570
    },
    {
      "epoch": 2.855,
      "grad_norm": 23.03436279296875,
      "learning_rate": 4.46875e-05,
      "loss": 1.0444,
      "step": 571
    },
    {
      "bce_eff_w": 0.2258,
      "bce_loss": -0.46486201882362366,
      "epoch": 2.855,
      "step": 571
    },
    {
      "epoch": 2.86,
      "grad_norm": 21.865320205688477,
      "learning_rate": 4.465625e-05,
      "loss": 3.708,
      "step": 572
    },
    {
      "bce_eff_w": 0.22595,
      "bce_loss": -0.46457692980766296,
      "epoch": 2.86,
      "step": 572
    },
    {
      "epoch": 2.865,
      "grad_norm": 23.317466735839844,
      "learning_rate": 4.4625e-05,
      "loss": 0.8765,
      "step": 573
    },
    {
      "bce_eff_w": 0.22610000000000002,
      "bce_loss": -0.4632134437561035,
      "epoch": 2.865,
      "step": 573
    },
    {
      "epoch": 2.87,
      "grad_norm": 20.661277770996094,
      "learning_rate": 4.459375e-05,
      "loss": 3.2313,
      "step": 574
    },
    {
      "bce_eff_w": 0.22625,
      "bce_loss": -0.4642062783241272,
      "epoch": 2.87,
      "step": 574
    },
    {
      "epoch": 2.875,
      "grad_norm": 18.750783920288086,
      "learning_rate": 4.45625e-05,
      "loss": 1.1121,
      "step": 575
    },
    {
      "bce_eff_w": 0.22640000000000002,
      "bce_loss": -0.4638223350048065,
      "epoch": 2.875,
      "step": 575
    },
    {
      "epoch": 2.88,
      "grad_norm": 18.456579208374023,
      "learning_rate": 4.453125e-05,
      "loss": 1.2598,
      "step": 576
    },
    {
      "bce_eff_w": 0.22655,
      "bce_loss": -0.4635012447834015,
      "epoch": 2.88,
      "step": 576
    },
    {
      "epoch": 2.885,
      "grad_norm": 22.462871551513672,
      "learning_rate": 4.4500000000000004e-05,
      "loss": 3.6165,
      "step": 577
    },
    {
      "bce_eff_w": 0.2267,
      "bce_loss": -0.4631105065345764,
      "epoch": 2.885,
      "step": 577
    },
    {
      "epoch": 2.89,
      "grad_norm": 21.92900848388672,
      "learning_rate": 4.446875e-05,
      "loss": 2.2287,
      "step": 578
    },
    {
      "bce_eff_w": 0.22685,
      "bce_loss": -0.4646260738372803,
      "epoch": 2.89,
      "step": 578
    },
    {
      "epoch": 2.895,
      "grad_norm": 26.5555362701416,
      "learning_rate": 4.44375e-05,
      "loss": 1.268,
      "step": 579
    },
    {
      "bce_eff_w": 0.227,
      "bce_loss": -0.4653010368347168,
      "epoch": 2.895,
      "step": 579
    },
    {
      "epoch": 2.9,
      "grad_norm": 18.860271453857422,
      "learning_rate": 4.4406250000000005e-05,
      "loss": 2.6445,
      "step": 580
    },
    {
      "bce_eff_w": 0.22715000000000002,
      "bce_loss": -0.46446657180786133,
      "epoch": 2.9,
      "step": 580
    },
    {
      "epoch": 2.9050000000000002,
      "grad_norm": 26.589513778686523,
      "learning_rate": 4.4375e-05,
      "loss": 1.079,
      "step": 581
    },
    {
      "bce_eff_w": 0.2273,
      "bce_loss": -0.4645840525627136,
      "epoch": 2.9050000000000002,
      "step": 581
    },
    {
      "epoch": 2.91,
      "grad_norm": 25.825820922851562,
      "learning_rate": 4.4343750000000004e-05,
      "loss": 1.4408,
      "step": 582
    },
    {
      "bce_eff_w": 0.22745,
      "bce_loss": -0.46394360065460205,
      "epoch": 2.91,
      "step": 582
    },
    {
      "epoch": 2.915,
      "grad_norm": 31.302040100097656,
      "learning_rate": 4.43125e-05,
      "loss": 1.3962,
      "step": 583
    },
    {
      "bce_eff_w": 0.22760000000000002,
      "bce_loss": -0.46375149488449097,
      "epoch": 2.915,
      "step": 583
    },
    {
      "epoch": 2.92,
      "grad_norm": 14.37166976928711,
      "learning_rate": 4.428125e-05,
      "loss": 4.3575,
      "step": 584
    },
    {
      "bce_eff_w": 0.22775,
      "bce_loss": -0.4623320698738098,
      "epoch": 2.92,
      "step": 584
    },
    {
      "epoch": 2.925,
      "grad_norm": 45.58527755737305,
      "learning_rate": 4.4250000000000005e-05,
      "loss": 2.4703,
      "step": 585
    },
    {
      "bce_eff_w": 0.22790000000000002,
      "bce_loss": -0.4623686373233795,
      "epoch": 2.925,
      "step": 585
    },
    {
      "epoch": 2.93,
      "grad_norm": 15.442753791809082,
      "learning_rate": 4.421875e-05,
      "loss": 1.688,
      "step": 586
    },
    {
      "bce_eff_w": 0.22805,
      "bce_loss": -0.465223103761673,
      "epoch": 2.93,
      "step": 586
    },
    {
      "epoch": 2.935,
      "grad_norm": 17.32935333251953,
      "learning_rate": 4.4187500000000003e-05,
      "loss": 4.0944,
      "step": 587
    },
    {
      "bce_eff_w": 0.22820000000000001,
      "bce_loss": -0.4645867347717285,
      "epoch": 2.935,
      "step": 587
    },
    {
      "epoch": 2.94,
      "grad_norm": 23.459732055664062,
      "learning_rate": 4.4156250000000006e-05,
      "loss": 3.3423,
      "step": 588
    },
    {
      "bce_eff_w": 0.22835,
      "bce_loss": -0.46506360173225403,
      "epoch": 2.94,
      "step": 588
    },
    {
      "epoch": 2.945,
      "grad_norm": 27.51533317565918,
      "learning_rate": 4.4125e-05,
      "loss": 1.4869,
      "step": 589
    },
    {
      "bce_eff_w": 0.2285,
      "bce_loss": -0.4639231562614441,
      "epoch": 2.945,
      "step": 589
    },
    {
      "epoch": 2.95,
      "grad_norm": 47.92783737182617,
      "learning_rate": 4.409375e-05,
      "loss": 2.6468,
      "step": 590
    },
    {
      "bce_eff_w": 0.22865000000000002,
      "bce_loss": -0.4651638865470886,
      "epoch": 2.95,
      "step": 590
    },
    {
      "epoch": 2.955,
      "grad_norm": 24.82028579711914,
      "learning_rate": 4.40625e-05,
      "loss": 2.3445,
      "step": 591
    },
    {
      "bce_eff_w": 0.2288,
      "bce_loss": -0.46386176347732544,
      "epoch": 2.955,
      "step": 591
    },
    {
      "epoch": 2.96,
      "grad_norm": 34.30171203613281,
      "learning_rate": 4.403125e-05,
      "loss": 2.5324,
      "step": 592
    },
    {
      "bce_eff_w": 0.22895000000000001,
      "bce_loss": -0.46493658423423767,
      "epoch": 2.96,
      "step": 592
    },
    {
      "epoch": 2.965,
      "grad_norm": 29.608734130859375,
      "learning_rate": 4.4000000000000006e-05,
      "loss": 2.8422,
      "step": 593
    },
    {
      "bce_eff_w": 0.22910000000000003,
      "bce_loss": -0.4642534852027893,
      "epoch": 2.965,
      "step": 593
    },
    {
      "epoch": 2.9699999999999998,
      "grad_norm": 22.65367889404297,
      "learning_rate": 4.396875e-05,
      "loss": 1.1718,
      "step": 594
    },
    {
      "bce_eff_w": 0.22925,
      "bce_loss": -0.4634948968887329,
      "epoch": 2.9699999999999998,
      "step": 594
    },
    {
      "epoch": 2.975,
      "grad_norm": 27.21321678161621,
      "learning_rate": 4.3937500000000004e-05,
      "loss": 2.2376,
      "step": 595
    },
    {
      "bce_eff_w": 0.22940000000000002,
      "bce_loss": -0.46386995911598206,
      "epoch": 2.975,
      "step": 595
    },
    {
      "epoch": 2.98,
      "grad_norm": 28.994081497192383,
      "learning_rate": 4.390625000000001e-05,
      "loss": 2.1728,
      "step": 596
    },
    {
      "bce_eff_w": 0.22955,
      "bce_loss": -0.46475812792778015,
      "epoch": 2.98,
      "step": 596
    },
    {
      "epoch": 2.985,
      "grad_norm": 22.636812210083008,
      "learning_rate": 4.3875e-05,
      "loss": 2.9566,
      "step": 597
    },
    {
      "bce_eff_w": 0.22970000000000002,
      "bce_loss": -0.46507611870765686,
      "epoch": 2.985,
      "step": 597
    },
    {
      "epoch": 2.99,
      "grad_norm": 28.90192222595215,
      "learning_rate": 4.384375e-05,
      "loss": 2.16,
      "step": 598
    },
    {
      "bce_eff_w": 0.22985,
      "bce_loss": -0.4649512767791748,
      "epoch": 2.99,
      "step": 598
    },
    {
      "epoch": 2.995,
      "grad_norm": 17.934226989746094,
      "learning_rate": 4.38125e-05,
      "loss": 0.431,
      "step": 599
    },
    {
      "bce_eff_w": 0.23,
      "bce_loss": -0.46475157141685486,
      "epoch": 2.995,
      "step": 599
    },
    {
      "epoch": 3.0,
      "grad_norm": 20.370433807373047,
      "learning_rate": 4.3781250000000004e-05,
      "loss": 2.5295,
      "step": 600
    },
    {
      "bce_eff_w": 0.23015000000000002,
      "bce_loss": -0.46209558844566345,
      "epoch": 3.0,
      "step": 600
    },
    {
      "epoch": 3.005,
      "grad_norm": 20.60677146911621,
      "learning_rate": 4.375e-05,
      "loss": 1.7641,
      "step": 601
    },
    {
      "bce_eff_w": 0.2303,
      "bce_loss": -0.4649839401245117,
      "epoch": 3.005,
      "step": 601
    },
    {
      "epoch": 3.01,
      "grad_norm": 23.587017059326172,
      "learning_rate": 4.371875e-05,
      "loss": 3.3672,
      "step": 602
    },
    {
      "bce_eff_w": 0.23045000000000002,
      "bce_loss": -0.4648139476776123,
      "epoch": 3.01,
      "step": 602
    },
    {
      "epoch": 3.015,
      "grad_norm": 22.990652084350586,
      "learning_rate": 4.3687500000000005e-05,
      "loss": 3.4745,
      "step": 603
    },
    {
      "bce_eff_w": 0.2306,
      "bce_loss": -0.46474048495292664,
      "epoch": 3.015,
      "step": 603
    },
    {
      "epoch": 3.02,
      "grad_norm": 26.878530502319336,
      "learning_rate": 4.365625000000001e-05,
      "loss": 0.9073,
      "step": 604
    },
    {
      "bce_eff_w": 0.23075,
      "bce_loss": -0.46424153447151184,
      "epoch": 3.02,
      "step": 604
    },
    {
      "epoch": 3.025,
      "grad_norm": 23.80826187133789,
      "learning_rate": 4.3625e-05,
      "loss": 1.7469,
      "step": 605
    },
    {
      "bce_eff_w": 0.2309,
      "bce_loss": -0.46530380845069885,
      "epoch": 3.025,
      "step": 605
    },
    {
      "epoch": 3.03,
      "grad_norm": 20.460269927978516,
      "learning_rate": 4.359375e-05,
      "loss": 3.1072,
      "step": 606
    },
    {
      "bce_eff_w": 0.23105,
      "bce_loss": -0.463008850812912,
      "epoch": 3.03,
      "step": 606
    },
    {
      "epoch": 3.035,
      "grad_norm": 28.209775924682617,
      "learning_rate": 4.35625e-05,
      "loss": 2.4389,
      "step": 607
    },
    {
      "bce_eff_w": 0.23120000000000002,
      "bce_loss": -0.4639098644256592,
      "epoch": 3.035,
      "step": 607
    },
    {
      "epoch": 3.04,
      "grad_norm": 30.022375106811523,
      "learning_rate": 4.3531250000000004e-05,
      "loss": 0.8337,
      "step": 608
    },
    {
      "bce_eff_w": 0.23135,
      "bce_loss": -0.4654315412044525,
      "epoch": 3.04,
      "step": 608
    },
    {
      "epoch": 3.045,
      "grad_norm": 18.07111358642578,
      "learning_rate": 4.35e-05,
      "loss": 3.6145,
      "step": 609
    },
    {
      "bce_eff_w": 0.2315,
      "bce_loss": -0.4626409709453583,
      "epoch": 3.045,
      "step": 609
    },
    {
      "epoch": 3.05,
      "grad_norm": 15.693861961364746,
      "learning_rate": 4.346875e-05,
      "loss": 3.8511,
      "step": 610
    },
    {
      "bce_eff_w": 0.23165000000000002,
      "bce_loss": -0.4631381928920746,
      "epoch": 3.05,
      "step": 610
    },
    {
      "epoch": 3.055,
      "grad_norm": 14.018596649169922,
      "learning_rate": 4.3437500000000006e-05,
      "loss": 0.0629,
      "step": 611
    },
    {
      "bce_eff_w": 0.2318,
      "bce_loss": -0.4631505012512207,
      "epoch": 3.055,
      "step": 611
    },
    {
      "epoch": 3.06,
      "grad_norm": 25.041318893432617,
      "learning_rate": 4.340625e-05,
      "loss": 1.2038,
      "step": 612
    },
    {
      "bce_eff_w": 0.23195000000000002,
      "bce_loss": -0.4634746313095093,
      "epoch": 3.06,
      "step": 612
    },
    {
      "epoch": 3.065,
      "grad_norm": 39.07719802856445,
      "learning_rate": 4.3375000000000004e-05,
      "loss": 1.933,
      "step": 613
    },
    {
      "bce_eff_w": 0.2321,
      "bce_loss": -0.46410423517227173,
      "epoch": 3.065,
      "step": 613
    },
    {
      "epoch": 3.07,
      "grad_norm": 27.436405181884766,
      "learning_rate": 4.334375e-05,
      "loss": 1.0184,
      "step": 614
    },
    {
      "bce_eff_w": 0.23225,
      "bce_loss": -0.4648137092590332,
      "epoch": 3.07,
      "step": 614
    },
    {
      "epoch": 3.075,
      "grad_norm": 15.203322410583496,
      "learning_rate": 4.33125e-05,
      "loss": 0.4199,
      "step": 615
    },
    {
      "bce_eff_w": 0.2324,
      "bce_loss": -0.46467018127441406,
      "epoch": 3.075,
      "step": 615
    },
    {
      "epoch": 3.08,
      "grad_norm": 26.917314529418945,
      "learning_rate": 4.328125e-05,
      "loss": 1.7369,
      "step": 616
    },
    {
      "bce_eff_w": 0.23255,
      "bce_loss": -0.4590699374675751,
      "epoch": 3.08,
      "step": 616
    },
    {
      "epoch": 3.085,
      "grad_norm": 25.427162170410156,
      "learning_rate": 4.325e-05,
      "loss": 1.5155,
      "step": 617
    },
    {
      "bce_eff_w": 0.23270000000000002,
      "bce_loss": -0.46217989921569824,
      "epoch": 3.085,
      "step": 617
    },
    {
      "epoch": 3.09,
      "grad_norm": 23.2714900970459,
      "learning_rate": 4.3218750000000004e-05,
      "loss": 1.8996,
      "step": 618
    },
    {
      "bce_eff_w": 0.23285,
      "bce_loss": -0.4646148085594177,
      "epoch": 3.09,
      "step": 618
    },
    {
      "epoch": 3.095,
      "grad_norm": 40.883792877197266,
      "learning_rate": 4.3187500000000006e-05,
      "loss": 2.2517,
      "step": 619
    },
    {
      "bce_eff_w": 0.233,
      "bce_loss": -0.46447524428367615,
      "epoch": 3.095,
      "step": 619
    },
    {
      "epoch": 3.1,
      "grad_norm": 23.811100006103516,
      "learning_rate": 4.315625e-05,
      "loss": 0.7689,
      "step": 620
    },
    {
      "bce_eff_w": 0.23315000000000002,
      "bce_loss": -0.4644320607185364,
      "epoch": 3.1,
      "step": 620
    },
    {
      "epoch": 3.105,
      "grad_norm": 28.03868865966797,
      "learning_rate": 4.3125000000000005e-05,
      "loss": 1.0614,
      "step": 621
    },
    {
      "bce_eff_w": 0.2333,
      "bce_loss": -0.46296900510787964,
      "epoch": 3.105,
      "step": 621
    },
    {
      "epoch": 3.11,
      "grad_norm": 23.365890502929688,
      "learning_rate": 4.309375e-05,
      "loss": 1.2942,
      "step": 622
    },
    {
      "bce_eff_w": 0.23345000000000002,
      "bce_loss": -0.4632100760936737,
      "epoch": 3.11,
      "step": 622
    },
    {
      "epoch": 3.115,
      "grad_norm": 12.01878833770752,
      "learning_rate": 4.30625e-05,
      "loss": 4.0582,
      "step": 623
    },
    {
      "bce_eff_w": 0.2336,
      "bce_loss": -0.46437087655067444,
      "epoch": 3.115,
      "step": 623
    },
    {
      "epoch": 3.12,
      "grad_norm": 22.813108444213867,
      "learning_rate": 4.303125e-05,
      "loss": 0.928,
      "step": 624
    },
    {
      "bce_eff_w": 0.23375,
      "bce_loss": -0.4644817113876343,
      "epoch": 3.12,
      "step": 624
    },
    {
      "epoch": 3.125,
      "grad_norm": 29.451629638671875,
      "learning_rate": 4.3e-05,
      "loss": 1.0556,
      "step": 625
    },
    {
      "bce_eff_w": 0.2339,
      "bce_loss": -0.4628268778324127,
      "epoch": 3.125,
      "step": 625
    },
    {
      "epoch": 3.13,
      "grad_norm": 21.91185760498047,
      "learning_rate": 4.2968750000000004e-05,
      "loss": 2.085,
      "step": 626
    },
    {
      "bce_eff_w": 0.23405,
      "bce_loss": -0.45939168334007263,
      "epoch": 3.13,
      "step": 626
    },
    {
      "epoch": 3.135,
      "grad_norm": 19.797576904296875,
      "learning_rate": 4.29375e-05,
      "loss": 2.5437,
      "step": 627
    },
    {
      "bce_eff_w": 0.23420000000000002,
      "bce_loss": -0.4639313220977783,
      "epoch": 3.135,
      "step": 627
    },
    {
      "epoch": 3.14,
      "grad_norm": 19.353046417236328,
      "learning_rate": 4.290625e-05,
      "loss": 4.1226,
      "step": 628
    },
    {
      "bce_eff_w": 0.23435,
      "bce_loss": -0.4640273451805115,
      "epoch": 3.14,
      "step": 628
    },
    {
      "epoch": 3.145,
      "grad_norm": 20.295217514038086,
      "learning_rate": 4.2875000000000005e-05,
      "loss": 2.8424,
      "step": 629
    },
    {
      "bce_eff_w": 0.23450000000000001,
      "bce_loss": -0.4651569128036499,
      "epoch": 3.145,
      "step": 629
    },
    {
      "epoch": 3.15,
      "grad_norm": 17.328113555908203,
      "learning_rate": 4.284375000000001e-05,
      "loss": 3.5912,
      "step": 630
    },
    {
      "bce_eff_w": 0.23465000000000003,
      "bce_loss": -0.46344423294067383,
      "epoch": 3.15,
      "step": 630
    },
    {
      "epoch": 3.155,
      "grad_norm": 25.6234130859375,
      "learning_rate": 4.28125e-05,
      "loss": 1.1742,
      "step": 631
    },
    {
      "bce_eff_w": 0.2348,
      "bce_loss": -0.4651135802268982,
      "epoch": 3.155,
      "step": 631
    },
    {
      "epoch": 3.16,
      "grad_norm": 26.95964241027832,
      "learning_rate": 4.278125e-05,
      "loss": 3.0177,
      "step": 632
    },
    {
      "bce_eff_w": 0.23495000000000002,
      "bce_loss": -0.46100178360939026,
      "epoch": 3.16,
      "step": 632
    },
    {
      "epoch": 3.165,
      "grad_norm": 20.867755889892578,
      "learning_rate": 4.275e-05,
      "loss": 1.3225,
      "step": 633
    },
    {
      "bce_eff_w": 0.2351,
      "bce_loss": -0.46037933230400085,
      "epoch": 3.165,
      "step": 633
    },
    {
      "epoch": 3.17,
      "grad_norm": 20.57203483581543,
      "learning_rate": 4.2718750000000005e-05,
      "loss": 1.0506,
      "step": 634
    },
    {
      "bce_eff_w": 0.23525000000000001,
      "bce_loss": -0.464804083108902,
      "epoch": 3.17,
      "step": 634
    },
    {
      "epoch": 3.175,
      "grad_norm": 5.397274494171143,
      "learning_rate": 4.26875e-05,
      "loss": -0.0265,
      "step": 635
    },
    {
      "bce_eff_w": 0.2354,
      "bce_loss": -0.46333420276641846,
      "epoch": 3.175,
      "step": 635
    },
    {
      "epoch": 3.18,
      "grad_norm": 17.729167938232422,
      "learning_rate": 4.2656250000000003e-05,
      "loss": 3.4504,
      "step": 636
    },
    {
      "bce_eff_w": 0.23555,
      "bce_loss": -0.46486103534698486,
      "epoch": 3.18,
      "step": 636
    },
    {
      "epoch": 3.185,
      "grad_norm": 25.35613441467285,
      "learning_rate": 4.2625000000000006e-05,
      "loss": 0.5523,
      "step": 637
    },
    {
      "bce_eff_w": 0.23570000000000002,
      "bce_loss": -0.46419692039489746,
      "epoch": 3.185,
      "step": 637
    },
    {
      "epoch": 3.19,
      "grad_norm": 19.278179168701172,
      "learning_rate": 4.259375e-05,
      "loss": 3.1142,
      "step": 638
    },
    {
      "bce_eff_w": 0.23585,
      "bce_loss": -0.4645613133907318,
      "epoch": 3.19,
      "step": 638
    },
    {
      "epoch": 3.195,
      "grad_norm": 23.398357391357422,
      "learning_rate": 4.25625e-05,
      "loss": 0.6887,
      "step": 639
    },
    {
      "bce_eff_w": 0.23600000000000002,
      "bce_loss": -0.46292436122894287,
      "epoch": 3.195,
      "step": 639
    },
    {
      "epoch": 3.2,
      "grad_norm": 21.352352142333984,
      "learning_rate": 4.253125e-05,
      "loss": 2.8537,
      "step": 640
    },
    {
      "bce_eff_w": 0.23615,
      "bce_loss": -0.4647827446460724,
      "epoch": 3.2,
      "step": 640
    },
    {
      "epoch": 3.205,
      "grad_norm": 18.144268035888672,
      "learning_rate": 4.25e-05,
      "loss": 2.4184,
      "step": 641
    },
    {
      "bce_eff_w": 0.2363,
      "bce_loss": -0.46188071370124817,
      "epoch": 3.205,
      "step": 641
    },
    {
      "epoch": 3.21,
      "grad_norm": 15.136992454528809,
      "learning_rate": 4.246875e-05,
      "loss": 4.3813,
      "step": 642
    },
    {
      "bce_eff_w": 0.23645,
      "bce_loss": -0.4615916609764099,
      "epoch": 3.21,
      "step": 642
    },
    {
      "epoch": 3.215,
      "grad_norm": 24.812702178955078,
      "learning_rate": 4.24375e-05,
      "loss": 3.6666,
      "step": 643
    },
    {
      "bce_eff_w": 0.2366,
      "bce_loss": -0.46443241834640503,
      "epoch": 3.215,
      "step": 643
    },
    {
      "epoch": 3.22,
      "grad_norm": 24.562301635742188,
      "learning_rate": 4.2406250000000004e-05,
      "loss": 0.6929,
      "step": 644
    },
    {
      "bce_eff_w": 0.23675000000000002,
      "bce_loss": -0.46446505188941956,
      "epoch": 3.22,
      "step": 644
    },
    {
      "epoch": 3.225,
      "grad_norm": 22.596872329711914,
      "learning_rate": 4.237500000000001e-05,
      "loss": 3.1404,
      "step": 645
    },
    {
      "bce_eff_w": 0.2369,
      "bce_loss": -0.4639952480792999,
      "epoch": 3.225,
      "step": 645
    },
    {
      "epoch": 3.23,
      "grad_norm": 24.908170700073242,
      "learning_rate": 4.234375e-05,
      "loss": 2.4468,
      "step": 646
    },
    {
      "bce_eff_w": 0.23705,
      "bce_loss": -0.46390432119369507,
      "epoch": 3.23,
      "step": 646
    },
    {
      "epoch": 3.235,
      "grad_norm": 21.785099029541016,
      "learning_rate": 4.23125e-05,
      "loss": 0.7459,
      "step": 647
    },
    {
      "bce_eff_w": 0.23720000000000002,
      "bce_loss": -0.46485435962677,
      "epoch": 3.235,
      "step": 647
    },
    {
      "epoch": 3.24,
      "grad_norm": 19.0146541595459,
      "learning_rate": 4.228125e-05,
      "loss": 2.8832,
      "step": 648
    },
    {
      "bce_eff_w": 0.23735,
      "bce_loss": -0.46421700716018677,
      "epoch": 3.24,
      "step": 648
    },
    {
      "epoch": 3.245,
      "grad_norm": 21.346616744995117,
      "learning_rate": 4.2250000000000004e-05,
      "loss": 1.4222,
      "step": 649
    },
    {
      "bce_eff_w": 0.23750000000000002,
      "bce_loss": -0.4632713496685028,
      "epoch": 3.245,
      "step": 649
    },
    {
      "epoch": 3.25,
      "grad_norm": 31.026351928710938,
      "learning_rate": 4.221875e-05,
      "loss": 2.0488,
      "step": 650
    },
    {
      "bce_eff_w": 0.23765,
      "bce_loss": -0.464657187461853,
      "epoch": 3.25,
      "step": 650
    },
    {
      "epoch": 3.255,
      "grad_norm": 30.151691436767578,
      "learning_rate": 4.21875e-05,
      "loss": 0.8395,
      "step": 651
    },
    {
      "bce_eff_w": 0.2378,
      "bce_loss": -0.46504220366477966,
      "epoch": 3.255,
      "step": 651
    },
    {
      "epoch": 3.26,
      "grad_norm": 42.49837112426758,
      "learning_rate": 4.2156250000000005e-05,
      "loss": 1.1098,
      "step": 652
    },
    {
      "bce_eff_w": 0.23795,
      "bce_loss": -0.4600179195404053,
      "epoch": 3.26,
      "step": 652
    },
    {
      "epoch": 3.265,
      "grad_norm": 19.572555541992188,
      "learning_rate": 4.2125e-05,
      "loss": 1.7842,
      "step": 653
    },
    {
      "bce_eff_w": 0.2381,
      "bce_loss": -0.463902086019516,
      "epoch": 3.265,
      "step": 653
    },
    {
      "epoch": 3.27,
      "grad_norm": 47.54218673706055,
      "learning_rate": 4.209375e-05,
      "loss": 4.32,
      "step": 654
    },
    {
      "bce_eff_w": 0.23825000000000002,
      "bce_loss": -0.46501871943473816,
      "epoch": 3.27,
      "step": 654
    },
    {
      "epoch": 3.275,
      "grad_norm": 22.30493927001953,
      "learning_rate": 4.2062500000000006e-05,
      "loss": 1.1303,
      "step": 655
    },
    {
      "bce_eff_w": 0.2384,
      "bce_loss": -0.4573453962802887,
      "epoch": 3.275,
      "step": 655
    },
    {
      "epoch": 3.2800000000000002,
      "grad_norm": 15.128446578979492,
      "learning_rate": 4.203125e-05,
      "loss": 0.4975,
      "step": 656
    },
    {
      "bce_eff_w": 0.23855,
      "bce_loss": -0.4645366370677948,
      "epoch": 3.2800000000000002,
      "step": 656
    },
    {
      "epoch": 3.285,
      "grad_norm": 19.42327880859375,
      "learning_rate": 4.2e-05,
      "loss": 2.2968,
      "step": 657
    },
    {
      "bce_eff_w": 0.23870000000000002,
      "bce_loss": -0.46431615948677063,
      "epoch": 3.285,
      "step": 657
    },
    {
      "epoch": 3.29,
      "grad_norm": 21.361217498779297,
      "learning_rate": 4.196875e-05,
      "loss": 1.3418,
      "step": 658
    },
    {
      "bce_eff_w": 0.23885,
      "bce_loss": -0.4635939300060272,
      "epoch": 3.29,
      "step": 658
    },
    {
      "epoch": 3.295,
      "grad_norm": 29.328248977661133,
      "learning_rate": 4.19375e-05,
      "loss": 1.5063,
      "step": 659
    },
    {
      "bce_eff_w": 0.23900000000000002,
      "bce_loss": -0.4639740288257599,
      "epoch": 3.295,
      "step": 659
    },
    {
      "epoch": 3.3,
      "grad_norm": 19.693300247192383,
      "learning_rate": 4.1906250000000006e-05,
      "loss": 2.8906,
      "step": 660
    },
    {
      "bce_eff_w": 0.23915,
      "bce_loss": -0.46484917402267456,
      "epoch": 3.3,
      "step": 660
    },
    {
      "epoch": 3.305,
      "grad_norm": 26.58542251586914,
      "learning_rate": 4.1875e-05,
      "loss": 1.8056,
      "step": 661
    },
    {
      "bce_eff_w": 0.2393,
      "bce_loss": -0.46444082260131836,
      "epoch": 3.305,
      "step": 661
    },
    {
      "epoch": 3.31,
      "grad_norm": 27.92359161376953,
      "learning_rate": 4.1843750000000004e-05,
      "loss": 1.6735,
      "step": 662
    },
    {
      "bce_eff_w": 0.23945,
      "bce_loss": -0.46495580673217773,
      "epoch": 3.31,
      "step": 662
    },
    {
      "epoch": 3.315,
      "grad_norm": 33.782291412353516,
      "learning_rate": 4.181250000000001e-05,
      "loss": 1.7114,
      "step": 663
    },
    {
      "bce_eff_w": 0.2396,
      "bce_loss": -0.46458297967910767,
      "epoch": 3.315,
      "step": 663
    },
    {
      "epoch": 3.32,
      "grad_norm": 25.4008731842041,
      "learning_rate": 4.178125e-05,
      "loss": 2.2927,
      "step": 664
    },
    {
      "bce_eff_w": 0.23975000000000002,
      "bce_loss": -0.4647247791290283,
      "epoch": 3.32,
      "step": 664
    },
    {
      "epoch": 3.325,
      "grad_norm": 20.43203353881836,
      "learning_rate": 4.175e-05,
      "loss": 1.0503,
      "step": 665
    },
    {
      "bce_eff_w": 0.2399,
      "bce_loss": -0.46376532316207886,
      "epoch": 3.325,
      "step": 665
    },
    {
      "epoch": 3.33,
      "grad_norm": 21.942609786987305,
      "learning_rate": 4.171875e-05,
      "loss": 1.3832,
      "step": 666
    },
    {
      "bce_eff_w": 0.24005,
      "bce_loss": -0.46447211503982544,
      "epoch": 3.33,
      "step": 666
    },
    {
      "epoch": 3.335,
      "grad_norm": 24.53567123413086,
      "learning_rate": 4.1687500000000004e-05,
      "loss": 0.9339,
      "step": 667
    },
    {
      "bce_eff_w": 0.24020000000000002,
      "bce_loss": -0.46453115344047546,
      "epoch": 3.335,
      "step": 667
    },
    {
      "epoch": 3.34,
      "grad_norm": 19.673397064208984,
      "learning_rate": 4.165625e-05,
      "loss": 2.038,
      "step": 668
    },
    {
      "bce_eff_w": 0.24035,
      "bce_loss": -0.4645915627479553,
      "epoch": 3.34,
      "step": 668
    },
    {
      "epoch": 3.3449999999999998,
      "grad_norm": 19.46537971496582,
      "learning_rate": 4.1625e-05,
      "loss": 0.6406,
      "step": 669
    },
    {
      "bce_eff_w": 0.24050000000000002,
      "bce_loss": -0.4581097960472107,
      "epoch": 3.3449999999999998,
      "step": 669
    },
    {
      "epoch": 3.35,
      "grad_norm": 19.57797622680664,
      "learning_rate": 4.1593750000000005e-05,
      "loss": 1.1461,
      "step": 670
    },
    {
      "bce_eff_w": 0.24065,
      "bce_loss": -0.4632793068885803,
      "epoch": 3.35,
      "step": 670
    },
    {
      "epoch": 3.355,
      "grad_norm": 14.074564933776855,
      "learning_rate": 4.156250000000001e-05,
      "loss": 4.0251,
      "step": 671
    },
    {
      "bce_eff_w": 0.24080000000000001,
      "bce_loss": -0.4647900462150574,
      "epoch": 3.355,
      "step": 671
    },
    {
      "epoch": 3.36,
      "grad_norm": 36.960838317871094,
      "learning_rate": 4.1531249999999996e-05,
      "loss": 0.3137,
      "step": 672
    },
    {
      "bce_eff_w": 0.24095,
      "bce_loss": -0.4642573595046997,
      "epoch": 3.36,
      "step": 672
    },
    {
      "epoch": 3.365,
      "grad_norm": 27.9400634765625,
      "learning_rate": 4.15e-05,
      "loss": 1.632,
      "step": 673
    },
    {
      "bce_eff_w": 0.2411,
      "bce_loss": -0.46475571393966675,
      "epoch": 3.365,
      "step": 673
    },
    {
      "epoch": 3.37,
      "grad_norm": 21.845211029052734,
      "learning_rate": 4.146875e-05,
      "loss": 1.5564,
      "step": 674
    },
    {
      "bce_eff_w": 0.24125000000000002,
      "bce_loss": -0.46541455388069153,
      "epoch": 3.37,
      "step": 674
    },
    {
      "epoch": 3.375,
      "grad_norm": 20.85030174255371,
      "learning_rate": 4.1437500000000004e-05,
      "loss": 2.4453,
      "step": 675
    },
    {
      "bce_eff_w": 0.2414,
      "bce_loss": -0.465221643447876,
      "epoch": 3.375,
      "step": 675
    },
    {
      "epoch": 3.38,
      "grad_norm": 17.89482879638672,
      "learning_rate": 4.140625e-05,
      "loss": 2.6462,
      "step": 676
    },
    {
      "bce_eff_w": 0.24155000000000001,
      "bce_loss": -0.4616635739803314,
      "epoch": 3.38,
      "step": 676
    },
    {
      "epoch": 3.385,
      "grad_norm": 29.23933982849121,
      "learning_rate": 4.1375e-05,
      "loss": 0.8862,
      "step": 677
    },
    {
      "bce_eff_w": 0.24170000000000003,
      "bce_loss": -0.4642726480960846,
      "epoch": 3.385,
      "step": 677
    },
    {
      "epoch": 3.39,
      "grad_norm": 21.24431800842285,
      "learning_rate": 4.1343750000000005e-05,
      "loss": 2.105,
      "step": 678
    },
    {
      "bce_eff_w": 0.24185,
      "bce_loss": -0.46467453241348267,
      "epoch": 3.39,
      "step": 678
    },
    {
      "epoch": 3.395,
      "grad_norm": 19.685794830322266,
      "learning_rate": 4.13125e-05,
      "loss": 1.5806,
      "step": 679
    },
    {
      "bce_eff_w": 0.24200000000000002,
      "bce_loss": -0.46406564116477966,
      "epoch": 3.395,
      "step": 679
    },
    {
      "epoch": 3.4,
      "grad_norm": 16.66815185546875,
      "learning_rate": 4.1281250000000004e-05,
      "loss": 0.4558,
      "step": 680
    },
    {
      "bce_eff_w": 0.24215,
      "bce_loss": -0.46512115001678467,
      "epoch": 3.4,
      "step": 680
    },
    {
      "epoch": 3.4050000000000002,
      "grad_norm": 26.02531623840332,
      "learning_rate": 4.125e-05,
      "loss": 2.0517,
      "step": 681
    },
    {
      "bce_eff_w": 0.24230000000000002,
      "bce_loss": -0.4653376340866089,
      "epoch": 3.4050000000000002,
      "step": 681
    },
    {
      "epoch": 3.41,
      "grad_norm": 19.620140075683594,
      "learning_rate": 4.121875e-05,
      "loss": 2.8325,
      "step": 682
    },
    {
      "bce_eff_w": 0.24245,
      "bce_loss": -0.45985233783721924,
      "epoch": 3.41,
      "step": 682
    },
    {
      "epoch": 3.415,
      "grad_norm": 20.021825790405273,
      "learning_rate": 4.11875e-05,
      "loss": 1.0481,
      "step": 683
    },
    {
      "bce_eff_w": 0.2426,
      "bce_loss": -0.46387410163879395,
      "epoch": 3.415,
      "step": 683
    },
    {
      "epoch": 3.42,
      "grad_norm": 37.4508171081543,
      "learning_rate": 4.115625e-05,
      "loss": 1.3644,
      "step": 684
    },
    {
      "bce_eff_w": 0.24275000000000002,
      "bce_loss": -0.4600067138671875,
      "epoch": 3.42,
      "step": 684
    },
    {
      "epoch": 3.425,
      "grad_norm": 21.363754272460938,
      "learning_rate": 4.1125000000000004e-05,
      "loss": 1.9841,
      "step": 685
    },
    {
      "bce_eff_w": 0.2429,
      "bce_loss": -0.4644602835178375,
      "epoch": 3.425,
      "step": 685
    },
    {
      "epoch": 3.43,
      "grad_norm": 25.16294288635254,
      "learning_rate": 4.1093750000000006e-05,
      "loss": 1.4576,
      "step": 686
    },
    {
      "bce_eff_w": 0.24305000000000002,
      "bce_loss": -0.46386927366256714,
      "epoch": 3.43,
      "step": 686
    },
    {
      "epoch": 3.435,
      "grad_norm": 28.398195266723633,
      "learning_rate": 4.10625e-05,
      "loss": 1.0387,
      "step": 687
    },
    {
      "bce_eff_w": 0.2432,
      "bce_loss": -0.4640653133392334,
      "epoch": 3.435,
      "step": 687
    },
    {
      "epoch": 3.44,
      "grad_norm": 28.41440773010254,
      "learning_rate": 4.1031250000000005e-05,
      "loss": 2.683,
      "step": 688
    },
    {
      "bce_eff_w": 0.24335,
      "bce_loss": -0.46261975169181824,
      "epoch": 3.44,
      "step": 688
    },
    {
      "epoch": 3.445,
      "grad_norm": 21.71752166748047,
      "learning_rate": 4.1e-05,
      "loss": 2.2636,
      "step": 689
    },
    {
      "bce_eff_w": 0.2435,
      "bce_loss": -0.4553408622741699,
      "epoch": 3.445,
      "step": 689
    },
    {
      "epoch": 3.45,
      "grad_norm": 14.195394515991211,
      "learning_rate": 4.096875e-05,
      "loss": 0.6415,
      "step": 690
    },
    {
      "bce_eff_w": 0.24365,
      "bce_loss": -0.4640435576438904,
      "epoch": 3.45,
      "step": 690
    },
    {
      "epoch": 3.455,
      "grad_norm": 29.04599952697754,
      "learning_rate": 4.09375e-05,
      "loss": 1.5816,
      "step": 691
    },
    {
      "bce_eff_w": 0.24380000000000002,
      "bce_loss": -0.4640257656574249,
      "epoch": 3.455,
      "step": 691
    },
    {
      "epoch": 3.46,
      "grad_norm": 29.651325225830078,
      "learning_rate": 4.090625e-05,
      "loss": 1.4039,
      "step": 692
    },
    {
      "bce_eff_w": 0.24395,
      "bce_loss": -0.4641270041465759,
      "epoch": 3.46,
      "step": 692
    },
    {
      "epoch": 3.465,
      "grad_norm": 22.309904098510742,
      "learning_rate": 4.0875000000000004e-05,
      "loss": 0.9646,
      "step": 693
    },
    {
      "bce_eff_w": 0.2441,
      "bce_loss": -0.4637049734592438,
      "epoch": 3.465,
      "step": 693
    },
    {
      "epoch": 3.4699999999999998,
      "grad_norm": 25.519981384277344,
      "learning_rate": 4.084375e-05,
      "loss": 2.8482,
      "step": 694
    },
    {
      "bce_eff_w": 0.24425000000000002,
      "bce_loss": -0.4622459411621094,
      "epoch": 3.4699999999999998,
      "step": 694
    },
    {
      "epoch": 3.475,
      "grad_norm": 33.14363098144531,
      "learning_rate": 4.08125e-05,
      "loss": 2.3072,
      "step": 695
    },
    {
      "bce_eff_w": 0.2444,
      "bce_loss": -0.4619182050228119,
      "epoch": 3.475,
      "step": 695
    },
    {
      "epoch": 3.48,
      "grad_norm": 16.718252182006836,
      "learning_rate": 4.0781250000000005e-05,
      "loss": 0.9391,
      "step": 696
    },
    {
      "bce_eff_w": 0.24455000000000002,
      "bce_loss": -0.46406495571136475,
      "epoch": 3.48,
      "step": 696
    },
    {
      "epoch": 3.485,
      "grad_norm": 35.64864730834961,
      "learning_rate": 4.075e-05,
      "loss": 0.9984,
      "step": 697
    },
    {
      "bce_eff_w": 0.2447,
      "bce_loss": -0.46122270822525024,
      "epoch": 3.485,
      "step": 697
    },
    {
      "epoch": 3.49,
      "grad_norm": 18.358736038208008,
      "learning_rate": 4.071875e-05,
      "loss": 1.6764,
      "step": 698
    },
    {
      "bce_eff_w": 0.24485,
      "bce_loss": -0.4635900855064392,
      "epoch": 3.49,
      "step": 698
    },
    {
      "epoch": 3.495,
      "grad_norm": 16.126611709594727,
      "learning_rate": 4.06875e-05,
      "loss": 4.1389,
      "step": 699
    },
    {
      "bce_eff_w": 0.245,
      "bce_loss": -0.4637942612171173,
      "epoch": 3.495,
      "step": 699
    },
    {
      "epoch": 3.5,
      "grad_norm": 35.370147705078125,
      "learning_rate": 4.065625e-05,
      "loss": 1.1091,
      "step": 700
    },
    {
      "bce_eff_w": 0.24515,
      "bce_loss": -0.46234866976737976,
      "epoch": 3.5,
      "step": 700
    },
    {
      "epoch": 3.505,
      "grad_norm": 23.21610450744629,
      "learning_rate": 4.0625000000000005e-05,
      "loss": 1.759,
      "step": 701
    },
    {
      "bce_eff_w": 0.24530000000000002,
      "bce_loss": -0.4649752080440521,
      "epoch": 3.505,
      "step": 701
    },
    {
      "epoch": 3.51,
      "grad_norm": 28.436277389526367,
      "learning_rate": 4.059375e-05,
      "loss": 2.1608,
      "step": 702
    },
    {
      "bce_eff_w": 0.24545,
      "bce_loss": -0.462118536233902,
      "epoch": 3.51,
      "step": 702
    },
    {
      "epoch": 3.515,
      "grad_norm": 18.819562911987305,
      "learning_rate": 4.0562500000000003e-05,
      "loss": 1.7393,
      "step": 703
    },
    {
      "bce_eff_w": 0.2456,
      "bce_loss": -0.46309807896614075,
      "epoch": 3.515,
      "step": 703
    },
    {
      "epoch": 3.52,
      "grad_norm": 32.50727462768555,
      "learning_rate": 4.0531250000000006e-05,
      "loss": 1.1784,
      "step": 704
    },
    {
      "bce_eff_w": 0.24575000000000002,
      "bce_loss": -0.46412521600723267,
      "epoch": 3.52,
      "step": 704
    },
    {
      "epoch": 3.525,
      "grad_norm": 19.472923278808594,
      "learning_rate": 4.05e-05,
      "loss": 1.4087,
      "step": 705
    },
    {
      "bce_eff_w": 0.2459,
      "bce_loss": -0.46395745873451233,
      "epoch": 3.525,
      "step": 705
    },
    {
      "epoch": 3.5300000000000002,
      "grad_norm": 27.047317504882812,
      "learning_rate": 4.046875e-05,
      "loss": 1.3372,
      "step": 706
    },
    {
      "bce_eff_w": 0.24605000000000002,
      "bce_loss": -0.46312037110328674,
      "epoch": 3.5300000000000002,
      "step": 706
    },
    {
      "epoch": 3.535,
      "grad_norm": 14.17323112487793,
      "learning_rate": 4.04375e-05,
      "loss": 3.8577,
      "step": 707
    },
    {
      "bce_eff_w": 0.2462,
      "bce_loss": -0.4633356034755707,
      "epoch": 3.535,
      "step": 707
    },
    {
      "epoch": 3.54,
      "grad_norm": 27.653583526611328,
      "learning_rate": 4.040625e-05,
      "loss": 3.3616,
      "step": 708
    },
    {
      "bce_eff_w": 0.24635,
      "bce_loss": -0.4608267843723297,
      "epoch": 3.54,
      "step": 708
    },
    {
      "epoch": 3.545,
      "grad_norm": 25.797805786132812,
      "learning_rate": 4.0375e-05,
      "loss": 3.0908,
      "step": 709
    },
    {
      "bce_eff_w": 0.2465,
      "bce_loss": -0.46439188718795776,
      "epoch": 3.545,
      "step": 709
    },
    {
      "epoch": 3.55,
      "grad_norm": 33.911163330078125,
      "learning_rate": 4.034375e-05,
      "loss": 1.157,
      "step": 710
    },
    {
      "bce_eff_w": 0.24665,
      "bce_loss": -0.4628027081489563,
      "epoch": 3.55,
      "step": 710
    },
    {
      "epoch": 3.555,
      "grad_norm": 14.389419555664062,
      "learning_rate": 4.0312500000000004e-05,
      "loss": 0.6447,
      "step": 711
    },
    {
      "bce_eff_w": 0.24680000000000002,
      "bce_loss": -0.46093735098838806,
      "epoch": 3.555,
      "step": 711
    },
    {
      "epoch": 3.56,
      "grad_norm": 38.89711380004883,
      "learning_rate": 4.028125000000001e-05,
      "loss": 1.3801,
      "step": 712
    },
    {
      "bce_eff_w": 0.24695,
      "bce_loss": -0.46405667066574097,
      "epoch": 3.56,
      "step": 712
    },
    {
      "epoch": 3.565,
      "grad_norm": 18.7733154296875,
      "learning_rate": 4.025e-05,
      "loss": 0.3735,
      "step": 713
    },
    {
      "bce_eff_w": 0.24710000000000001,
      "bce_loss": -0.4646863043308258,
      "epoch": 3.565,
      "step": 713
    },
    {
      "epoch": 3.57,
      "grad_norm": 21.025875091552734,
      "learning_rate": 4.021875e-05,
      "loss": 2.401,
      "step": 714
    },
    {
      "bce_eff_w": 0.24725000000000003,
      "bce_loss": -0.4643503725528717,
      "epoch": 3.57,
      "step": 714
    },
    {
      "epoch": 3.575,
      "grad_norm": 22.45489501953125,
      "learning_rate": 4.01875e-05,
      "loss": 2.6712,
      "step": 715
    },
    {
      "bce_eff_w": 0.2474,
      "bce_loss": -0.4611591100692749,
      "epoch": 3.575,
      "step": 715
    },
    {
      "epoch": 3.58,
      "grad_norm": 25.55322265625,
      "learning_rate": 4.0156250000000004e-05,
      "loss": 1.2805,
      "step": 716
    },
    {
      "bce_eff_w": 0.24755000000000002,
      "bce_loss": -0.4634450078010559,
      "epoch": 3.58,
      "step": 716
    },
    {
      "epoch": 3.585,
      "grad_norm": 32.48051452636719,
      "learning_rate": 4.0125e-05,
      "loss": 1.5336,
      "step": 717
    },
    {
      "bce_eff_w": 0.2477,
      "bce_loss": -0.4642985165119171,
      "epoch": 3.585,
      "step": 717
    },
    {
      "epoch": 3.59,
      "grad_norm": 23.48603630065918,
      "learning_rate": 4.009375e-05,
      "loss": 0.8191,
      "step": 718
    },
    {
      "bce_eff_w": 0.24785000000000001,
      "bce_loss": -0.46464988589286804,
      "epoch": 3.59,
      "step": 718
    },
    {
      "epoch": 3.5949999999999998,
      "grad_norm": 24.584938049316406,
      "learning_rate": 4.0062500000000005e-05,
      "loss": 0.9716,
      "step": 719
    },
    {
      "bce_eff_w": 0.248,
      "bce_loss": -0.46413686871528625,
      "epoch": 3.5949999999999998,
      "step": 719
    },
    {
      "epoch": 3.6,
      "grad_norm": 25.341535568237305,
      "learning_rate": 4.003125e-05,
      "loss": 1.479,
      "step": 720
    },
    {
      "bce_eff_w": 0.24815,
      "bce_loss": -0.4646797776222229,
      "epoch": 3.6,
      "step": 720
    },
    {
      "epoch": 3.605,
      "grad_norm": 30.424882888793945,
      "learning_rate": 4e-05,
      "loss": 2.3459,
      "step": 721
    },
    {
      "bce_eff_w": 0.24830000000000002,
      "bce_loss": -0.459494411945343,
      "epoch": 3.605,
      "step": 721
    },
    {
      "epoch": 3.61,
      "grad_norm": 18.9650936126709,
      "learning_rate": 3.996875e-05,
      "loss": 3.0989,
      "step": 722
    },
    {
      "bce_eff_w": 0.24845,
      "bce_loss": -0.46369966864585876,
      "epoch": 3.61,
      "step": 722
    },
    {
      "epoch": 3.615,
      "grad_norm": 17.048587799072266,
      "learning_rate": 3.99375e-05,
      "loss": 0.4517,
      "step": 723
    },
    {
      "bce_eff_w": 0.24860000000000002,
      "bce_loss": -0.4631596505641937,
      "epoch": 3.615,
      "step": 723
    },
    {
      "epoch": 3.62,
      "grad_norm": 35.79137420654297,
      "learning_rate": 3.990625e-05,
      "loss": 1.8343,
      "step": 724
    },
    {
      "bce_eff_w": 0.24875000000000003,
      "bce_loss": -0.46463772654533386,
      "epoch": 3.62,
      "step": 724
    },
    {
      "epoch": 3.625,
      "grad_norm": 17.10588836669922,
      "learning_rate": 3.9875e-05,
      "loss": 0.4941,
      "step": 725
    },
    {
      "bce_eff_w": 0.2489,
      "bce_loss": -0.4609038829803467,
      "epoch": 3.625,
      "step": 725
    },
    {
      "epoch": 3.63,
      "grad_norm": 19.3151798248291,
      "learning_rate": 3.984375e-05,
      "loss": 2.7073,
      "step": 726
    },
    {
      "bce_eff_w": 0.24905000000000002,
      "bce_loss": -0.46458443999290466,
      "epoch": 3.63,
      "step": 726
    },
    {
      "epoch": 3.635,
      "grad_norm": 25.784536361694336,
      "learning_rate": 3.9812500000000005e-05,
      "loss": 1.8409,
      "step": 727
    },
    {
      "bce_eff_w": 0.2492,
      "bce_loss": -0.46414151787757874,
      "epoch": 3.635,
      "step": 727
    },
    {
      "epoch": 3.64,
      "grad_norm": 55.5394287109375,
      "learning_rate": 3.978125e-05,
      "loss": 2.6287,
      "step": 728
    },
    {
      "bce_eff_w": 0.24935000000000002,
      "bce_loss": -0.45995819568634033,
      "epoch": 3.64,
      "step": 728
    },
    {
      "epoch": 3.645,
      "grad_norm": 18.966501235961914,
      "learning_rate": 3.9750000000000004e-05,
      "loss": 2.0404,
      "step": 729
    },
    {
      "bce_eff_w": 0.2495,
      "bce_loss": -0.4646470546722412,
      "epoch": 3.645,
      "step": 729
    },
    {
      "epoch": 3.65,
      "grad_norm": 18.971960067749023,
      "learning_rate": 3.9718750000000007e-05,
      "loss": 3.1627,
      "step": 730
    },
    {
      "bce_eff_w": 0.24965,
      "bce_loss": -0.4647798538208008,
      "epoch": 3.65,
      "step": 730
    },
    {
      "epoch": 3.6550000000000002,
      "grad_norm": 20.585695266723633,
      "learning_rate": 3.96875e-05,
      "loss": 1.8526,
      "step": 731
    },
    {
      "bce_eff_w": 0.24980000000000002,
      "bce_loss": -0.46417638659477234,
      "epoch": 3.6550000000000002,
      "step": 731
    },
    {
      "epoch": 3.66,
      "grad_norm": 24.210830688476562,
      "learning_rate": 3.965625e-05,
      "loss": 0.7987,
      "step": 732
    },
    {
      "bce_eff_w": 0.24995,
      "bce_loss": -0.46435004472732544,
      "epoch": 3.66,
      "step": 732
    },
    {
      "epoch": 3.665,
      "grad_norm": 17.786943435668945,
      "learning_rate": 3.9625e-05,
      "loss": 1.5687,
      "step": 733
    },
    {
      "bce_eff_w": 0.2501,
      "bce_loss": -0.463255375623703,
      "epoch": 3.665,
      "step": 733
    },
    {
      "epoch": 3.67,
      "grad_norm": 18.845645904541016,
      "learning_rate": 3.9593750000000004e-05,
      "loss": 3.1534,
      "step": 734
    },
    {
      "bce_eff_w": 0.25025000000000003,
      "bce_loss": -0.4611167013645172,
      "epoch": 3.67,
      "step": 734
    },
    {
      "epoch": 3.675,
      "grad_norm": 31.303115844726562,
      "learning_rate": 3.95625e-05,
      "loss": 1.5005,
      "step": 735
    },
    {
      "bce_eff_w": 0.2504,
      "bce_loss": -0.4634634852409363,
      "epoch": 3.675,
      "step": 735
    },
    {
      "epoch": 3.68,
      "grad_norm": 34.162261962890625,
      "learning_rate": 3.953125e-05,
      "loss": 1.7607,
      "step": 736
    },
    {
      "bce_eff_w": 0.25055,
      "bce_loss": -0.46366721391677856,
      "epoch": 3.68,
      "step": 736
    },
    {
      "epoch": 3.685,
      "grad_norm": 30.624725341796875,
      "learning_rate": 3.9500000000000005e-05,
      "loss": 1.5019,
      "step": 737
    },
    {
      "bce_eff_w": 0.25070000000000003,
      "bce_loss": -0.4637104868888855,
      "epoch": 3.685,
      "step": 737
    },
    {
      "epoch": 3.69,
      "grad_norm": 25.04301643371582,
      "learning_rate": 3.946875000000001e-05,
      "loss": 1.072,
      "step": 738
    },
    {
      "bce_eff_w": 0.25085,
      "bce_loss": -0.46405792236328125,
      "epoch": 3.69,
      "step": 738
    },
    {
      "epoch": 3.695,
      "grad_norm": 27.096799850463867,
      "learning_rate": 3.9437499999999996e-05,
      "loss": 1.448,
      "step": 739
    },
    {
      "bce_eff_w": 0.251,
      "bce_loss": -0.45883581042289734,
      "epoch": 3.695,
      "step": 739
    },
    {
      "epoch": 3.7,
      "grad_norm": 13.75328540802002,
      "learning_rate": 3.940625e-05,
      "loss": 4.0164,
      "step": 740
    },
    {
      "bce_eff_w": 0.25115,
      "bce_loss": -0.4644300639629364,
      "epoch": 3.7,
      "step": 740
    },
    {
      "epoch": 3.705,
      "grad_norm": 21.270673751831055,
      "learning_rate": 3.9375e-05,
      "loss": 0.5291,
      "step": 741
    },
    {
      "bce_eff_w": 0.2513,
      "bce_loss": -0.46526390314102173,
      "epoch": 3.705,
      "step": 741
    },
    {
      "epoch": 3.71,
      "grad_norm": 39.11629104614258,
      "learning_rate": 3.9343750000000004e-05,
      "loss": 2.3989,
      "step": 742
    },
    {
      "bce_eff_w": 0.25145,
      "bce_loss": -0.46465373039245605,
      "epoch": 3.71,
      "step": 742
    },
    {
      "epoch": 3.715,
      "grad_norm": 25.49349594116211,
      "learning_rate": 3.93125e-05,
      "loss": 1.2648,
      "step": 743
    },
    {
      "bce_eff_w": 0.2516,
      "bce_loss": -0.4589938521385193,
      "epoch": 3.715,
      "step": 743
    },
    {
      "epoch": 3.7199999999999998,
      "grad_norm": 20.292566299438477,
      "learning_rate": 3.928125e-05,
      "loss": 0.6675,
      "step": 744
    },
    {
      "bce_eff_w": 0.25175000000000003,
      "bce_loss": -0.4637465476989746,
      "epoch": 3.7199999999999998,
      "step": 744
    },
    {
      "epoch": 3.725,
      "grad_norm": 20.532312393188477,
      "learning_rate": 3.9250000000000005e-05,
      "loss": 2.5676,
      "step": 745
    },
    {
      "bce_eff_w": 0.2519,
      "bce_loss": -0.46197712421417236,
      "epoch": 3.725,
      "step": 745
    },
    {
      "epoch": 3.73,
      "grad_norm": 27.451786041259766,
      "learning_rate": 3.921875e-05,
      "loss": 1.4547,
      "step": 746
    },
    {
      "bce_eff_w": 0.25205,
      "bce_loss": -0.45868101716041565,
      "epoch": 3.73,
      "step": 746
    },
    {
      "epoch": 3.735,
      "grad_norm": 21.421844482421875,
      "learning_rate": 3.91875e-05,
      "loss": 1.0779,
      "step": 747
    },
    {
      "bce_eff_w": 0.2522,
      "bce_loss": -0.4646749496459961,
      "epoch": 3.735,
      "step": 747
    },
    {
      "epoch": 3.74,
      "grad_norm": 28.150543212890625,
      "learning_rate": 3.915625e-05,
      "loss": 0.764,
      "step": 748
    },
    {
      "bce_eff_w": 0.25235,
      "bce_loss": -0.4642297029495239,
      "epoch": 3.74,
      "step": 748
    },
    {
      "epoch": 3.745,
      "grad_norm": 18.08463478088379,
      "learning_rate": 3.9125e-05,
      "loss": 1.7395,
      "step": 749
    },
    {
      "bce_eff_w": 0.2525,
      "bce_loss": -0.4620153605937958,
      "epoch": 3.745,
      "step": 749
    },
    {
      "epoch": 3.75,
      "grad_norm": 22.467222213745117,
      "learning_rate": 3.909375e-05,
      "loss": 1.7116,
      "step": 750
    },
    {
      "bce_eff_w": 0.25265,
      "bce_loss": -0.46369245648384094,
      "epoch": 3.75,
      "step": 750
    },
    {
      "epoch": 3.755,
      "grad_norm": 26.727935791015625,
      "learning_rate": 3.90625e-05,
      "loss": 2.5711,
      "step": 751
    },
    {
      "bce_eff_w": 0.2528,
      "bce_loss": -0.4634539783000946,
      "epoch": 3.755,
      "step": 751
    },
    {
      "epoch": 3.76,
      "grad_norm": 21.687746047973633,
      "learning_rate": 3.9031250000000003e-05,
      "loss": 2.3422,
      "step": 752
    },
    {
      "bce_eff_w": 0.25295,
      "bce_loss": -0.45853960514068604,
      "epoch": 3.76,
      "step": 752
    },
    {
      "epoch": 3.765,
      "grad_norm": 12.364150047302246,
      "learning_rate": 3.9000000000000006e-05,
      "loss": 0.6351,
      "step": 753
    },
    {
      "bce_eff_w": 0.2531,
      "bce_loss": -0.464138001203537,
      "epoch": 3.765,
      "step": 753
    },
    {
      "epoch": 3.77,
      "grad_norm": 18.569074630737305,
      "learning_rate": 3.896875e-05,
      "loss": 0.2489,
      "step": 754
    },
    {
      "bce_eff_w": 0.25325000000000003,
      "bce_loss": -0.4587177634239197,
      "epoch": 3.77,
      "step": 754
    },
    {
      "epoch": 3.775,
      "grad_norm": 18.87578010559082,
      "learning_rate": 3.8937500000000005e-05,
      "loss": 1.3821,
      "step": 755
    },
    {
      "bce_eff_w": 0.2534,
      "bce_loss": -0.4638887643814087,
      "epoch": 3.775,
      "step": 755
    },
    {
      "epoch": 3.7800000000000002,
      "grad_norm": 23.676633834838867,
      "learning_rate": 3.890625e-05,
      "loss": 1.0489,
      "step": 756
    },
    {
      "bce_eff_w": 0.25355,
      "bce_loss": -0.46366894245147705,
      "epoch": 3.7800000000000002,
      "step": 756
    },
    {
      "epoch": 3.785,
      "grad_norm": 23.517148971557617,
      "learning_rate": 3.8875e-05,
      "loss": 2.3784,
      "step": 757
    },
    {
      "bce_eff_w": 0.25370000000000004,
      "bce_loss": -0.4640553593635559,
      "epoch": 3.785,
      "step": 757
    },
    {
      "epoch": 3.79,
      "grad_norm": 18.790904998779297,
      "learning_rate": 3.884375e-05,
      "loss": 2.4831,
      "step": 758
    },
    {
      "bce_eff_w": 0.25385,
      "bce_loss": -0.46199527382850647,
      "epoch": 3.79,
      "step": 758
    },
    {
      "epoch": 3.795,
      "grad_norm": 29.166194915771484,
      "learning_rate": 3.88125e-05,
      "loss": 0.3788,
      "step": 759
    },
    {
      "bce_eff_w": 0.254,
      "bce_loss": -0.4567998945713043,
      "epoch": 3.795,
      "step": 759
    },
    {
      "epoch": 3.8,
      "grad_norm": 20.0102481842041,
      "learning_rate": 3.8781250000000004e-05,
      "loss": 1.6067,
      "step": 760
    },
    {
      "bce_eff_w": 0.25415,
      "bce_loss": -0.4627774953842163,
      "epoch": 3.8,
      "step": 760
    },
    {
      "epoch": 3.805,
      "grad_norm": 13.558172225952148,
      "learning_rate": 3.875e-05,
      "loss": 3.8992,
      "step": 761
    },
    {
      "bce_eff_w": 0.2543,
      "bce_loss": -0.46053147315979004,
      "epoch": 3.805,
      "step": 761
    },
    {
      "epoch": 3.81,
      "grad_norm": 16.78794288635254,
      "learning_rate": 3.871875e-05,
      "loss": 0.461,
      "step": 762
    },
    {
      "bce_eff_w": 0.25445,
      "bce_loss": -0.46413183212280273,
      "epoch": 3.81,
      "step": 762
    },
    {
      "epoch": 3.815,
      "grad_norm": 21.151933670043945,
      "learning_rate": 3.8687500000000005e-05,
      "loss": 0.574,
      "step": 763
    },
    {
      "bce_eff_w": 0.2546,
      "bce_loss": -0.46215468645095825,
      "epoch": 3.815,
      "step": 763
    },
    {
      "epoch": 3.82,
      "grad_norm": 54.245277404785156,
      "learning_rate": 3.865625e-05,
      "loss": 2.1755,
      "step": 764
    },
    {
      "bce_eff_w": 0.25475000000000003,
      "bce_loss": -0.4533556401729584,
      "epoch": 3.82,
      "step": 764
    },
    {
      "epoch": 3.825,
      "grad_norm": 19.597963333129883,
      "learning_rate": 3.8625e-05,
      "loss": 1.3426,
      "step": 765
    },
    {
      "bce_eff_w": 0.2549,
      "bce_loss": -0.458497017621994,
      "epoch": 3.825,
      "step": 765
    },
    {
      "epoch": 3.83,
      "grad_norm": 20.358108520507812,
      "learning_rate": 3.859375e-05,
      "loss": 0.8183,
      "step": 766
    },
    {
      "bce_eff_w": 0.25505,
      "bce_loss": -0.4646666944026947,
      "epoch": 3.83,
      "step": 766
    },
    {
      "epoch": 3.835,
      "grad_norm": 22.679710388183594,
      "learning_rate": 3.85625e-05,
      "loss": 2.1872,
      "step": 767
    },
    {
      "bce_eff_w": 0.2552,
      "bce_loss": -0.46315598487854004,
      "epoch": 3.835,
      "step": 767
    },
    {
      "epoch": 3.84,
      "grad_norm": 24.45191764831543,
      "learning_rate": 3.8531250000000005e-05,
      "loss": 1.4021,
      "step": 768
    },
    {
      "bce_eff_w": 0.25535,
      "bce_loss": -0.4637273848056793,
      "epoch": 3.84,
      "step": 768
    },
    {
      "epoch": 3.8449999999999998,
      "grad_norm": 20.438907623291016,
      "learning_rate": 3.85e-05,
      "loss": 2.6686,
      "step": 769
    },
    {
      "bce_eff_w": 0.2555,
      "bce_loss": -0.46352672576904297,
      "epoch": 3.8449999999999998,
      "step": 769
    },
    {
      "epoch": 3.85,
      "grad_norm": 24.176212310791016,
      "learning_rate": 3.846875e-05,
      "loss": 0.8483,
      "step": 770
    },
    {
      "bce_eff_w": 0.25565,
      "bce_loss": -0.4554443955421448,
      "epoch": 3.85,
      "step": 770
    },
    {
      "epoch": 3.855,
      "grad_norm": 20.72347640991211,
      "learning_rate": 3.8437500000000006e-05,
      "loss": 1.347,
      "step": 771
    },
    {
      "bce_eff_w": 0.2558,
      "bce_loss": -0.46448835730552673,
      "epoch": 3.855,
      "step": 771
    },
    {
      "epoch": 3.86,
      "grad_norm": 22.872339248657227,
      "learning_rate": 3.840625e-05,
      "loss": 1.1232,
      "step": 772
    },
    {
      "bce_eff_w": 0.25595,
      "bce_loss": -0.46027281880378723,
      "epoch": 3.86,
      "step": 772
    },
    {
      "epoch": 3.865,
      "grad_norm": 34.08718490600586,
      "learning_rate": 3.8375e-05,
      "loss": 0.4197,
      "step": 773
    },
    {
      "bce_eff_w": 0.2561,
      "bce_loss": -0.4585884213447571,
      "epoch": 3.865,
      "step": 773
    },
    {
      "epoch": 3.87,
      "grad_norm": 37.86335372924805,
      "learning_rate": 3.834375e-05,
      "loss": 3.4439,
      "step": 774
    },
    {
      "bce_eff_w": 0.25625,
      "bce_loss": -0.4633314311504364,
      "epoch": 3.87,
      "step": 774
    },
    {
      "epoch": 3.875,
      "grad_norm": 31.37503433227539,
      "learning_rate": 3.83125e-05,
      "loss": 1.3849,
      "step": 775
    },
    {
      "bce_eff_w": 0.2564,
      "bce_loss": -0.4634340703487396,
      "epoch": 3.875,
      "step": 775
    },
    {
      "epoch": 3.88,
      "grad_norm": 26.20261001586914,
      "learning_rate": 3.828125e-05,
      "loss": 1.5663,
      "step": 776
    },
    {
      "bce_eff_w": 0.25655,
      "bce_loss": -0.46116358041763306,
      "epoch": 3.88,
      "step": 776
    },
    {
      "epoch": 3.885,
      "grad_norm": 21.025028228759766,
      "learning_rate": 3.825e-05,
      "loss": 2.0972,
      "step": 777
    },
    {
      "bce_eff_w": 0.25670000000000004,
      "bce_loss": -0.4637201428413391,
      "epoch": 3.885,
      "step": 777
    },
    {
      "epoch": 3.89,
      "grad_norm": 19.27289581298828,
      "learning_rate": 3.8218750000000004e-05,
      "loss": 0.2209,
      "step": 778
    },
    {
      "bce_eff_w": 0.25685,
      "bce_loss": -0.4619078040122986,
      "epoch": 3.89,
      "step": 778
    },
    {
      "epoch": 3.895,
      "grad_norm": 24.481433868408203,
      "learning_rate": 3.818750000000001e-05,
      "loss": 1.0488,
      "step": 779
    },
    {
      "bce_eff_w": 0.257,
      "bce_loss": -0.46484997868537903,
      "epoch": 3.895,
      "step": 779
    },
    {
      "epoch": 3.9,
      "grad_norm": 18.030546188354492,
      "learning_rate": 3.815625e-05,
      "loss": 1.9976,
      "step": 780
    },
    {
      "bce_eff_w": 0.25715,
      "bce_loss": -0.46330684423446655,
      "epoch": 3.9,
      "step": 780
    },
    {
      "epoch": 3.9050000000000002,
      "grad_norm": 30.591934204101562,
      "learning_rate": 3.8125e-05,
      "loss": 0.6909,
      "step": 781
    },
    {
      "bce_eff_w": 0.25730000000000003,
      "bce_loss": -0.46198034286499023,
      "epoch": 3.9050000000000002,
      "step": 781
    },
    {
      "epoch": 3.91,
      "grad_norm": 31.249393463134766,
      "learning_rate": 3.809375e-05,
      "loss": 2.2003,
      "step": 782
    },
    {
      "bce_eff_w": 0.25745,
      "bce_loss": -0.46438127756118774,
      "epoch": 3.91,
      "step": 782
    },
    {
      "epoch": 3.915,
      "grad_norm": 15.415249824523926,
      "learning_rate": 3.8062500000000004e-05,
      "loss": 0.3912,
      "step": 783
    },
    {
      "bce_eff_w": 0.2576,
      "bce_loss": -0.4633459746837616,
      "epoch": 3.915,
      "step": 783
    },
    {
      "epoch": 3.92,
      "grad_norm": 20.266735076904297,
      "learning_rate": 3.803125e-05,
      "loss": 2.3308,
      "step": 784
    },
    {
      "bce_eff_w": 0.25775000000000003,
      "bce_loss": -0.46298661828041077,
      "epoch": 3.92,
      "step": 784
    },
    {
      "epoch": 3.925,
      "grad_norm": 23.44310760498047,
      "learning_rate": 3.8e-05,
      "loss": 0.8327,
      "step": 785
    },
    {
      "bce_eff_w": 0.2579,
      "bce_loss": -0.46458953619003296,
      "epoch": 3.925,
      "step": 785
    },
    {
      "epoch": 3.93,
      "grad_norm": 27.93260955810547,
      "learning_rate": 3.7968750000000005e-05,
      "loss": 1.6982,
      "step": 786
    },
    {
      "bce_eff_w": 0.25805,
      "bce_loss": -0.46004655957221985,
      "epoch": 3.93,
      "step": 786
    },
    {
      "epoch": 3.935,
      "grad_norm": 20.520410537719727,
      "learning_rate": 3.79375e-05,
      "loss": 1.4693,
      "step": 787
    },
    {
      "bce_eff_w": 0.2582,
      "bce_loss": -0.4644330143928528,
      "epoch": 3.935,
      "step": 787
    },
    {
      "epoch": 3.94,
      "grad_norm": 20.467884063720703,
      "learning_rate": 3.790625e-05,
      "loss": 1.7725,
      "step": 788
    },
    {
      "bce_eff_w": 0.25835,
      "bce_loss": -0.4640822112560272,
      "epoch": 3.94,
      "step": 788
    },
    {
      "epoch": 3.945,
      "grad_norm": 28.10074234008789,
      "learning_rate": 3.7875e-05,
      "loss": 2.8894,
      "step": 789
    },
    {
      "bce_eff_w": 0.2585,
      "bce_loss": -0.4601326286792755,
      "epoch": 3.945,
      "step": 789
    },
    {
      "epoch": 3.95,
      "grad_norm": 34.906494140625,
      "learning_rate": 3.784375e-05,
      "loss": 1.4381,
      "step": 790
    },
    {
      "bce_eff_w": 0.25865,
      "bce_loss": -0.46422284841537476,
      "epoch": 3.95,
      "step": 790
    },
    {
      "epoch": 3.955,
      "grad_norm": 20.325868606567383,
      "learning_rate": 3.78125e-05,
      "loss": 1.919,
      "step": 791
    },
    {
      "bce_eff_w": 0.25880000000000003,
      "bce_loss": -0.46032074093818665,
      "epoch": 3.955,
      "step": 791
    },
    {
      "epoch": 3.96,
      "grad_norm": 17.751026153564453,
      "learning_rate": 3.778125e-05,
      "loss": 0.959,
      "step": 792
    },
    {
      "bce_eff_w": 0.25895,
      "bce_loss": -0.462963342666626,
      "epoch": 3.96,
      "step": 792
    },
    {
      "epoch": 3.965,
      "grad_norm": 26.97235679626465,
      "learning_rate": 3.775e-05,
      "loss": 2.5259,
      "step": 793
    },
    {
      "bce_eff_w": 0.2591,
      "bce_loss": -0.4612095057964325,
      "epoch": 3.965,
      "step": 793
    },
    {
      "epoch": 3.9699999999999998,
      "grad_norm": 15.598529815673828,
      "learning_rate": 3.7718750000000005e-05,
      "loss": 3.9903,
      "step": 794
    },
    {
      "bce_eff_w": 0.25925,
      "bce_loss": -0.45858821272850037,
      "epoch": 3.9699999999999998,
      "step": 794
    },
    {
      "epoch": 3.975,
      "grad_norm": 20.110383987426758,
      "learning_rate": 3.76875e-05,
      "loss": 1.3646,
      "step": 795
    },
    {
      "bce_eff_w": 0.2594,
      "bce_loss": -0.46101999282836914,
      "epoch": 3.975,
      "step": 795
    },
    {
      "epoch": 3.98,
      "grad_norm": 24.364294052124023,
      "learning_rate": 3.7656250000000004e-05,
      "loss": 2.9706,
      "step": 796
    },
    {
      "bce_eff_w": 0.25955,
      "bce_loss": -0.4639281928539276,
      "epoch": 3.98,
      "step": 796
    },
    {
      "epoch": 3.985,
      "grad_norm": 21.823930740356445,
      "learning_rate": 3.7625e-05,
      "loss": 1.6554,
      "step": 797
    },
    {
      "bce_eff_w": 0.25970000000000004,
      "bce_loss": -0.45846518874168396,
      "epoch": 3.985,
      "step": 797
    },
    {
      "epoch": 3.99,
      "grad_norm": 28.831640243530273,
      "learning_rate": 3.759375e-05,
      "loss": 3.0566,
      "step": 798
    },
    {
      "bce_eff_w": 0.25985,
      "bce_loss": -0.4645341634750366,
      "epoch": 3.99,
      "step": 798
    },
    {
      "epoch": 3.995,
      "grad_norm": 31.18349266052246,
      "learning_rate": 3.75625e-05,
      "loss": 1.5083,
      "step": 799
    },
    {
      "bce_eff_w": 0.26,
      "bce_loss": -0.46351832151412964,
      "epoch": 3.995,
      "step": 799
    },
    {
      "epoch": 4.0,
      "grad_norm": 23.25067138671875,
      "learning_rate": 3.753125e-05,
      "loss": 3.4493,
      "step": 800
    },
    {
      "bce_eff_w": 0.26015,
      "bce_loss": -0.4636914134025574,
      "epoch": 4.0,
      "step": 800
    },
    {
      "epoch": 4.005,
      "grad_norm": 16.5283145904541,
      "learning_rate": 3.7500000000000003e-05,
      "loss": 0.3896,
      "step": 801
    },
    {
      "bce_eff_w": 0.26030000000000003,
      "bce_loss": -0.46001166105270386,
      "epoch": 4.005,
      "step": 801
    },
    {
      "epoch": 4.01,
      "grad_norm": 14.814713478088379,
      "learning_rate": 3.746875e-05,
      "loss": 0.7634,
      "step": 802
    },
    {
      "bce_eff_w": 0.26045,
      "bce_loss": -0.4597136080265045,
      "epoch": 4.01,
      "step": 802
    },
    {
      "epoch": 4.015,
      "grad_norm": 23.158275604248047,
      "learning_rate": 3.74375e-05,
      "loss": 0.7498,
      "step": 803
    },
    {
      "bce_eff_w": 0.2606,
      "bce_loss": -0.46390238404273987,
      "epoch": 4.015,
      "step": 803
    },
    {
      "epoch": 4.02,
      "grad_norm": 27.547243118286133,
      "learning_rate": 3.7406250000000005e-05,
      "loss": 1.5473,
      "step": 804
    },
    {
      "bce_eff_w": 0.26075000000000004,
      "bce_loss": -0.4595614969730377,
      "epoch": 4.02,
      "step": 804
    },
    {
      "epoch": 4.025,
      "grad_norm": 19.69466209411621,
      "learning_rate": 3.737500000000001e-05,
      "loss": 0.8997,
      "step": 805
    },
    {
      "bce_eff_w": 0.2609,
      "bce_loss": -0.4623912572860718,
      "epoch": 4.025,
      "step": 805
    },
    {
      "epoch": 4.03,
      "grad_norm": 18.921613693237305,
      "learning_rate": 3.7343749999999996e-05,
      "loss": 2.9359,
      "step": 806
    },
    {
      "bce_eff_w": 0.26105,
      "bce_loss": -0.46415290236473083,
      "epoch": 4.03,
      "step": 806
    },
    {
      "epoch": 4.035,
      "grad_norm": 18.169374465942383,
      "learning_rate": 3.73125e-05,
      "loss": 1.7361,
      "step": 807
    },
    {
      "bce_eff_w": 0.2612,
      "bce_loss": -0.46353134512901306,
      "epoch": 4.035,
      "step": 807
    },
    {
      "epoch": 4.04,
      "grad_norm": 32.823631286621094,
      "learning_rate": 3.728125e-05,
      "loss": 1.1888,
      "step": 808
    },
    {
      "bce_eff_w": 0.26135,
      "bce_loss": -0.4615771472454071,
      "epoch": 4.04,
      "step": 808
    },
    {
      "epoch": 4.045,
      "grad_norm": 18.729646682739258,
      "learning_rate": 3.7250000000000004e-05,
      "loss": 1.7148,
      "step": 809
    },
    {
      "bce_eff_w": 0.2615,
      "bce_loss": -0.4646094739437103,
      "epoch": 4.045,
      "step": 809
    },
    {
      "epoch": 4.05,
      "grad_norm": 36.387611389160156,
      "learning_rate": 3.721875e-05,
      "loss": 1.4815,
      "step": 810
    },
    {
      "bce_eff_w": 0.26165,
      "bce_loss": -0.46283796429634094,
      "epoch": 4.05,
      "step": 810
    },
    {
      "epoch": 4.055,
      "grad_norm": 22.781620025634766,
      "learning_rate": 3.71875e-05,
      "loss": 2.1525,
      "step": 811
    },
    {
      "bce_eff_w": 0.26180000000000003,
      "bce_loss": -0.4642857611179352,
      "epoch": 4.055,
      "step": 811
    },
    {
      "epoch": 4.06,
      "grad_norm": 22.812503814697266,
      "learning_rate": 3.7156250000000005e-05,
      "loss": 2.1272,
      "step": 812
    },
    {
      "bce_eff_w": 0.26195,
      "bce_loss": -0.46166402101516724,
      "epoch": 4.06,
      "step": 812
    },
    {
      "epoch": 4.065,
      "grad_norm": 23.320735931396484,
      "learning_rate": 3.7125e-05,
      "loss": 1.0091,
      "step": 813
    },
    {
      "bce_eff_w": 0.2621,
      "bce_loss": -0.4609759449958801,
      "epoch": 4.065,
      "step": 813
    },
    {
      "epoch": 4.07,
      "grad_norm": 32.88042068481445,
      "learning_rate": 3.709375e-05,
      "loss": 2.5972,
      "step": 814
    },
    {
      "bce_eff_w": 0.26225,
      "bce_loss": -0.46383586525917053,
      "epoch": 4.07,
      "step": 814
    },
    {
      "epoch": 4.075,
      "grad_norm": 25.70917320251465,
      "learning_rate": 3.70625e-05,
      "loss": 1.1742,
      "step": 815
    },
    {
      "bce_eff_w": 0.2624,
      "bce_loss": -0.4626835882663727,
      "epoch": 4.075,
      "step": 815
    },
    {
      "epoch": 4.08,
      "grad_norm": 25.64774513244629,
      "learning_rate": 3.703125e-05,
      "loss": 1.1104,
      "step": 816
    },
    {
      "bce_eff_w": 0.26255,
      "bce_loss": -0.46419110894203186,
      "epoch": 4.08,
      "step": 816
    },
    {
      "epoch": 4.085,
      "grad_norm": 21.631946563720703,
      "learning_rate": 3.7e-05,
      "loss": 0.8455,
      "step": 817
    },
    {
      "bce_eff_w": 0.2627,
      "bce_loss": -0.46346670389175415,
      "epoch": 4.085,
      "step": 817
    },
    {
      "epoch": 4.09,
      "grad_norm": 21.33476448059082,
      "learning_rate": 3.696875e-05,
      "loss": 1.1714,
      "step": 818
    },
    {
      "bce_eff_w": 0.26285000000000003,
      "bce_loss": -0.46208736300468445,
      "epoch": 4.09,
      "step": 818
    },
    {
      "epoch": 4.095,
      "grad_norm": 32.16432189941406,
      "learning_rate": 3.69375e-05,
      "loss": 0.9554,
      "step": 819
    },
    {
      "bce_eff_w": 0.263,
      "bce_loss": -0.46417805552482605,
      "epoch": 4.095,
      "step": 819
    },
    {
      "epoch": 4.1,
      "grad_norm": 21.189435958862305,
      "learning_rate": 3.6906250000000006e-05,
      "loss": 0.4955,
      "step": 820
    },
    {
      "bce_eff_w": 0.26315,
      "bce_loss": -0.4641692340373993,
      "epoch": 4.1,
      "step": 820
    },
    {
      "epoch": 4.105,
      "grad_norm": 24.21077537536621,
      "learning_rate": 3.6875e-05,
      "loss": 1.4608,
      "step": 821
    },
    {
      "bce_eff_w": 0.2633,
      "bce_loss": -0.4631254971027374,
      "epoch": 4.105,
      "step": 821
    },
    {
      "epoch": 4.11,
      "grad_norm": 22.984521865844727,
      "learning_rate": 3.684375e-05,
      "loss": 2.0644,
      "step": 822
    },
    {
      "bce_eff_w": 0.26345,
      "bce_loss": -0.4632888734340668,
      "epoch": 4.11,
      "step": 822
    },
    {
      "epoch": 4.115,
      "grad_norm": 24.2781925201416,
      "learning_rate": 3.68125e-05,
      "loss": 2.3118,
      "step": 823
    },
    {
      "bce_eff_w": 0.2636,
      "bce_loss": -0.46391257643699646,
      "epoch": 4.115,
      "step": 823
    },
    {
      "epoch": 4.12,
      "grad_norm": 22.439023971557617,
      "learning_rate": 3.678125e-05,
      "loss": 1.8385,
      "step": 824
    },
    {
      "bce_eff_w": 0.26375000000000004,
      "bce_loss": -0.4637538194656372,
      "epoch": 4.12,
      "step": 824
    },
    {
      "epoch": 4.125,
      "grad_norm": 22.030031204223633,
      "learning_rate": 3.675e-05,
      "loss": 1.1241,
      "step": 825
    },
    {
      "bce_eff_w": 0.2639,
      "bce_loss": -0.46467480063438416,
      "epoch": 4.125,
      "step": 825
    },
    {
      "epoch": 4.13,
      "grad_norm": 24.47505760192871,
      "learning_rate": 3.671875e-05,
      "loss": 1.5369,
      "step": 826
    },
    {
      "bce_eff_w": 0.26405,
      "bce_loss": -0.46448853611946106,
      "epoch": 4.13,
      "step": 826
    },
    {
      "epoch": 4.135,
      "grad_norm": 27.340900421142578,
      "learning_rate": 3.6687500000000004e-05,
      "loss": 2.9708,
      "step": 827
    },
    {
      "bce_eff_w": 0.2642,
      "bce_loss": -0.4599163234233856,
      "epoch": 4.135,
      "step": 827
    },
    {
      "epoch": 4.14,
      "grad_norm": 20.158937454223633,
      "learning_rate": 3.665625e-05,
      "loss": 0.9253,
      "step": 828
    },
    {
      "bce_eff_w": 0.26435,
      "bce_loss": -0.46010276675224304,
      "epoch": 4.14,
      "step": 828
    },
    {
      "epoch": 4.145,
      "grad_norm": 18.02052116394043,
      "learning_rate": 3.6625e-05,
      "loss": 0.8733,
      "step": 829
    },
    {
      "bce_eff_w": 0.2645,
      "bce_loss": -0.46447592973709106,
      "epoch": 4.145,
      "step": 829
    },
    {
      "epoch": 4.15,
      "grad_norm": 22.74982452392578,
      "learning_rate": 3.6593750000000005e-05,
      "loss": 1.6286,
      "step": 830
    },
    {
      "bce_eff_w": 0.26465,
      "bce_loss": -0.4588658809661865,
      "epoch": 4.15,
      "step": 830
    },
    {
      "epoch": 4.155,
      "grad_norm": 15.61699104309082,
      "learning_rate": 3.65625e-05,
      "loss": 0.8925,
      "step": 831
    },
    {
      "bce_eff_w": 0.26480000000000004,
      "bce_loss": -0.4580291509628296,
      "epoch": 4.155,
      "step": 831
    },
    {
      "epoch": 4.16,
      "grad_norm": 24.324447631835938,
      "learning_rate": 3.653125e-05,
      "loss": 2.0594,
      "step": 832
    },
    {
      "bce_eff_w": 0.26495,
      "bce_loss": -0.4645940065383911,
      "epoch": 4.16,
      "step": 832
    },
    {
      "epoch": 4.165,
      "grad_norm": 33.92457962036133,
      "learning_rate": 3.65e-05,
      "loss": 1.2256,
      "step": 833
    },
    {
      "bce_eff_w": 0.2651,
      "bce_loss": -0.46174144744873047,
      "epoch": 4.165,
      "step": 833
    },
    {
      "epoch": 4.17,
      "grad_norm": 19.377025604248047,
      "learning_rate": 3.646875e-05,
      "loss": 1.9553,
      "step": 834
    },
    {
      "bce_eff_w": 0.26525,
      "bce_loss": -0.463765025138855,
      "epoch": 4.17,
      "step": 834
    },
    {
      "epoch": 4.175,
      "grad_norm": 7.24439001083374,
      "learning_rate": 3.6437500000000005e-05,
      "loss": -0.0348,
      "step": 835
    },
    {
      "bce_eff_w": 0.2654,
      "bce_loss": -0.46446698904037476,
      "epoch": 4.175,
      "step": 835
    },
    {
      "epoch": 4.18,
      "grad_norm": 25.526735305786133,
      "learning_rate": 3.640625e-05,
      "loss": 1.5504,
      "step": 836
    },
    {
      "bce_eff_w": 0.26555,
      "bce_loss": -0.4620644152164459,
      "epoch": 4.18,
      "step": 836
    },
    {
      "epoch": 4.185,
      "grad_norm": 20.197471618652344,
      "learning_rate": 3.6375e-05,
      "loss": 0.7737,
      "step": 837
    },
    {
      "bce_eff_w": 0.2657,
      "bce_loss": -0.46409422159194946,
      "epoch": 4.185,
      "step": 837
    },
    {
      "epoch": 4.19,
      "grad_norm": 25.98113250732422,
      "learning_rate": 3.6343750000000006e-05,
      "loss": 1.8844,
      "step": 838
    },
    {
      "bce_eff_w": 0.26585000000000003,
      "bce_loss": -0.4631496071815491,
      "epoch": 4.19,
      "step": 838
    },
    {
      "epoch": 4.195,
      "grad_norm": 24.927316665649414,
      "learning_rate": 3.63125e-05,
      "loss": 0.4957,
      "step": 839
    },
    {
      "bce_eff_w": 0.266,
      "bce_loss": -0.46429523825645447,
      "epoch": 4.195,
      "step": 839
    },
    {
      "epoch": 4.2,
      "grad_norm": 25.04204559326172,
      "learning_rate": 3.628125e-05,
      "loss": 1.3839,
      "step": 840
    },
    {
      "bce_eff_w": 0.26615,
      "bce_loss": -0.4630568325519562,
      "epoch": 4.2,
      "step": 840
    },
    {
      "epoch": 4.205,
      "grad_norm": 25.377063751220703,
      "learning_rate": 3.625e-05,
      "loss": 0.3881,
      "step": 841
    },
    {
      "bce_eff_w": 0.2663,
      "bce_loss": -0.46349602937698364,
      "epoch": 4.205,
      "step": 841
    },
    {
      "epoch": 4.21,
      "grad_norm": 14.470449447631836,
      "learning_rate": 3.621875e-05,
      "loss": 0.2171,
      "step": 842
    },
    {
      "bce_eff_w": 0.26645,
      "bce_loss": -0.4644605815410614,
      "epoch": 4.21,
      "step": 842
    },
    {
      "epoch": 4.215,
      "grad_norm": 18.013397216796875,
      "learning_rate": 3.61875e-05,
      "loss": 0.4403,
      "step": 843
    },
    {
      "bce_eff_w": 0.2666,
      "bce_loss": -0.46371281147003174,
      "epoch": 4.215,
      "step": 843
    },
    {
      "epoch": 4.22,
      "grad_norm": 20.26593589782715,
      "learning_rate": 3.615625e-05,
      "loss": 0.3364,
      "step": 844
    },
    {
      "bce_eff_w": 0.26675000000000004,
      "bce_loss": -0.4625897705554962,
      "epoch": 4.22,
      "step": 844
    },
    {
      "epoch": 4.225,
      "grad_norm": 15.935894966125488,
      "learning_rate": 3.6125000000000004e-05,
      "loss": 2.4667,
      "step": 845
    },
    {
      "bce_eff_w": 0.2669,
      "bce_loss": -0.462414026260376,
      "epoch": 4.225,
      "step": 845
    },
    {
      "epoch": 4.23,
      "grad_norm": 20.969820022583008,
      "learning_rate": 3.6093750000000007e-05,
      "loss": 1.8364,
      "step": 846
    },
    {
      "bce_eff_w": 0.26705,
      "bce_loss": -0.4630500078201294,
      "epoch": 4.23,
      "step": 846
    },
    {
      "epoch": 4.235,
      "grad_norm": 19.976036071777344,
      "learning_rate": 3.60625e-05,
      "loss": 2.9207,
      "step": 847
    },
    {
      "bce_eff_w": 0.2672,
      "bce_loss": -0.4567052721977234,
      "epoch": 4.235,
      "step": 847
    },
    {
      "epoch": 4.24,
      "grad_norm": 16.854978561401367,
      "learning_rate": 3.603125e-05,
      "loss": 0.572,
      "step": 848
    },
    {
      "bce_eff_w": 0.26735,
      "bce_loss": -0.4579761028289795,
      "epoch": 4.24,
      "step": 848
    },
    {
      "epoch": 4.245,
      "grad_norm": 26.266517639160156,
      "learning_rate": 3.6e-05,
      "loss": 0.7841,
      "step": 849
    },
    {
      "bce_eff_w": 0.2675,
      "bce_loss": -0.4614284634590149,
      "epoch": 4.245,
      "step": 849
    },
    {
      "epoch": 4.25,
      "grad_norm": 13.493300437927246,
      "learning_rate": 3.5968750000000004e-05,
      "loss": 3.4506,
      "step": 850
    },
    {
      "bce_eff_w": 0.26765,
      "bce_loss": -0.459111750125885,
      "epoch": 4.25,
      "step": 850
    },
    {
      "epoch": 4.255,
      "grad_norm": 12.531318664550781,
      "learning_rate": 3.59375e-05,
      "loss": 0.4116,
      "step": 851
    },
    {
      "bce_eff_w": 0.26780000000000004,
      "bce_loss": -0.46353065967559814,
      "epoch": 4.255,
      "step": 851
    },
    {
      "epoch": 4.26,
      "grad_norm": 28.878376007080078,
      "learning_rate": 3.590625e-05,
      "loss": 1.1458,
      "step": 852
    },
    {
      "bce_eff_w": 0.26795,
      "bce_loss": -0.4572853446006775,
      "epoch": 4.26,
      "step": 852
    },
    {
      "epoch": 4.265,
      "grad_norm": 15.369438171386719,
      "learning_rate": 3.5875000000000005e-05,
      "loss": 0.5847,
      "step": 853
    },
    {
      "bce_eff_w": 0.2681,
      "bce_loss": -0.4638243019580841,
      "epoch": 4.265,
      "step": 853
    },
    {
      "epoch": 4.27,
      "grad_norm": 16.45306968688965,
      "learning_rate": 3.584375e-05,
      "loss": 2.5221,
      "step": 854
    },
    {
      "bce_eff_w": 0.26825,
      "bce_loss": -0.4607773423194885,
      "epoch": 4.27,
      "step": 854
    },
    {
      "epoch": 4.275,
      "grad_norm": 36.444095611572266,
      "learning_rate": 3.58125e-05,
      "loss": 0.6741,
      "step": 855
    },
    {
      "bce_eff_w": 0.2684,
      "bce_loss": -0.4628354609012604,
      "epoch": 4.275,
      "step": 855
    },
    {
      "epoch": 4.28,
      "grad_norm": 44.83688735961914,
      "learning_rate": 3.578125e-05,
      "loss": 2.2483,
      "step": 856
    },
    {
      "bce_eff_w": 0.26855,
      "bce_loss": -0.4612388610839844,
      "epoch": 4.28,
      "step": 856
    },
    {
      "epoch": 4.285,
      "grad_norm": 32.40009307861328,
      "learning_rate": 3.575e-05,
      "loss": 1.7253,
      "step": 857
    },
    {
      "bce_eff_w": 0.2687,
      "bce_loss": -0.4638226628303528,
      "epoch": 4.285,
      "step": 857
    },
    {
      "epoch": 4.29,
      "grad_norm": 26.7235107421875,
      "learning_rate": 3.571875e-05,
      "loss": 0.7518,
      "step": 858
    },
    {
      "bce_eff_w": 0.26885000000000003,
      "bce_loss": -0.4594308137893677,
      "epoch": 4.29,
      "step": 858
    },
    {
      "epoch": 4.295,
      "grad_norm": 13.127110481262207,
      "learning_rate": 3.56875e-05,
      "loss": 3.7297,
      "step": 859
    },
    {
      "bce_eff_w": 0.269,
      "bce_loss": -0.4636494815349579,
      "epoch": 4.295,
      "step": 859
    },
    {
      "epoch": 4.3,
      "grad_norm": 20.188913345336914,
      "learning_rate": 3.565625e-05,
      "loss": 2.2795,
      "step": 860
    },
    {
      "bce_eff_w": 0.26915,
      "bce_loss": -0.46237510442733765,
      "epoch": 4.3,
      "step": 860
    },
    {
      "epoch": 4.305,
      "grad_norm": 17.30654525756836,
      "learning_rate": 3.5625000000000005e-05,
      "loss": 0.9762,
      "step": 861
    },
    {
      "bce_eff_w": 0.2693,
      "bce_loss": -0.46319884061813354,
      "epoch": 4.305,
      "step": 861
    },
    {
      "epoch": 4.31,
      "grad_norm": 17.97825050354004,
      "learning_rate": 3.559375e-05,
      "loss": 1.4603,
      "step": 862
    },
    {
      "bce_eff_w": 0.26945,
      "bce_loss": -0.46447229385375977,
      "epoch": 4.31,
      "step": 862
    },
    {
      "epoch": 4.315,
      "grad_norm": 36.889671325683594,
      "learning_rate": 3.5562500000000004e-05,
      "loss": 1.6779,
      "step": 863
    },
    {
      "bce_eff_w": 0.2696,
      "bce_loss": -0.45841357111930847,
      "epoch": 4.315,
      "step": 863
    },
    {
      "epoch": 4.32,
      "grad_norm": 16.1090087890625,
      "learning_rate": 3.553125e-05,
      "loss": 1.1319,
      "step": 864
    },
    {
      "bce_eff_w": 0.26975000000000005,
      "bce_loss": -0.46056067943573,
      "epoch": 4.32,
      "step": 864
    },
    {
      "epoch": 4.325,
      "grad_norm": 19.239885330200195,
      "learning_rate": 3.55e-05,
      "loss": 1.7646,
      "step": 865
    },
    {
      "bce_eff_w": 0.26990000000000003,
      "bce_loss": -0.46385103464126587,
      "epoch": 4.325,
      "step": 865
    },
    {
      "epoch": 4.33,
      "grad_norm": 20.760406494140625,
      "learning_rate": 3.546875e-05,
      "loss": 1.2875,
      "step": 866
    },
    {
      "bce_eff_w": 0.27005,
      "bce_loss": -0.46350783109664917,
      "epoch": 4.33,
      "step": 866
    },
    {
      "epoch": 4.335,
      "grad_norm": 27.392555236816406,
      "learning_rate": 3.54375e-05,
      "loss": 0.7694,
      "step": 867
    },
    {
      "bce_eff_w": 0.2702,
      "bce_loss": -0.4626428484916687,
      "epoch": 4.335,
      "step": 867
    },
    {
      "epoch": 4.34,
      "grad_norm": 30.02642822265625,
      "learning_rate": 3.5406250000000003e-05,
      "loss": 1.3901,
      "step": 868
    },
    {
      "bce_eff_w": 0.27035,
      "bce_loss": -0.46299979090690613,
      "epoch": 4.34,
      "step": 868
    },
    {
      "epoch": 4.345,
      "grad_norm": 27.623924255371094,
      "learning_rate": 3.5375e-05,
      "loss": 1.3679,
      "step": 869
    },
    {
      "bce_eff_w": 0.2705,
      "bce_loss": -0.45847564935684204,
      "epoch": 4.345,
      "step": 869
    },
    {
      "epoch": 4.35,
      "grad_norm": 27.217958450317383,
      "learning_rate": 3.534375e-05,
      "loss": 0.1299,
      "step": 870
    },
    {
      "bce_eff_w": 0.27065,
      "bce_loss": -0.4633524715900421,
      "epoch": 4.35,
      "step": 870
    },
    {
      "epoch": 4.355,
      "grad_norm": 21.475322723388672,
      "learning_rate": 3.5312500000000005e-05,
      "loss": 0.4984,
      "step": 871
    },
    {
      "bce_eff_w": 0.2708,
      "bce_loss": -0.46357542276382446,
      "epoch": 4.355,
      "step": 871
    },
    {
      "epoch": 4.36,
      "grad_norm": 25.11994743347168,
      "learning_rate": 3.528125e-05,
      "loss": 2.4751,
      "step": 872
    },
    {
      "bce_eff_w": 0.27095,
      "bce_loss": -0.4609120488166809,
      "epoch": 4.36,
      "step": 872
    },
    {
      "epoch": 4.365,
      "grad_norm": 20.118633270263672,
      "learning_rate": 3.525e-05,
      "loss": 0.7728,
      "step": 873
    },
    {
      "bce_eff_w": 0.2711,
      "bce_loss": -0.46363645792007446,
      "epoch": 4.365,
      "step": 873
    },
    {
      "epoch": 4.37,
      "grad_norm": 19.53400421142578,
      "learning_rate": 3.521875e-05,
      "loss": 0.7963,
      "step": 874
    },
    {
      "bce_eff_w": 0.27125,
      "bce_loss": -0.4621540307998657,
      "epoch": 4.37,
      "step": 874
    },
    {
      "epoch": 4.375,
      "grad_norm": 10.19546127319336,
      "learning_rate": 3.51875e-05,
      "loss": 0.2728,
      "step": 875
    },
    {
      "bce_eff_w": 0.2714,
      "bce_loss": -0.4630052447319031,
      "epoch": 4.375,
      "step": 875
    },
    {
      "epoch": 4.38,
      "grad_norm": 24.28887367248535,
      "learning_rate": 3.5156250000000004e-05,
      "loss": 1.7626,
      "step": 876
    },
    {
      "bce_eff_w": 0.27155,
      "bce_loss": -0.45997950434684753,
      "epoch": 4.38,
      "step": 876
    },
    {
      "epoch": 4.385,
      "grad_norm": 19.17862319946289,
      "learning_rate": 3.5125e-05,
      "loss": 3.2935,
      "step": 877
    },
    {
      "bce_eff_w": 0.2717,
      "bce_loss": -0.4618368446826935,
      "epoch": 4.385,
      "step": 877
    },
    {
      "epoch": 4.39,
      "grad_norm": 30.146228790283203,
      "learning_rate": 3.509375e-05,
      "loss": 1.5843,
      "step": 878
    },
    {
      "bce_eff_w": 0.27185000000000004,
      "bce_loss": -0.4624636471271515,
      "epoch": 4.39,
      "step": 878
    },
    {
      "epoch": 4.395,
      "grad_norm": 13.997047424316406,
      "learning_rate": 3.5062500000000005e-05,
      "loss": 0.1569,
      "step": 879
    },
    {
      "bce_eff_w": 0.272,
      "bce_loss": -0.4644263982772827,
      "epoch": 4.395,
      "step": 879
    },
    {
      "epoch": 4.4,
      "grad_norm": 22.466463088989258,
      "learning_rate": 3.503125e-05,
      "loss": 0.5005,
      "step": 880
    },
    {
      "bce_eff_w": 0.27215,
      "bce_loss": -0.46309441328048706,
      "epoch": 4.4,
      "step": 880
    },
    {
      "epoch": 4.405,
      "grad_norm": 36.88018798828125,
      "learning_rate": 3.5e-05,
      "loss": 2.9812,
      "step": 881
    },
    {
      "bce_eff_w": 0.2723,
      "bce_loss": -0.4632941782474518,
      "epoch": 4.405,
      "step": 881
    },
    {
      "epoch": 4.41,
      "grad_norm": 22.58434295654297,
      "learning_rate": 3.496875e-05,
      "loss": 0.3039,
      "step": 882
    },
    {
      "bce_eff_w": 0.27245,
      "bce_loss": -0.4632841646671295,
      "epoch": 4.41,
      "step": 882
    },
    {
      "epoch": 4.415,
      "grad_norm": 18.119304656982422,
      "learning_rate": 3.49375e-05,
      "loss": 0.4233,
      "step": 883
    },
    {
      "bce_eff_w": 0.2726,
      "bce_loss": -0.46069881319999695,
      "epoch": 4.415,
      "step": 883
    },
    {
      "epoch": 4.42,
      "grad_norm": 18.988794326782227,
      "learning_rate": 3.4906250000000005e-05,
      "loss": 1.2276,
      "step": 884
    },
    {
      "bce_eff_w": 0.27275,
      "bce_loss": -0.46434286236763,
      "epoch": 4.42,
      "step": 884
    },
    {
      "epoch": 4.425,
      "grad_norm": 17.524877548217773,
      "learning_rate": 3.4875e-05,
      "loss": 1.9949,
      "step": 885
    },
    {
      "bce_eff_w": 0.27290000000000003,
      "bce_loss": -0.45919156074523926,
      "epoch": 4.425,
      "step": 885
    },
    {
      "epoch": 4.43,
      "grad_norm": 33.98344039916992,
      "learning_rate": 3.484375e-05,
      "loss": 1.0123,
      "step": 886
    },
    {
      "bce_eff_w": 0.27305,
      "bce_loss": -0.4626501500606537,
      "epoch": 4.43,
      "step": 886
    },
    {
      "epoch": 4.435,
      "grad_norm": 28.408483505249023,
      "learning_rate": 3.4812500000000006e-05,
      "loss": 2.5804,
      "step": 887
    },
    {
      "bce_eff_w": 0.2732,
      "bce_loss": -0.4637030065059662,
      "epoch": 4.435,
      "step": 887
    },
    {
      "epoch": 4.44,
      "grad_norm": 17.3770751953125,
      "learning_rate": 3.478125e-05,
      "loss": 0.2356,
      "step": 888
    },
    {
      "bce_eff_w": 0.27335,
      "bce_loss": -0.4583788216114044,
      "epoch": 4.44,
      "step": 888
    },
    {
      "epoch": 4.445,
      "grad_norm": 17.383790969848633,
      "learning_rate": 3.475e-05,
      "loss": 1.6124,
      "step": 889
    },
    {
      "bce_eff_w": 0.2735,
      "bce_loss": -0.45591631531715393,
      "epoch": 4.445,
      "step": 889
    },
    {
      "epoch": 4.45,
      "grad_norm": 17.14681625366211,
      "learning_rate": 3.471875e-05,
      "loss": 0.5352,
      "step": 890
    },
    {
      "bce_eff_w": 0.27365,
      "bce_loss": -0.4599117636680603,
      "epoch": 4.45,
      "step": 890
    },
    {
      "epoch": 4.455,
      "grad_norm": 24.833024978637695,
      "learning_rate": 3.46875e-05,
      "loss": 0.8937,
      "step": 891
    },
    {
      "bce_eff_w": 0.2738,
      "bce_loss": -0.46260765194892883,
      "epoch": 4.455,
      "step": 891
    },
    {
      "epoch": 4.46,
      "grad_norm": 28.784395217895508,
      "learning_rate": 3.465625e-05,
      "loss": 1.2229,
      "step": 892
    },
    {
      "bce_eff_w": 0.27395,
      "bce_loss": -0.4558141827583313,
      "epoch": 4.46,
      "step": 892
    },
    {
      "epoch": 4.465,
      "grad_norm": 19.65683937072754,
      "learning_rate": 3.4625e-05,
      "loss": 1.1112,
      "step": 893
    },
    {
      "bce_eff_w": 0.2741,
      "bce_loss": -0.4590490460395813,
      "epoch": 4.465,
      "step": 893
    },
    {
      "epoch": 4.47,
      "grad_norm": 31.021913528442383,
      "learning_rate": 3.4593750000000004e-05,
      "loss": 2.2537,
      "step": 894
    },
    {
      "bce_eff_w": 0.27425,
      "bce_loss": -0.4602305591106415,
      "epoch": 4.47,
      "step": 894
    },
    {
      "epoch": 4.475,
      "grad_norm": 14.698123931884766,
      "learning_rate": 3.45625e-05,
      "loss": 0.3808,
      "step": 895
    },
    {
      "bce_eff_w": 0.2744,
      "bce_loss": -0.4625203013420105,
      "epoch": 4.475,
      "step": 895
    },
    {
      "epoch": 4.48,
      "grad_norm": 37.84175109863281,
      "learning_rate": 3.453125e-05,
      "loss": 0.8983,
      "step": 896
    },
    {
      "bce_eff_w": 0.27455,
      "bce_loss": -0.4640291929244995,
      "epoch": 4.48,
      "step": 896
    },
    {
      "epoch": 4.485,
      "grad_norm": 43.87821578979492,
      "learning_rate": 3.45e-05,
      "loss": 1.2558,
      "step": 897
    },
    {
      "bce_eff_w": 0.2747,
      "bce_loss": -0.4610326290130615,
      "epoch": 4.485,
      "step": 897
    },
    {
      "epoch": 4.49,
      "grad_norm": 20.722078323364258,
      "learning_rate": 3.446875e-05,
      "loss": 1.4427,
      "step": 898
    },
    {
      "bce_eff_w": 0.27485000000000004,
      "bce_loss": -0.46154356002807617,
      "epoch": 4.49,
      "step": 898
    },
    {
      "epoch": 4.495,
      "grad_norm": 18.993906021118164,
      "learning_rate": 3.4437500000000004e-05,
      "loss": 3.692,
      "step": 899
    },
    {
      "bce_eff_w": 0.275,
      "bce_loss": -0.46348702907562256,
      "epoch": 4.495,
      "step": 899
    },
    {
      "epoch": 4.5,
      "grad_norm": 20.345001220703125,
      "learning_rate": 3.440625e-05,
      "loss": 0.3186,
      "step": 900
    },
    {
      "bce_eff_w": 0.27515,
      "bce_loss": -0.45821768045425415,
      "epoch": 4.5,
      "step": 900
    },
    {
      "epoch": 4.505,
      "grad_norm": 27.364383697509766,
      "learning_rate": 3.4375e-05,
      "loss": 0.5465,
      "step": 901
    },
    {
      "bce_eff_w": 0.2753,
      "bce_loss": -0.46522924304008484,
      "epoch": 4.505,
      "step": 901
    },
    {
      "epoch": 4.51,
      "grad_norm": 17.67534637451172,
      "learning_rate": 3.4343750000000005e-05,
      "loss": 2.8724,
      "step": 902
    },
    {
      "bce_eff_w": 0.27545000000000003,
      "bce_loss": -0.4638429582118988,
      "epoch": 4.51,
      "step": 902
    },
    {
      "epoch": 4.515,
      "grad_norm": 25.87007713317871,
      "learning_rate": 3.43125e-05,
      "loss": 0.4263,
      "step": 903
    },
    {
      "bce_eff_w": 0.2756,
      "bce_loss": -0.46328750252723694,
      "epoch": 4.515,
      "step": 903
    },
    {
      "epoch": 4.52,
      "grad_norm": 20.247045516967773,
      "learning_rate": 3.428125e-05,
      "loss": 0.5302,
      "step": 904
    },
    {
      "bce_eff_w": 0.27575,
      "bce_loss": -0.46166694164276123,
      "epoch": 4.52,
      "step": 904
    },
    {
      "epoch": 4.525,
      "grad_norm": 21.94108772277832,
      "learning_rate": 3.4250000000000006e-05,
      "loss": 0.8407,
      "step": 905
    },
    {
      "bce_eff_w": 0.27590000000000003,
      "bce_loss": -0.46175166964530945,
      "epoch": 4.525,
      "step": 905
    },
    {
      "epoch": 4.53,
      "grad_norm": 21.37444305419922,
      "learning_rate": 3.421875e-05,
      "loss": 0.7451,
      "step": 906
    },
    {
      "bce_eff_w": 0.27605,
      "bce_loss": -0.4618404507637024,
      "epoch": 4.53,
      "step": 906
    },
    {
      "epoch": 4.535,
      "grad_norm": 16.978097915649414,
      "learning_rate": 3.41875e-05,
      "loss": 3.2036,
      "step": 907
    },
    {
      "bce_eff_w": 0.2762,
      "bce_loss": -0.46169179677963257,
      "epoch": 4.535,
      "step": 907
    },
    {
      "epoch": 4.54,
      "grad_norm": 11.750277519226074,
      "learning_rate": 3.415625e-05,
      "loss": 0.0888,
      "step": 908
    },
    {
      "bce_eff_w": 0.27635,
      "bce_loss": -0.46500563621520996,
      "epoch": 4.54,
      "step": 908
    },
    {
      "epoch": 4.545,
      "grad_norm": 25.32876205444336,
      "learning_rate": 3.4125e-05,
      "loss": 0.8838,
      "step": 909
    },
    {
      "bce_eff_w": 0.2765,
      "bce_loss": -0.4627766013145447,
      "epoch": 4.545,
      "step": 909
    },
    {
      "epoch": 4.55,
      "grad_norm": 32.71589279174805,
      "learning_rate": 3.4093750000000005e-05,
      "loss": 1.5252,
      "step": 910
    },
    {
      "bce_eff_w": 0.27665,
      "bce_loss": -0.4574875831604004,
      "epoch": 4.55,
      "step": 910
    },
    {
      "epoch": 4.555,
      "grad_norm": 17.042892456054688,
      "learning_rate": 3.40625e-05,
      "loss": 0.6101,
      "step": 911
    },
    {
      "bce_eff_w": 0.2768,
      "bce_loss": -0.4635726511478424,
      "epoch": 4.555,
      "step": 911
    },
    {
      "epoch": 4.5600000000000005,
      "grad_norm": 19.626447677612305,
      "learning_rate": 3.4031250000000004e-05,
      "loss": 2.2509,
      "step": 912
    },
    {
      "bce_eff_w": 0.27695000000000003,
      "bce_loss": -0.46255528926849365,
      "epoch": 4.5600000000000005,
      "step": 912
    },
    {
      "epoch": 4.5649999999999995,
      "grad_norm": 19.14229965209961,
      "learning_rate": 3.4000000000000007e-05,
      "loss": 0.6783,
      "step": 913
    },
    {
      "bce_eff_w": 0.2771,
      "bce_loss": -0.46476447582244873,
      "epoch": 4.5649999999999995,
      "step": 913
    },
    {
      "epoch": 4.57,
      "grad_norm": 25.41866683959961,
      "learning_rate": 3.396875e-05,
      "loss": 2.4075,
      "step": 914
    },
    {
      "bce_eff_w": 0.27725,
      "bce_loss": -0.46214020252227783,
      "epoch": 4.57,
      "step": 914
    },
    {
      "epoch": 4.575,
      "grad_norm": 23.719959259033203,
      "learning_rate": 3.39375e-05,
      "loss": 0.6761,
      "step": 915
    },
    {
      "bce_eff_w": 0.2774,
      "bce_loss": -0.4645209312438965,
      "epoch": 4.575,
      "step": 915
    },
    {
      "epoch": 4.58,
      "grad_norm": 22.35774040222168,
      "learning_rate": 3.390625e-05,
      "loss": 1.3671,
      "step": 916
    },
    {
      "bce_eff_w": 0.27755,
      "bce_loss": -0.46224847435951233,
      "epoch": 4.58,
      "step": 916
    },
    {
      "epoch": 4.585,
      "grad_norm": 15.372345924377441,
      "learning_rate": 3.3875000000000003e-05,
      "loss": 3.3673,
      "step": 917
    },
    {
      "bce_eff_w": 0.2777,
      "bce_loss": -0.4639311730861664,
      "epoch": 4.585,
      "step": 917
    },
    {
      "epoch": 4.59,
      "grad_norm": 17.734914779663086,
      "learning_rate": 3.384375e-05,
      "loss": 1.2957,
      "step": 918
    },
    {
      "bce_eff_w": 0.27785000000000004,
      "bce_loss": -0.46192896366119385,
      "epoch": 4.59,
      "step": 918
    },
    {
      "epoch": 4.595,
      "grad_norm": 20.666372299194336,
      "learning_rate": 3.38125e-05,
      "loss": 0.717,
      "step": 919
    },
    {
      "bce_eff_w": 0.278,
      "bce_loss": -0.4642500877380371,
      "epoch": 4.595,
      "step": 919
    },
    {
      "epoch": 4.6,
      "grad_norm": 27.284791946411133,
      "learning_rate": 3.3781250000000005e-05,
      "loss": 2.4785,
      "step": 920
    },
    {
      "bce_eff_w": 0.27815,
      "bce_loss": -0.4633522033691406,
      "epoch": 4.6,
      "step": 920
    },
    {
      "epoch": 4.605,
      "grad_norm": 22.31779670715332,
      "learning_rate": 3.375000000000001e-05,
      "loss": 2.26,
      "step": 921
    },
    {
      "bce_eff_w": 0.2783,
      "bce_loss": -0.46129804849624634,
      "epoch": 4.605,
      "step": 921
    },
    {
      "epoch": 4.61,
      "grad_norm": 20.09904670715332,
      "learning_rate": 3.3718749999999996e-05,
      "loss": 1.073,
      "step": 922
    },
    {
      "bce_eff_w": 0.27845000000000003,
      "bce_loss": -0.4593167006969452,
      "epoch": 4.61,
      "step": 922
    },
    {
      "epoch": 4.615,
      "grad_norm": 29.79250717163086,
      "learning_rate": 3.36875e-05,
      "loss": 1.9374,
      "step": 923
    },
    {
      "bce_eff_w": 0.2786,
      "bce_loss": -0.4641103744506836,
      "epoch": 4.615,
      "step": 923
    },
    {
      "epoch": 4.62,
      "grad_norm": 19.05419921875,
      "learning_rate": 3.365625e-05,
      "loss": 0.5146,
      "step": 924
    },
    {
      "bce_eff_w": 0.27875,
      "bce_loss": -0.4633926749229431,
      "epoch": 4.62,
      "step": 924
    },
    {
      "epoch": 4.625,
      "grad_norm": 39.04126739501953,
      "learning_rate": 3.3625000000000004e-05,
      "loss": 1.419,
      "step": 925
    },
    {
      "bce_eff_w": 0.27890000000000004,
      "bce_loss": -0.4639115333557129,
      "epoch": 4.625,
      "step": 925
    },
    {
      "epoch": 4.63,
      "grad_norm": 10.216931343078613,
      "learning_rate": 3.359375e-05,
      "loss": 0.0088,
      "step": 926
    },
    {
      "bce_eff_w": 0.27905,
      "bce_loss": -0.4636455774307251,
      "epoch": 4.63,
      "step": 926
    },
    {
      "epoch": 4.635,
      "grad_norm": 18.498361587524414,
      "learning_rate": 3.35625e-05,
      "loss": 0.7349,
      "step": 927
    },
    {
      "bce_eff_w": 0.2792,
      "bce_loss": -0.4607783555984497,
      "epoch": 4.635,
      "step": 927
    },
    {
      "epoch": 4.64,
      "grad_norm": 20.42919158935547,
      "learning_rate": 3.3531250000000005e-05,
      "loss": 2.0038,
      "step": 928
    },
    {
      "bce_eff_w": 0.27935,
      "bce_loss": -0.462370902299881,
      "epoch": 4.64,
      "step": 928
    },
    {
      "epoch": 4.645,
      "grad_norm": 47.18323516845703,
      "learning_rate": 3.35e-05,
      "loss": 2.1033,
      "step": 929
    },
    {
      "bce_eff_w": 0.2795,
      "bce_loss": -0.46253159642219543,
      "epoch": 4.645,
      "step": 929
    },
    {
      "epoch": 4.65,
      "grad_norm": 25.119421005249023,
      "learning_rate": 3.3468750000000004e-05,
      "loss": 0.9706,
      "step": 930
    },
    {
      "bce_eff_w": 0.27965,
      "bce_loss": -0.46210238337516785,
      "epoch": 4.65,
      "step": 930
    },
    {
      "epoch": 4.655,
      "grad_norm": 28.29493522644043,
      "learning_rate": 3.34375e-05,
      "loss": 0.8038,
      "step": 931
    },
    {
      "bce_eff_w": 0.2798,
      "bce_loss": -0.46252959966659546,
      "epoch": 4.655,
      "step": 931
    },
    {
      "epoch": 4.66,
      "grad_norm": 23.250797271728516,
      "learning_rate": 3.340625e-05,
      "loss": 0.9055,
      "step": 932
    },
    {
      "bce_eff_w": 0.27995000000000003,
      "bce_loss": -0.46287259459495544,
      "epoch": 4.66,
      "step": 932
    },
    {
      "epoch": 4.665,
      "grad_norm": 26.4674129486084,
      "learning_rate": 3.3375e-05,
      "loss": 0.5454,
      "step": 933
    },
    {
      "bce_eff_w": 0.2801,
      "bce_loss": -0.46292081475257874,
      "epoch": 4.665,
      "step": 933
    },
    {
      "epoch": 4.67,
      "grad_norm": 20.031444549560547,
      "learning_rate": 3.334375e-05,
      "loss": 1.7602,
      "step": 934
    },
    {
      "bce_eff_w": 0.28025,
      "bce_loss": -0.46433785557746887,
      "epoch": 4.67,
      "step": 934
    },
    {
      "epoch": 4.675,
      "grad_norm": 22.635833740234375,
      "learning_rate": 3.33125e-05,
      "loss": 0.3842,
      "step": 935
    },
    {
      "bce_eff_w": 0.2804,
      "bce_loss": -0.46218010783195496,
      "epoch": 4.675,
      "step": 935
    },
    {
      "epoch": 4.68,
      "grad_norm": 6.864044189453125,
      "learning_rate": 3.3281250000000006e-05,
      "loss": -0.0591,
      "step": 936
    },
    {
      "bce_eff_w": 0.28055,
      "bce_loss": -0.4644019603729248,
      "epoch": 4.68,
      "step": 936
    },
    {
      "epoch": 4.6850000000000005,
      "grad_norm": 29.01436996459961,
      "learning_rate": 3.325e-05,
      "loss": 0.1738,
      "step": 937
    },
    {
      "bce_eff_w": 0.2807,
      "bce_loss": -0.45800837874412537,
      "epoch": 4.6850000000000005,
      "step": 937
    },
    {
      "epoch": 4.6899999999999995,
      "grad_norm": 26.59663963317871,
      "learning_rate": 3.3218750000000004e-05,
      "loss": 3.0401,
      "step": 938
    },
    {
      "bce_eff_w": 0.28085000000000004,
      "bce_loss": -0.4647030830383301,
      "epoch": 4.6899999999999995,
      "step": 938
    },
    {
      "epoch": 4.695,
      "grad_norm": 21.46460723876953,
      "learning_rate": 3.31875e-05,
      "loss": 0.9205,
      "step": 939
    },
    {
      "bce_eff_w": 0.281,
      "bce_loss": -0.45939868688583374,
      "epoch": 4.695,
      "step": 939
    },
    {
      "epoch": 4.7,
      "grad_norm": 31.824007034301758,
      "learning_rate": 3.315625e-05,
      "loss": 1.6467,
      "step": 940
    },
    {
      "bce_eff_w": 0.28115,
      "bce_loss": -0.4614761769771576,
      "epoch": 4.7,
      "step": 940
    },
    {
      "epoch": 4.705,
      "grad_norm": 25.525699615478516,
      "learning_rate": 3.3125e-05,
      "loss": 0.9849,
      "step": 941
    },
    {
      "bce_eff_w": 0.2813,
      "bce_loss": -0.46477562189102173,
      "epoch": 4.705,
      "step": 941
    },
    {
      "epoch": 4.71,
      "grad_norm": 20.85590362548828,
      "learning_rate": 3.309375e-05,
      "loss": 0.7377,
      "step": 942
    },
    {
      "bce_eff_w": 0.28145000000000003,
      "bce_loss": -0.4637696444988251,
      "epoch": 4.71,
      "step": 942
    },
    {
      "epoch": 4.715,
      "grad_norm": 23.232776641845703,
      "learning_rate": 3.3062500000000004e-05,
      "loss": 2.0022,
      "step": 943
    },
    {
      "bce_eff_w": 0.2816,
      "bce_loss": -0.46415993571281433,
      "epoch": 4.715,
      "step": 943
    },
    {
      "epoch": 4.72,
      "grad_norm": 29.377845764160156,
      "learning_rate": 3.303125e-05,
      "loss": 0.8338,
      "step": 944
    },
    {
      "bce_eff_w": 0.28175,
      "bce_loss": -0.46405190229415894,
      "epoch": 4.72,
      "step": 944
    },
    {
      "epoch": 4.725,
      "grad_norm": 24.736896514892578,
      "learning_rate": 3.3e-05,
      "loss": 0.8224,
      "step": 945
    },
    {
      "bce_eff_w": 0.28190000000000004,
      "bce_loss": -0.4608002007007599,
      "epoch": 4.725,
      "step": 945
    },
    {
      "epoch": 4.73,
      "grad_norm": 24.448945999145508,
      "learning_rate": 3.2968750000000005e-05,
      "loss": 1.7405,
      "step": 946
    },
    {
      "bce_eff_w": 0.28205,
      "bce_loss": -0.46083953976631165,
      "epoch": 4.73,
      "step": 946
    },
    {
      "epoch": 4.735,
      "grad_norm": 23.823305130004883,
      "learning_rate": 3.29375e-05,
      "loss": 2.6151,
      "step": 947
    },
    {
      "bce_eff_w": 0.2822,
      "bce_loss": -0.4637671411037445,
      "epoch": 4.735,
      "step": 947
    },
    {
      "epoch": 4.74,
      "grad_norm": 32.76198959350586,
      "learning_rate": 3.290625e-05,
      "loss": 1.4345,
      "step": 948
    },
    {
      "bce_eff_w": 0.28235,
      "bce_loss": -0.4606684148311615,
      "epoch": 4.74,
      "step": 948
    },
    {
      "epoch": 4.745,
      "grad_norm": 27.13465118408203,
      "learning_rate": 3.2875e-05,
      "loss": 0.4751,
      "step": 949
    },
    {
      "bce_eff_w": 0.28250000000000003,
      "bce_loss": -0.4634127616882324,
      "epoch": 4.745,
      "step": 949
    },
    {
      "epoch": 4.75,
      "grad_norm": 16.44879150390625,
      "learning_rate": 3.284375e-05,
      "loss": 2.1777,
      "step": 950
    },
    {
      "bce_eff_w": 0.28265,
      "bce_loss": -0.4651782512664795,
      "epoch": 4.75,
      "step": 950
    },
    {
      "epoch": 4.755,
      "grad_norm": 19.3489990234375,
      "learning_rate": 3.2812500000000005e-05,
      "loss": 1.9163,
      "step": 951
    },
    {
      "bce_eff_w": 0.2828,
      "bce_loss": -0.4634060263633728,
      "epoch": 4.755,
      "step": 951
    },
    {
      "epoch": 4.76,
      "grad_norm": 20.416576385498047,
      "learning_rate": 3.278125e-05,
      "loss": 1.9817,
      "step": 952
    },
    {
      "bce_eff_w": 0.28295000000000003,
      "bce_loss": -0.4637252986431122,
      "epoch": 4.76,
      "step": 952
    },
    {
      "epoch": 4.765,
      "grad_norm": 29.99519157409668,
      "learning_rate": 3.275e-05,
      "loss": 0.7347,
      "step": 953
    },
    {
      "bce_eff_w": 0.2831,
      "bce_loss": -0.46141231060028076,
      "epoch": 4.765,
      "step": 953
    },
    {
      "epoch": 4.77,
      "grad_norm": 24.925172805786133,
      "learning_rate": 3.2718750000000006e-05,
      "loss": 1.185,
      "step": 954
    },
    {
      "bce_eff_w": 0.28325,
      "bce_loss": -0.4637952446937561,
      "epoch": 4.77,
      "step": 954
    },
    {
      "epoch": 4.775,
      "grad_norm": 16.661224365234375,
      "learning_rate": 3.26875e-05,
      "loss": 2.1268,
      "step": 955
    },
    {
      "bce_eff_w": 0.2834,
      "bce_loss": -0.458160936832428,
      "epoch": 4.775,
      "step": 955
    },
    {
      "epoch": 4.78,
      "grad_norm": 14.970745086669922,
      "learning_rate": 3.265625e-05,
      "loss": 3.2217,
      "step": 956
    },
    {
      "bce_eff_w": 0.28355,
      "bce_loss": -0.4625435769557953,
      "epoch": 4.78,
      "step": 956
    },
    {
      "epoch": 4.785,
      "grad_norm": 24.166301727294922,
      "learning_rate": 3.2625e-05,
      "loss": 0.11,
      "step": 957
    },
    {
      "bce_eff_w": 0.2837,
      "bce_loss": -0.46448054909706116,
      "epoch": 4.785,
      "step": 957
    },
    {
      "epoch": 4.79,
      "grad_norm": 17.48735809326172,
      "learning_rate": 3.259375e-05,
      "loss": 1.2744,
      "step": 958
    },
    {
      "bce_eff_w": 0.28385000000000005,
      "bce_loss": -0.4637475609779358,
      "epoch": 4.79,
      "step": 958
    },
    {
      "epoch": 4.795,
      "grad_norm": 18.097688674926758,
      "learning_rate": 3.25625e-05,
      "loss": 0.3272,
      "step": 959
    },
    {
      "bce_eff_w": 0.28400000000000003,
      "bce_loss": -0.46234339475631714,
      "epoch": 4.795,
      "step": 959
    },
    {
      "epoch": 4.8,
      "grad_norm": 20.026124954223633,
      "learning_rate": 3.253125e-05,
      "loss": 0.0674,
      "step": 960
    },
    {
      "bce_eff_w": 0.28415,
      "bce_loss": -0.45774945616722107,
      "epoch": 4.8,
      "step": 960
    },
    {
      "epoch": 4.805,
      "grad_norm": 24.53348731994629,
      "learning_rate": 3.2500000000000004e-05,
      "loss": 2.6062,
      "step": 961
    },
    {
      "bce_eff_w": 0.2843,
      "bce_loss": -0.4598256051540375,
      "epoch": 4.805,
      "step": 961
    },
    {
      "epoch": 4.8100000000000005,
      "grad_norm": 25.963111877441406,
      "learning_rate": 3.2468750000000007e-05,
      "loss": 0.328,
      "step": 962
    },
    {
      "bce_eff_w": 0.28445,
      "bce_loss": -0.45921313762664795,
      "epoch": 4.8100000000000005,
      "step": 962
    },
    {
      "epoch": 4.8149999999999995,
      "grad_norm": 28.106287002563477,
      "learning_rate": 3.24375e-05,
      "loss": 1.0447,
      "step": 963
    },
    {
      "bce_eff_w": 0.2846,
      "bce_loss": -0.4577749967575073,
      "epoch": 4.8149999999999995,
      "step": 963
    },
    {
      "epoch": 4.82,
      "grad_norm": 17.55670928955078,
      "learning_rate": 3.240625e-05,
      "loss": 1.2229,
      "step": 964
    },
    {
      "bce_eff_w": 0.28475,
      "bce_loss": -0.46304744482040405,
      "epoch": 4.82,
      "step": 964
    },
    {
      "epoch": 4.825,
      "grad_norm": 29.906251907348633,
      "learning_rate": 3.2375e-05,
      "loss": 0.3512,
      "step": 965
    },
    {
      "bce_eff_w": 0.2849,
      "bce_loss": -0.46103593707084656,
      "epoch": 4.825,
      "step": 965
    },
    {
      "epoch": 4.83,
      "grad_norm": 21.7232608795166,
      "learning_rate": 3.2343750000000004e-05,
      "loss": 1.3928,
      "step": 966
    },
    {
      "bce_eff_w": 0.28505,
      "bce_loss": -0.46213164925575256,
      "epoch": 4.83,
      "step": 966
    },
    {
      "epoch": 4.835,
      "grad_norm": 16.861671447753906,
      "learning_rate": 3.23125e-05,
      "loss": 0.3157,
      "step": 967
    },
    {
      "bce_eff_w": 0.2852,
      "bce_loss": -0.46296873688697815,
      "epoch": 4.835,
      "step": 967
    },
    {
      "epoch": 4.84,
      "grad_norm": 23.9545955657959,
      "learning_rate": 3.228125e-05,
      "loss": 1.0107,
      "step": 968
    },
    {
      "bce_eff_w": 0.28535,
      "bce_loss": -0.45276939868927,
      "epoch": 4.84,
      "step": 968
    },
    {
      "epoch": 4.845,
      "grad_norm": 10.665818214416504,
      "learning_rate": 3.2250000000000005e-05,
      "loss": 0.2042,
      "step": 969
    },
    {
      "bce_eff_w": 0.2855,
      "bce_loss": -0.463430255651474,
      "epoch": 4.845,
      "step": 969
    },
    {
      "epoch": 4.85,
      "grad_norm": 25.306379318237305,
      "learning_rate": 3.221875e-05,
      "loss": 0.8119,
      "step": 970
    },
    {
      "bce_eff_w": 0.28565,
      "bce_loss": -0.45271772146224976,
      "epoch": 4.85,
      "step": 970
    },
    {
      "epoch": 4.855,
      "grad_norm": 21.456403732299805,
      "learning_rate": 3.21875e-05,
      "loss": 1.4718,
      "step": 971
    },
    {
      "bce_eff_w": 0.2858,
      "bce_loss": -0.4551543593406677,
      "epoch": 4.855,
      "step": 971
    },
    {
      "epoch": 4.86,
      "grad_norm": 17.5866756439209,
      "learning_rate": 3.215625e-05,
      "loss": 0.6628,
      "step": 972
    },
    {
      "bce_eff_w": 0.28595,
      "bce_loss": -0.46402040123939514,
      "epoch": 4.86,
      "step": 972
    },
    {
      "epoch": 4.865,
      "grad_norm": 16.411861419677734,
      "learning_rate": 3.2125e-05,
      "loss": 2.229,
      "step": 973
    },
    {
      "bce_eff_w": 0.2861,
      "bce_loss": -0.46212315559387207,
      "epoch": 4.865,
      "step": 973
    },
    {
      "epoch": 4.87,
      "grad_norm": 19.681180953979492,
      "learning_rate": 3.209375e-05,
      "loss": 1.2155,
      "step": 974
    },
    {
      "bce_eff_w": 0.28625,
      "bce_loss": -0.46185654401779175,
      "epoch": 4.87,
      "step": 974
    },
    {
      "epoch": 4.875,
      "grad_norm": 15.550122261047363,
      "learning_rate": 3.20625e-05,
      "loss": 3.4117,
      "step": 975
    },
    {
      "bce_eff_w": 0.2864,
      "bce_loss": -0.4632917046546936,
      "epoch": 4.875,
      "step": 975
    },
    {
      "epoch": 4.88,
      "grad_norm": 18.9754638671875,
      "learning_rate": 3.203125e-05,
      "loss": 1.5638,
      "step": 976
    },
    {
      "bce_eff_w": 0.28654999999999997,
      "bce_loss": -0.4625278115272522,
      "epoch": 4.88,
      "step": 976
    },
    {
      "epoch": 4.885,
      "grad_norm": 21.675569534301758,
      "learning_rate": 3.2000000000000005e-05,
      "loss": 1.1625,
      "step": 977
    },
    {
      "bce_eff_w": 0.2867,
      "bce_loss": -0.4529399573802948,
      "epoch": 4.885,
      "step": 977
    },
    {
      "epoch": 4.89,
      "grad_norm": 14.871078491210938,
      "learning_rate": 3.196875e-05,
      "loss": 0.3763,
      "step": 978
    },
    {
      "bce_eff_w": 0.28685,
      "bce_loss": -0.45707806944847107,
      "epoch": 4.89,
      "step": 978
    },
    {
      "epoch": 4.895,
      "grad_norm": 20.53673553466797,
      "learning_rate": 3.1937500000000004e-05,
      "loss": 1.8924,
      "step": 979
    },
    {
      "bce_eff_w": 0.28700000000000003,
      "bce_loss": -0.4565449059009552,
      "epoch": 4.895,
      "step": 979
    },
    {
      "epoch": 4.9,
      "grad_norm": 19.436595916748047,
      "learning_rate": 3.1906250000000006e-05,
      "loss": 2.6088,
      "step": 980
    },
    {
      "bce_eff_w": 0.28715,
      "bce_loss": -0.4647928476333618,
      "epoch": 4.9,
      "step": 980
    },
    {
      "epoch": 4.905,
      "grad_norm": 16.87645721435547,
      "learning_rate": 3.1875e-05,
      "loss": 1.7368,
      "step": 981
    },
    {
      "bce_eff_w": 0.2873,
      "bce_loss": -0.46426376700401306,
      "epoch": 4.905,
      "step": 981
    },
    {
      "epoch": 4.91,
      "grad_norm": 17.519380569458008,
      "learning_rate": 3.184375e-05,
      "loss": 1.9465,
      "step": 982
    },
    {
      "bce_eff_w": 0.28745,
      "bce_loss": -0.46198731660842896,
      "epoch": 4.91,
      "step": 982
    },
    {
      "epoch": 4.915,
      "grad_norm": 15.50132942199707,
      "learning_rate": 3.18125e-05,
      "loss": 0.2187,
      "step": 983
    },
    {
      "bce_eff_w": 0.2876,
      "bce_loss": -0.463628351688385,
      "epoch": 4.915,
      "step": 983
    },
    {
      "epoch": 4.92,
      "grad_norm": 19.927581787109375,
      "learning_rate": 3.1781250000000003e-05,
      "loss": 0.3665,
      "step": 984
    },
    {
      "bce_eff_w": 0.28775,
      "bce_loss": -0.4542529582977295,
      "epoch": 4.92,
      "step": 984
    },
    {
      "epoch": 4.925,
      "grad_norm": 22.083444595336914,
      "learning_rate": 3.175e-05,
      "loss": 1.1096,
      "step": 985
    },
    {
      "bce_eff_w": 0.2879,
      "bce_loss": -0.45039451122283936,
      "epoch": 4.925,
      "step": 985
    },
    {
      "epoch": 4.93,
      "grad_norm": 17.004573822021484,
      "learning_rate": 3.171875e-05,
      "loss": 0.7059,
      "step": 986
    },
    {
      "bce_eff_w": 0.28805000000000003,
      "bce_loss": -0.4628695547580719,
      "epoch": 4.93,
      "step": 986
    },
    {
      "epoch": 4.9350000000000005,
      "grad_norm": 23.494142532348633,
      "learning_rate": 3.1687500000000005e-05,
      "loss": 0.4916,
      "step": 987
    },
    {
      "bce_eff_w": 0.2882,
      "bce_loss": -0.4634179174900055,
      "epoch": 4.9350000000000005,
      "step": 987
    },
    {
      "epoch": 4.9399999999999995,
      "grad_norm": 24.76317596435547,
      "learning_rate": 3.165625000000001e-05,
      "loss": 1.7077,
      "step": 988
    },
    {
      "bce_eff_w": 0.28835,
      "bce_loss": -0.4628296494483948,
      "epoch": 4.9399999999999995,
      "step": 988
    },
    {
      "epoch": 4.945,
      "grad_norm": 22.406911849975586,
      "learning_rate": 3.1624999999999996e-05,
      "loss": 0.4363,
      "step": 989
    },
    {
      "bce_eff_w": 0.2885,
      "bce_loss": -0.4643179476261139,
      "epoch": 4.945,
      "step": 989
    },
    {
      "epoch": 4.95,
      "grad_norm": 15.287118911743164,
      "learning_rate": 3.159375e-05,
      "loss": 2.4537,
      "step": 990
    },
    {
      "bce_eff_w": 0.28865,
      "bce_loss": -0.46360379457473755,
      "epoch": 4.95,
      "step": 990
    },
    {
      "epoch": 4.955,
      "grad_norm": 5.052629470825195,
      "learning_rate": 3.15625e-05,
      "loss": -0.0619,
      "step": 991
    },
    {
      "bce_eff_w": 0.2888,
      "bce_loss": -0.4629421830177307,
      "epoch": 4.955,
      "step": 991
    },
    {
      "epoch": 4.96,
      "grad_norm": 38.42142105102539,
      "learning_rate": 3.1531250000000004e-05,
      "loss": 2.6919,
      "step": 992
    },
    {
      "bce_eff_w": 0.28895,
      "bce_loss": -0.4639379680156708,
      "epoch": 4.96,
      "step": 992
    },
    {
      "epoch": 4.965,
      "grad_norm": 18.12915802001953,
      "learning_rate": 3.15e-05,
      "loss": 1.4892,
      "step": 993
    },
    {
      "bce_eff_w": 0.2891,
      "bce_loss": -0.461586058139801,
      "epoch": 4.965,
      "step": 993
    },
    {
      "epoch": 4.97,
      "grad_norm": 25.868417739868164,
      "learning_rate": 3.146875e-05,
      "loss": 0.5936,
      "step": 994
    },
    {
      "bce_eff_w": 0.28925,
      "bce_loss": -0.46410655975341797,
      "epoch": 4.97,
      "step": 994
    },
    {
      "epoch": 4.975,
      "grad_norm": 38.18778991699219,
      "learning_rate": 3.1437500000000005e-05,
      "loss": 1.1213,
      "step": 995
    },
    {
      "bce_eff_w": 0.2894,
      "bce_loss": -0.4630899727344513,
      "epoch": 4.975,
      "step": 995
    },
    {
      "epoch": 4.98,
      "grad_norm": 28.83405113220215,
      "learning_rate": 3.140625e-05,
      "loss": 1.1107,
      "step": 996
    },
    {
      "bce_eff_w": 0.28955,
      "bce_loss": -0.4515332281589508,
      "epoch": 4.98,
      "step": 996
    },
    {
      "epoch": 4.985,
      "grad_norm": 22.852975845336914,
      "learning_rate": 3.1375e-05,
      "loss": 0.896,
      "step": 997
    },
    {
      "bce_eff_w": 0.2897,
      "bce_loss": -0.46184423565864563,
      "epoch": 4.985,
      "step": 997
    },
    {
      "epoch": 4.99,
      "grad_norm": 15.478127479553223,
      "learning_rate": 3.134375e-05,
      "loss": 3.2079,
      "step": 998
    },
    {
      "bce_eff_w": 0.28985,
      "bce_loss": -0.46290528774261475,
      "epoch": 4.99,
      "step": 998
    },
    {
      "epoch": 4.995,
      "grad_norm": 30.082595825195312,
      "learning_rate": 3.13125e-05,
      "loss": 1.8323,
      "step": 999
    },
    {
      "bce_eff_w": 0.29000000000000004,
      "bce_loss": -0.4594341814517975,
      "epoch": 4.995,
      "step": 999
    },
    {
      "epoch": 5.0,
      "grad_norm": 19.553592681884766,
      "learning_rate": 3.128125e-05,
      "loss": 2.2964,
      "step": 1000
    },
    {
      "bce_eff_w": 0.29015,
      "bce_loss": -0.45542556047439575,
      "epoch": 5.0,
      "step": 1000
    },
    {
      "epoch": 5.005,
      "grad_norm": 17.91409683227539,
      "learning_rate": 3.125e-05,
      "loss": 1.0284,
      "step": 1001
    },
    {
      "bce_eff_w": 0.2903,
      "bce_loss": -0.4597468078136444,
      "epoch": 5.005,
      "step": 1001
    },
    {
      "epoch": 5.01,
      "grad_norm": 12.138276100158691,
      "learning_rate": 3.121875e-05,
      "loss": 0.068,
      "step": 1002
    },
    {
      "bce_eff_w": 0.29045,
      "bce_loss": -0.46335408091545105,
      "epoch": 5.01,
      "step": 1002
    },
    {
      "epoch": 5.015,
      "grad_norm": 16.726436614990234,
      "learning_rate": 3.1187500000000006e-05,
      "loss": 1.7964,
      "step": 1003
    },
    {
      "bce_eff_w": 0.2906,
      "bce_loss": -0.4630318284034729,
      "epoch": 5.015,
      "step": 1003
    },
    {
      "epoch": 5.02,
      "grad_norm": 22.347219467163086,
      "learning_rate": 3.115625e-05,
      "loss": 0.4845,
      "step": 1004
    },
    {
      "bce_eff_w": 0.29075,
      "bce_loss": -0.46175289154052734,
      "epoch": 5.02,
      "step": 1004
    },
    {
      "epoch": 5.025,
      "grad_norm": 20.168075561523438,
      "learning_rate": 3.1125000000000004e-05,
      "loss": 0.5579,
      "step": 1005
    },
    {
      "bce_eff_w": 0.2909,
      "bce_loss": -0.460971474647522,
      "epoch": 5.025,
      "step": 1005
    },
    {
      "epoch": 5.03,
      "grad_norm": 19.875835418701172,
      "learning_rate": 3.109375e-05,
      "loss": 2.4872,
      "step": 1006
    },
    {
      "bce_eff_w": 0.29105000000000003,
      "bce_loss": -0.4648101329803467,
      "epoch": 5.03,
      "step": 1006
    },
    {
      "epoch": 5.035,
      "grad_norm": 24.402835845947266,
      "learning_rate": 3.10625e-05,
      "loss": 2.4344,
      "step": 1007
    },
    {
      "bce_eff_w": 0.2912,
      "bce_loss": -0.46243518590927124,
      "epoch": 5.035,
      "step": 1007
    },
    {
      "epoch": 5.04,
      "grad_norm": 20.46363639831543,
      "learning_rate": 3.103125e-05,
      "loss": 1.8532,
      "step": 1008
    },
    {
      "bce_eff_w": 0.29135,
      "bce_loss": -0.45829570293426514,
      "epoch": 5.04,
      "step": 1008
    },
    {
      "epoch": 5.045,
      "grad_norm": 25.18379783630371,
      "learning_rate": 3.1e-05,
      "loss": 1.2298,
      "step": 1009
    },
    {
      "bce_eff_w": 0.2915,
      "bce_loss": -0.46363985538482666,
      "epoch": 5.045,
      "step": 1009
    },
    {
      "epoch": 5.05,
      "grad_norm": 15.001667976379395,
      "learning_rate": 3.0968750000000004e-05,
      "loss": 0.4005,
      "step": 1010
    },
    {
      "bce_eff_w": 0.29165,
      "bce_loss": -0.46261629462242126,
      "epoch": 5.05,
      "step": 1010
    },
    {
      "epoch": 5.055,
      "grad_norm": 22.128324508666992,
      "learning_rate": 3.09375e-05,
      "loss": 0.6493,
      "step": 1011
    },
    {
      "bce_eff_w": 0.2918,
      "bce_loss": -0.46302923560142517,
      "epoch": 5.055,
      "step": 1011
    },
    {
      "epoch": 5.06,
      "grad_norm": 19.588315963745117,
      "learning_rate": 3.090625e-05,
      "loss": 0.5002,
      "step": 1012
    },
    {
      "bce_eff_w": 0.29195,
      "bce_loss": -0.46341297030448914,
      "epoch": 5.06,
      "step": 1012
    },
    {
      "epoch": 5.065,
      "grad_norm": 3.6741232872009277,
      "learning_rate": 3.0875000000000005e-05,
      "loss": -0.0921,
      "step": 1013
    },
    {
      "bce_eff_w": 0.2921,
      "bce_loss": -0.46443966031074524,
      "epoch": 5.065,
      "step": 1013
    },
    {
      "epoch": 5.07,
      "grad_norm": 16.786178588867188,
      "learning_rate": 3.084375e-05,
      "loss": 1.7511,
      "step": 1014
    },
    {
      "bce_eff_w": 0.29225,
      "bce_loss": -0.46156445145606995,
      "epoch": 5.07,
      "step": 1014
    },
    {
      "epoch": 5.075,
      "grad_norm": 17.809852600097656,
      "learning_rate": 3.08125e-05,
      "loss": 1.3514,
      "step": 1015
    },
    {
      "bce_eff_w": 0.2924,
      "bce_loss": -0.46383601427078247,
      "epoch": 5.075,
      "step": 1015
    },
    {
      "epoch": 5.08,
      "grad_norm": 23.7473201751709,
      "learning_rate": 3.078125e-05,
      "loss": 0.6058,
      "step": 1016
    },
    {
      "bce_eff_w": 0.29255,
      "bce_loss": -0.4581243097782135,
      "epoch": 5.08,
      "step": 1016
    },
    {
      "epoch": 5.085,
      "grad_norm": 13.520681381225586,
      "learning_rate": 3.075e-05,
      "loss": 0.3103,
      "step": 1017
    },
    {
      "bce_eff_w": 0.2927,
      "bce_loss": -0.46188727021217346,
      "epoch": 5.085,
      "step": 1017
    },
    {
      "epoch": 5.09,
      "grad_norm": 24.955894470214844,
      "learning_rate": 3.0718750000000005e-05,
      "loss": 0.7789,
      "step": 1018
    },
    {
      "bce_eff_w": 0.29285,
      "bce_loss": -0.4617869257926941,
      "epoch": 5.09,
      "step": 1018
    },
    {
      "epoch": 5.095,
      "grad_norm": 17.453327178955078,
      "learning_rate": 3.06875e-05,
      "loss": 0.4098,
      "step": 1019
    },
    {
      "bce_eff_w": 0.29300000000000004,
      "bce_loss": -0.46371912956237793,
      "epoch": 5.095,
      "step": 1019
    },
    {
      "epoch": 5.1,
      "grad_norm": 20.806032180786133,
      "learning_rate": 3.065625e-05,
      "loss": 0.6379,
      "step": 1020
    },
    {
      "bce_eff_w": 0.29315,
      "bce_loss": -0.4616609215736389,
      "epoch": 5.1,
      "step": 1020
    },
    {
      "epoch": 5.105,
      "grad_norm": 13.619138717651367,
      "learning_rate": 3.0625000000000006e-05,
      "loss": 0.524,
      "step": 1021
    },
    {
      "bce_eff_w": 0.2933,
      "bce_loss": -0.46103617548942566,
      "epoch": 5.105,
      "step": 1021
    },
    {
      "epoch": 5.11,
      "grad_norm": 27.065887451171875,
      "learning_rate": 3.059375e-05,
      "loss": 1.1631,
      "step": 1022
    },
    {
      "bce_eff_w": 0.29345,
      "bce_loss": -0.462363600730896,
      "epoch": 5.11,
      "step": 1022
    },
    {
      "epoch": 5.115,
      "grad_norm": 19.74294090270996,
      "learning_rate": 3.05625e-05,
      "loss": 1.4658,
      "step": 1023
    },
    {
      "bce_eff_w": 0.2936,
      "bce_loss": -0.4562513530254364,
      "epoch": 5.115,
      "step": 1023
    },
    {
      "epoch": 5.12,
      "grad_norm": 18.02804946899414,
      "learning_rate": 3.053125e-05,
      "loss": 1.2232,
      "step": 1024
    },
    {
      "bce_eff_w": 0.29375,
      "bce_loss": -0.4561823606491089,
      "epoch": 5.12,
      "step": 1024
    },
    {
      "epoch": 5.125,
      "grad_norm": 15.471036911010742,
      "learning_rate": 3.05e-05,
      "loss": 0.1642,
      "step": 1025
    },
    {
      "bce_eff_w": 0.2939,
      "bce_loss": -0.4630272388458252,
      "epoch": 5.125,
      "step": 1025
    },
    {
      "epoch": 5.13,
      "grad_norm": 22.678674697875977,
      "learning_rate": 3.0468750000000002e-05,
      "loss": 2.2576,
      "step": 1026
    },
    {
      "bce_eff_w": 0.29405000000000003,
      "bce_loss": -0.4603367745876312,
      "epoch": 5.13,
      "step": 1026
    },
    {
      "epoch": 5.135,
      "grad_norm": 24.363788604736328,
      "learning_rate": 3.04375e-05,
      "loss": 1.4911,
      "step": 1027
    },
    {
      "bce_eff_w": 0.2942,
      "bce_loss": -0.46039465069770813,
      "epoch": 5.135,
      "step": 1027
    },
    {
      "epoch": 5.14,
      "grad_norm": 25.547056198120117,
      "learning_rate": 3.0406250000000004e-05,
      "loss": 0.6797,
      "step": 1028
    },
    {
      "bce_eff_w": 0.29435,
      "bce_loss": -0.4610779881477356,
      "epoch": 5.14,
      "step": 1028
    },
    {
      "epoch": 5.145,
      "grad_norm": 24.4047794342041,
      "learning_rate": 3.0375000000000003e-05,
      "loss": 0.5151,
      "step": 1029
    },
    {
      "bce_eff_w": 0.2945,
      "bce_loss": -0.463848352432251,
      "epoch": 5.145,
      "step": 1029
    },
    {
      "epoch": 5.15,
      "grad_norm": 17.867145538330078,
      "learning_rate": 3.0343750000000006e-05,
      "loss": 0.3159,
      "step": 1030
    },
    {
      "bce_eff_w": 0.29465,
      "bce_loss": -0.46462222933769226,
      "epoch": 5.15,
      "step": 1030
    },
    {
      "epoch": 5.155,
      "grad_norm": 17.95937728881836,
      "learning_rate": 3.0312499999999998e-05,
      "loss": 1.2666,
      "step": 1031
    },
    {
      "bce_eff_w": 0.2948,
      "bce_loss": -0.46328070759773254,
      "epoch": 5.155,
      "step": 1031
    },
    {
      "epoch": 5.16,
      "grad_norm": 14.851941108703613,
      "learning_rate": 3.028125e-05,
      "loss": 0.3308,
      "step": 1032
    },
    {
      "bce_eff_w": 0.29495,
      "bce_loss": -0.4628634452819824,
      "epoch": 5.16,
      "step": 1032
    },
    {
      "epoch": 5.165,
      "grad_norm": 22.308443069458008,
      "learning_rate": 3.025e-05,
      "loss": 1.3552,
      "step": 1033
    },
    {
      "bce_eff_w": 0.29510000000000003,
      "bce_loss": -0.46103283762931824,
      "epoch": 5.165,
      "step": 1033
    },
    {
      "epoch": 5.17,
      "grad_norm": 21.845043182373047,
      "learning_rate": 3.0218750000000003e-05,
      "loss": 1.8964,
      "step": 1034
    },
    {
      "bce_eff_w": 0.29525,
      "bce_loss": -0.46411240100860596,
      "epoch": 5.17,
      "step": 1034
    },
    {
      "epoch": 5.175,
      "grad_norm": 25.921384811401367,
      "learning_rate": 3.0187500000000002e-05,
      "loss": 1.2366,
      "step": 1035
    },
    {
      "bce_eff_w": 0.2954,
      "bce_loss": -0.46014609932899475,
      "epoch": 5.175,
      "step": 1035
    },
    {
      "epoch": 5.18,
      "grad_norm": 20.005332946777344,
      "learning_rate": 3.015625e-05,
      "loss": 0.2421,
      "step": 1036
    },
    {
      "bce_eff_w": 0.29555,
      "bce_loss": -0.4647059738636017,
      "epoch": 5.18,
      "step": 1036
    },
    {
      "epoch": 5.185,
      "grad_norm": 18.12390899658203,
      "learning_rate": 3.0125000000000004e-05,
      "loss": 0.3775,
      "step": 1037
    },
    {
      "bce_eff_w": 0.2957,
      "bce_loss": -0.46090033650398254,
      "epoch": 5.185,
      "step": 1037
    },
    {
      "epoch": 5.19,
      "grad_norm": 13.698630332946777,
      "learning_rate": 3.0093750000000003e-05,
      "loss": 3.0931,
      "step": 1038
    },
    {
      "bce_eff_w": 0.29585,
      "bce_loss": -0.4591868221759796,
      "epoch": 5.19,
      "step": 1038
    },
    {
      "epoch": 5.195,
      "grad_norm": 16.712247848510742,
      "learning_rate": 3.00625e-05,
      "loss": 3.2852,
      "step": 1039
    },
    {
      "bce_eff_w": 0.29600000000000004,
      "bce_loss": -0.4578368365764618,
      "epoch": 5.195,
      "step": 1039
    },
    {
      "epoch": 5.2,
      "grad_norm": 25.261091232299805,
      "learning_rate": 3.0031249999999998e-05,
      "loss": 2.2692,
      "step": 1040
    },
    {
      "bce_eff_w": 0.29615,
      "bce_loss": -0.46153324842453003,
      "epoch": 5.2,
      "step": 1040
    },
    {
      "epoch": 5.205,
      "grad_norm": 19.369903564453125,
      "learning_rate": 3e-05,
      "loss": 1.4586,
      "step": 1041
    },
    {
      "bce_eff_w": 0.2963,
      "bce_loss": -0.46128201484680176,
      "epoch": 5.205,
      "step": 1041
    },
    {
      "epoch": 5.21,
      "grad_norm": 27.655248641967773,
      "learning_rate": 2.996875e-05,
      "loss": 0.8616,
      "step": 1042
    },
    {
      "bce_eff_w": 0.29645,
      "bce_loss": -0.459354043006897,
      "epoch": 5.21,
      "step": 1042
    },
    {
      "epoch": 5.215,
      "grad_norm": 25.711040496826172,
      "learning_rate": 2.9937500000000003e-05,
      "loss": 1.037,
      "step": 1043
    },
    {
      "bce_eff_w": 0.29660000000000003,
      "bce_loss": -0.4647688865661621,
      "epoch": 5.215,
      "step": 1043
    },
    {
      "epoch": 5.22,
      "grad_norm": 14.72752571105957,
      "learning_rate": 2.9906250000000002e-05,
      "loss": 2.0215,
      "step": 1044
    },
    {
      "bce_eff_w": 0.29675,
      "bce_loss": -0.4633720815181732,
      "epoch": 5.22,
      "step": 1044
    },
    {
      "epoch": 5.225,
      "grad_norm": 20.848257064819336,
      "learning_rate": 2.9875000000000004e-05,
      "loss": 0.8076,
      "step": 1045
    },
    {
      "bce_eff_w": 0.2969,
      "bce_loss": -0.4592236876487732,
      "epoch": 5.225,
      "step": 1045
    },
    {
      "epoch": 5.23,
      "grad_norm": 15.354975700378418,
      "learning_rate": 2.9843750000000004e-05,
      "loss": 0.1988,
      "step": 1046
    },
    {
      "bce_eff_w": 0.29705000000000004,
      "bce_loss": -0.4608989655971527,
      "epoch": 5.23,
      "step": 1046
    },
    {
      "epoch": 5.235,
      "grad_norm": 23.678810119628906,
      "learning_rate": 2.98125e-05,
      "loss": 1.6814,
      "step": 1047
    },
    {
      "bce_eff_w": 0.2972,
      "bce_loss": -0.4632042944431305,
      "epoch": 5.235,
      "step": 1047
    },
    {
      "epoch": 5.24,
      "grad_norm": 14.135326385498047,
      "learning_rate": 2.978125e-05,
      "loss": 1.6543,
      "step": 1048
    },
    {
      "bce_eff_w": 0.29735,
      "bce_loss": -0.4627912938594818,
      "epoch": 5.24,
      "step": 1048
    },
    {
      "epoch": 5.245,
      "grad_norm": 26.38257598876953,
      "learning_rate": 2.975e-05,
      "loss": 0.5562,
      "step": 1049
    },
    {
      "bce_eff_w": 0.2975,
      "bce_loss": -0.4625156819820404,
      "epoch": 5.245,
      "step": 1049
    },
    {
      "epoch": 5.25,
      "grad_norm": 26.864891052246094,
      "learning_rate": 2.971875e-05,
      "loss": 1.4574,
      "step": 1050
    },
    {
      "bce_eff_w": 0.29765,
      "bce_loss": -0.4514230191707611,
      "epoch": 5.25,
      "step": 1050
    },
    {
      "epoch": 5.255,
      "grad_norm": 14.91312313079834,
      "learning_rate": 2.96875e-05,
      "loss": 0.3136,
      "step": 1051
    },
    {
      "bce_eff_w": 0.2978,
      "bce_loss": -0.4513437747955322,
      "epoch": 5.255,
      "step": 1051
    },
    {
      "epoch": 5.26,
      "grad_norm": 10.157925605773926,
      "learning_rate": 2.9656250000000003e-05,
      "loss": 0.2248,
      "step": 1052
    },
    {
      "bce_eff_w": 0.29795,
      "bce_loss": -0.4604770839214325,
      "epoch": 5.26,
      "step": 1052
    },
    {
      "epoch": 5.265,
      "grad_norm": 15.854205131530762,
      "learning_rate": 2.9625000000000002e-05,
      "loss": 1.3689,
      "step": 1053
    },
    {
      "bce_eff_w": 0.29810000000000003,
      "bce_loss": -0.4626235365867615,
      "epoch": 5.265,
      "step": 1053
    },
    {
      "epoch": 5.27,
      "grad_norm": 17.325111389160156,
      "learning_rate": 2.9593750000000004e-05,
      "loss": 0.6701,
      "step": 1054
    },
    {
      "bce_eff_w": 0.29825,
      "bce_loss": -0.46110472083091736,
      "epoch": 5.27,
      "step": 1054
    },
    {
      "epoch": 5.275,
      "grad_norm": 27.946352005004883,
      "learning_rate": 2.9562500000000004e-05,
      "loss": 0.7226,
      "step": 1055
    },
    {
      "bce_eff_w": 0.2984,
      "bce_loss": -0.4621909260749817,
      "epoch": 5.275,
      "step": 1055
    },
    {
      "epoch": 5.28,
      "grad_norm": 11.357888221740723,
      "learning_rate": 2.953125e-05,
      "loss": 0.1008,
      "step": 1056
    },
    {
      "bce_eff_w": 0.29855,
      "bce_loss": -0.455661416053772,
      "epoch": 5.28,
      "step": 1056
    },
    {
      "epoch": 5.285,
      "grad_norm": 18.549785614013672,
      "learning_rate": 2.95e-05,
      "loss": 0.7234,
      "step": 1057
    },
    {
      "bce_eff_w": 0.2987,
      "bce_loss": -0.45766061544418335,
      "epoch": 5.285,
      "step": 1057
    },
    {
      "epoch": 5.29,
      "grad_norm": 13.750754356384277,
      "learning_rate": 2.946875e-05,
      "loss": 0.1011,
      "step": 1058
    },
    {
      "bce_eff_w": 0.29885,
      "bce_loss": -0.46293941140174866,
      "epoch": 5.29,
      "step": 1058
    },
    {
      "epoch": 5.295,
      "grad_norm": 18.838804244995117,
      "learning_rate": 2.94375e-05,
      "loss": 0.0969,
      "step": 1059
    },
    {
      "bce_eff_w": 0.29900000000000004,
      "bce_loss": -0.46168196201324463,
      "epoch": 5.295,
      "step": 1059
    },
    {
      "epoch": 5.3,
      "grad_norm": 23.75558853149414,
      "learning_rate": 2.9406250000000003e-05,
      "loss": 0.5817,
      "step": 1060
    },
    {
      "bce_eff_w": 0.29915,
      "bce_loss": -0.46435362100601196,
      "epoch": 5.3,
      "step": 1060
    },
    {
      "epoch": 5.305,
      "grad_norm": 15.551460266113281,
      "learning_rate": 2.9375000000000003e-05,
      "loss": 0.273,
      "step": 1061
    },
    {
      "bce_eff_w": 0.2993,
      "bce_loss": -0.4546089768409729,
      "epoch": 5.305,
      "step": 1061
    },
    {
      "epoch": 5.31,
      "grad_norm": 32.30644607543945,
      "learning_rate": 2.9343750000000002e-05,
      "loss": 1.4333,
      "step": 1062
    },
    {
      "bce_eff_w": 0.29945,
      "bce_loss": -0.46112513542175293,
      "epoch": 5.31,
      "step": 1062
    },
    {
      "epoch": 5.315,
      "grad_norm": 23.8591365814209,
      "learning_rate": 2.9312500000000004e-05,
      "loss": 0.633,
      "step": 1063
    },
    {
      "bce_eff_w": 0.29960000000000003,
      "bce_loss": -0.46271270513534546,
      "epoch": 5.315,
      "step": 1063
    },
    {
      "epoch": 5.32,
      "grad_norm": 20.780364990234375,
      "learning_rate": 2.928125e-05,
      "loss": 0.7368,
      "step": 1064
    },
    {
      "bce_eff_w": 0.29975,
      "bce_loss": -0.46205803751945496,
      "epoch": 5.32,
      "step": 1064
    },
    {
      "epoch": 5.325,
      "grad_norm": 17.093128204345703,
      "learning_rate": 2.925e-05,
      "loss": 1.4476,
      "step": 1065
    },
    {
      "bce_eff_w": 0.2999,
      "bce_loss": -0.46333131194114685,
      "epoch": 5.325,
      "step": 1065
    },
    {
      "epoch": 5.33,
      "grad_norm": 25.473407745361328,
      "learning_rate": 2.921875e-05,
      "loss": 2.293,
      "step": 1066
    },
    {
      "bce_eff_w": 0.30005000000000004,
      "bce_loss": -0.45898452401161194,
      "epoch": 5.33,
      "step": 1066
    },
    {
      "epoch": 5.335,
      "grad_norm": 16.11993408203125,
      "learning_rate": 2.91875e-05,
      "loss": 0.7378,
      "step": 1067
    },
    {
      "bce_eff_w": 0.3002,
      "bce_loss": -0.462713360786438,
      "epoch": 5.335,
      "step": 1067
    },
    {
      "epoch": 5.34,
      "grad_norm": 14.972058296203613,
      "learning_rate": 2.915625e-05,
      "loss": 0.1482,
      "step": 1068
    },
    {
      "bce_eff_w": 0.30035,
      "bce_loss": -0.46311962604522705,
      "epoch": 5.34,
      "step": 1068
    },
    {
      "epoch": 5.345,
      "grad_norm": 17.41226577758789,
      "learning_rate": 2.9125000000000003e-05,
      "loss": 0.6809,
      "step": 1069
    },
    {
      "bce_eff_w": 0.3005,
      "bce_loss": -0.45348191261291504,
      "epoch": 5.345,
      "step": 1069
    },
    {
      "epoch": 5.35,
      "grad_norm": 14.4125394821167,
      "learning_rate": 2.9093750000000002e-05,
      "loss": 0.153,
      "step": 1070
    },
    {
      "bce_eff_w": 0.30065000000000003,
      "bce_loss": -0.46332409977912903,
      "epoch": 5.35,
      "step": 1070
    },
    {
      "epoch": 5.355,
      "grad_norm": 19.004112243652344,
      "learning_rate": 2.9062500000000005e-05,
      "loss": 2.5728,
      "step": 1071
    },
    {
      "bce_eff_w": 0.3008,
      "bce_loss": -0.4559310972690582,
      "epoch": 5.355,
      "step": 1071
    },
    {
      "epoch": 5.36,
      "grad_norm": 28.9862060546875,
      "learning_rate": 2.9031249999999998e-05,
      "loss": 1.1293,
      "step": 1072
    },
    {
      "bce_eff_w": 0.30095,
      "bce_loss": -0.460816890001297,
      "epoch": 5.36,
      "step": 1072
    },
    {
      "epoch": 5.365,
      "grad_norm": 23.882465362548828,
      "learning_rate": 2.9e-05,
      "loss": 1.3325,
      "step": 1073
    },
    {
      "bce_eff_w": 0.30110000000000003,
      "bce_loss": -0.46257027983665466,
      "epoch": 5.365,
      "step": 1073
    },
    {
      "epoch": 5.37,
      "grad_norm": 27.777904510498047,
      "learning_rate": 2.896875e-05,
      "loss": 0.2034,
      "step": 1074
    },
    {
      "bce_eff_w": 0.30125,
      "bce_loss": -0.4638940393924713,
      "epoch": 5.37,
      "step": 1074
    },
    {
      "epoch": 5.375,
      "grad_norm": 22.4091739654541,
      "learning_rate": 2.8937500000000002e-05,
      "loss": 0.7683,
      "step": 1075
    },
    {
      "bce_eff_w": 0.3014,
      "bce_loss": -0.46161016821861267,
      "epoch": 5.375,
      "step": 1075
    },
    {
      "epoch": 5.38,
      "grad_norm": 10.760051727294922,
      "learning_rate": 2.890625e-05,
      "loss": 0.1488,
      "step": 1076
    },
    {
      "bce_eff_w": 0.30155,
      "bce_loss": -0.460248202085495,
      "epoch": 5.38,
      "step": 1076
    },
    {
      "epoch": 5.385,
      "grad_norm": 15.852547645568848,
      "learning_rate": 2.8875e-05,
      "loss": 2.8238,
      "step": 1077
    },
    {
      "bce_eff_w": 0.3017,
      "bce_loss": -0.4611031413078308,
      "epoch": 5.385,
      "step": 1077
    },
    {
      "epoch": 5.39,
      "grad_norm": 37.24437713623047,
      "learning_rate": 2.8843750000000003e-05,
      "loss": 1.5751,
      "step": 1078
    },
    {
      "bce_eff_w": 0.30185,
      "bce_loss": -0.46098169684410095,
      "epoch": 5.39,
      "step": 1078
    },
    {
      "epoch": 5.395,
      "grad_norm": 22.135061264038086,
      "learning_rate": 2.8812500000000002e-05,
      "loss": 2.4123,
      "step": 1079
    },
    {
      "bce_eff_w": 0.30200000000000005,
      "bce_loss": -0.45992594957351685,
      "epoch": 5.395,
      "step": 1079
    },
    {
      "epoch": 5.4,
      "grad_norm": 34.366065979003906,
      "learning_rate": 2.8781250000000005e-05,
      "loss": 1.3641,
      "step": 1080
    },
    {
      "bce_eff_w": 0.30215000000000003,
      "bce_loss": -0.46020904183387756,
      "epoch": 5.4,
      "step": 1080
    },
    {
      "epoch": 5.405,
      "grad_norm": 17.05373191833496,
      "learning_rate": 2.8749999999999997e-05,
      "loss": 0.3484,
      "step": 1081
    },
    {
      "bce_eff_w": 0.3023,
      "bce_loss": -0.46278688311576843,
      "epoch": 5.405,
      "step": 1081
    },
    {
      "epoch": 5.41,
      "grad_norm": 31.487258911132812,
      "learning_rate": 2.871875e-05,
      "loss": 1.0593,
      "step": 1082
    },
    {
      "bce_eff_w": 0.30245,
      "bce_loss": -0.4636821746826172,
      "epoch": 5.41,
      "step": 1082
    },
    {
      "epoch": 5.415,
      "grad_norm": 23.708072662353516,
      "learning_rate": 2.86875e-05,
      "loss": 1.1218,
      "step": 1083
    },
    {
      "bce_eff_w": 0.30260000000000004,
      "bce_loss": -0.4630430340766907,
      "epoch": 5.415,
      "step": 1083
    },
    {
      "epoch": 5.42,
      "grad_norm": 22.078262329101562,
      "learning_rate": 2.8656250000000002e-05,
      "loss": 0.6361,
      "step": 1084
    },
    {
      "bce_eff_w": 0.30275,
      "bce_loss": -0.46122562885284424,
      "epoch": 5.42,
      "step": 1084
    },
    {
      "epoch": 5.425,
      "grad_norm": 20.056293487548828,
      "learning_rate": 2.8625e-05,
      "loss": 2.069,
      "step": 1085
    },
    {
      "bce_eff_w": 0.3029,
      "bce_loss": -0.4640791416168213,
      "epoch": 5.425,
      "step": 1085
    },
    {
      "epoch": 5.43,
      "grad_norm": 16.77267837524414,
      "learning_rate": 2.8593750000000004e-05,
      "loss": 0.475,
      "step": 1086
    },
    {
      "bce_eff_w": 0.30305000000000004,
      "bce_loss": -0.45856595039367676,
      "epoch": 5.43,
      "step": 1086
    },
    {
      "epoch": 5.435,
      "grad_norm": 20.570302963256836,
      "learning_rate": 2.8562500000000003e-05,
      "loss": 1.6068,
      "step": 1087
    },
    {
      "bce_eff_w": 0.3032,
      "bce_loss": -0.4637538194656372,
      "epoch": 5.435,
      "step": 1087
    },
    {
      "epoch": 5.44,
      "grad_norm": 16.71854591369629,
      "learning_rate": 2.8531250000000002e-05,
      "loss": 0.1311,
      "step": 1088
    },
    {
      "bce_eff_w": 0.30335,
      "bce_loss": -0.4632037281990051,
      "epoch": 5.44,
      "step": 1088
    },
    {
      "epoch": 5.445,
      "grad_norm": 30.022932052612305,
      "learning_rate": 2.8499999999999998e-05,
      "loss": 2.0705,
      "step": 1089
    },
    {
      "bce_eff_w": 0.3035,
      "bce_loss": -0.46360209584236145,
      "epoch": 5.445,
      "step": 1089
    },
    {
      "epoch": 5.45,
      "grad_norm": 29.022174835205078,
      "learning_rate": 2.846875e-05,
      "loss": 0.9824,
      "step": 1090
    },
    {
      "bce_eff_w": 0.30365,
      "bce_loss": -0.46078068017959595,
      "epoch": 5.45,
      "step": 1090
    },
    {
      "epoch": 5.455,
      "grad_norm": 27.38866424560547,
      "learning_rate": 2.84375e-05,
      "loss": 0.7716,
      "step": 1091
    },
    {
      "bce_eff_w": 0.3038,
      "bce_loss": -0.46417000889778137,
      "epoch": 5.455,
      "step": 1091
    },
    {
      "epoch": 5.46,
      "grad_norm": 21.51116371154785,
      "learning_rate": 2.840625e-05,
      "loss": 0.3931,
      "step": 1092
    },
    {
      "bce_eff_w": 0.30395,
      "bce_loss": -0.45663246512413025,
      "epoch": 5.46,
      "step": 1092
    },
    {
      "epoch": 5.465,
      "grad_norm": 17.713438034057617,
      "learning_rate": 2.8375000000000002e-05,
      "loss": 1.0436,
      "step": 1093
    },
    {
      "bce_eff_w": 0.3041,
      "bce_loss": -0.45533469319343567,
      "epoch": 5.465,
      "step": 1093
    },
    {
      "epoch": 5.47,
      "grad_norm": 13.914803504943848,
      "learning_rate": 2.834375e-05,
      "loss": 3.0242,
      "step": 1094
    },
    {
      "bce_eff_w": 0.30425,
      "bce_loss": -0.4635801613330841,
      "epoch": 5.47,
      "step": 1094
    },
    {
      "epoch": 5.475,
      "grad_norm": 22.654808044433594,
      "learning_rate": 2.8312500000000004e-05,
      "loss": 0.4217,
      "step": 1095
    },
    {
      "bce_eff_w": 0.3044,
      "bce_loss": -0.46320509910583496,
      "epoch": 5.475,
      "step": 1095
    },
    {
      "epoch": 5.48,
      "grad_norm": 20.762592315673828,
      "learning_rate": 2.8281250000000003e-05,
      "loss": 1.7724,
      "step": 1096
    },
    {
      "bce_eff_w": 0.30455,
      "bce_loss": -0.4603954255580902,
      "epoch": 5.48,
      "step": 1096
    },
    {
      "epoch": 5.485,
      "grad_norm": 21.815576553344727,
      "learning_rate": 2.825e-05,
      "loss": 0.4891,
      "step": 1097
    },
    {
      "bce_eff_w": 0.30469999999999997,
      "bce_loss": -0.46476903557777405,
      "epoch": 5.485,
      "step": 1097
    },
    {
      "epoch": 5.49,
      "grad_norm": 39.48088073730469,
      "learning_rate": 2.8218749999999998e-05,
      "loss": 1.0031,
      "step": 1098
    },
    {
      "bce_eff_w": 0.30485,
      "bce_loss": -0.4609759747982025,
      "epoch": 5.49,
      "step": 1098
    },
    {
      "epoch": 5.495,
      "grad_norm": 34.39228439331055,
      "learning_rate": 2.81875e-05,
      "loss": 1.6259,
      "step": 1099
    },
    {
      "bce_eff_w": 0.305,
      "bce_loss": -0.46309736371040344,
      "epoch": 5.495,
      "step": 1099
    },
    {
      "epoch": 5.5,
      "grad_norm": 11.974906921386719,
      "learning_rate": 2.815625e-05,
      "loss": 0.0583,
      "step": 1100
    },
    {
      "bce_eff_w": 0.30515000000000003,
      "bce_loss": -0.4594033360481262,
      "epoch": 5.5,
      "step": 1100
    },
    {
      "epoch": 5.505,
      "grad_norm": 28.667144775390625,
      "learning_rate": 2.8125000000000003e-05,
      "loss": 0.8196,
      "step": 1101
    },
    {
      "bce_eff_w": 0.3053,
      "bce_loss": -0.4571191966533661,
      "epoch": 5.505,
      "step": 1101
    },
    {
      "epoch": 5.51,
      "grad_norm": 33.44983673095703,
      "learning_rate": 2.8093750000000002e-05,
      "loss": 1.9548,
      "step": 1102
    },
    {
      "bce_eff_w": 0.30545,
      "bce_loss": -0.46327754855155945,
      "epoch": 5.51,
      "step": 1102
    },
    {
      "epoch": 5.515,
      "grad_norm": 11.237430572509766,
      "learning_rate": 2.80625e-05,
      "loss": 0.0693,
      "step": 1103
    },
    {
      "bce_eff_w": 0.3056,
      "bce_loss": -0.4628119170665741,
      "epoch": 5.515,
      "step": 1103
    },
    {
      "epoch": 5.52,
      "grad_norm": 14.973567008972168,
      "learning_rate": 2.8031250000000004e-05,
      "loss": 2.6912,
      "step": 1104
    },
    {
      "bce_eff_w": 0.30575,
      "bce_loss": -0.46419772505760193,
      "epoch": 5.52,
      "step": 1104
    },
    {
      "epoch": 5.525,
      "grad_norm": 13.582584381103516,
      "learning_rate": 2.8000000000000003e-05,
      "loss": 2.0335,
      "step": 1105
    },
    {
      "bce_eff_w": 0.3059,
      "bce_loss": -0.4555312991142273,
      "epoch": 5.525,
      "step": 1105
    },
    {
      "epoch": 5.53,
      "grad_norm": 27.028085708618164,
      "learning_rate": 2.796875e-05,
      "loss": 1.5234,
      "step": 1106
    },
    {
      "bce_eff_w": 0.30605,
      "bce_loss": -0.4569351375102997,
      "epoch": 5.53,
      "step": 1106
    },
    {
      "epoch": 5.535,
      "grad_norm": 34.26734924316406,
      "learning_rate": 2.79375e-05,
      "loss": 0.5344,
      "step": 1107
    },
    {
      "bce_eff_w": 0.3062,
      "bce_loss": -0.46457213163375854,
      "epoch": 5.535,
      "step": 1107
    },
    {
      "epoch": 5.54,
      "grad_norm": 16.361846923828125,
      "learning_rate": 2.790625e-05,
      "loss": 1.599,
      "step": 1108
    },
    {
      "bce_eff_w": 0.30635,
      "bce_loss": -0.46269649267196655,
      "epoch": 5.54,
      "step": 1108
    },
    {
      "epoch": 5.545,
      "grad_norm": 23.75588035583496,
      "learning_rate": 2.7875e-05,
      "loss": 1.2805,
      "step": 1109
    },
    {
      "bce_eff_w": 0.3065,
      "bce_loss": -0.46393850445747375,
      "epoch": 5.545,
      "step": 1109
    },
    {
      "epoch": 5.55,
      "grad_norm": 20.041351318359375,
      "learning_rate": 2.7843750000000003e-05,
      "loss": 1.8635,
      "step": 1110
    },
    {
      "bce_eff_w": 0.30665,
      "bce_loss": -0.46197548508644104,
      "epoch": 5.55,
      "step": 1110
    },
    {
      "epoch": 5.555,
      "grad_norm": 21.559307098388672,
      "learning_rate": 2.7812500000000002e-05,
      "loss": 0.8967,
      "step": 1111
    },
    {
      "bce_eff_w": 0.3068,
      "bce_loss": -0.4641740322113037,
      "epoch": 5.555,
      "step": 1111
    },
    {
      "epoch": 5.5600000000000005,
      "grad_norm": 25.429187774658203,
      "learning_rate": 2.7781250000000004e-05,
      "loss": 0.6867,
      "step": 1112
    },
    {
      "bce_eff_w": 0.30695,
      "bce_loss": -0.4640520513057709,
      "epoch": 5.5600000000000005,
      "step": 1112
    },
    {
      "epoch": 5.5649999999999995,
      "grad_norm": 31.6406192779541,
      "learning_rate": 2.7750000000000004e-05,
      "loss": 0.8661,
      "step": 1113
    },
    {
      "bce_eff_w": 0.3071,
      "bce_loss": -0.46351170539855957,
      "epoch": 5.5649999999999995,
      "step": 1113
    },
    {
      "epoch": 5.57,
      "grad_norm": 22.446868896484375,
      "learning_rate": 2.771875e-05,
      "loss": 0.3956,
      "step": 1114
    },
    {
      "bce_eff_w": 0.30725,
      "bce_loss": -0.46115732192993164,
      "epoch": 5.57,
      "step": 1114
    },
    {
      "epoch": 5.575,
      "grad_norm": 11.246606826782227,
      "learning_rate": 2.76875e-05,
      "loss": 0.0257,
      "step": 1115
    },
    {
      "bce_eff_w": 0.3074,
      "bce_loss": -0.46172794699668884,
      "epoch": 5.575,
      "step": 1115
    },
    {
      "epoch": 5.58,
      "grad_norm": 22.704835891723633,
      "learning_rate": 2.765625e-05,
      "loss": 1.352,
      "step": 1116
    },
    {
      "bce_eff_w": 0.30755,
      "bce_loss": -0.4609786868095398,
      "epoch": 5.58,
      "step": 1116
    },
    {
      "epoch": 5.585,
      "grad_norm": 22.92591094970703,
      "learning_rate": 2.7625e-05,
      "loss": 0.5489,
      "step": 1117
    },
    {
      "bce_eff_w": 0.3077,
      "bce_loss": -0.4635563790798187,
      "epoch": 5.585,
      "step": 1117
    },
    {
      "epoch": 5.59,
      "grad_norm": 18.26894760131836,
      "learning_rate": 2.759375e-05,
      "loss": 0.9356,
      "step": 1118
    },
    {
      "bce_eff_w": 0.30785,
      "bce_loss": -0.4624820053577423,
      "epoch": 5.59,
      "step": 1118
    },
    {
      "epoch": 5.595,
      "grad_norm": 22.612712860107422,
      "learning_rate": 2.7562500000000002e-05,
      "loss": 1.1681,
      "step": 1119
    },
    {
      "bce_eff_w": 0.308,
      "bce_loss": -0.46041160821914673,
      "epoch": 5.595,
      "step": 1119
    },
    {
      "epoch": 5.6,
      "grad_norm": 8.501351356506348,
      "learning_rate": 2.7531250000000002e-05,
      "loss": -0.0397,
      "step": 1120
    },
    {
      "bce_eff_w": 0.30815000000000003,
      "bce_loss": -0.46427541971206665,
      "epoch": 5.6,
      "step": 1120
    },
    {
      "epoch": 5.605,
      "grad_norm": 25.057600021362305,
      "learning_rate": 2.7500000000000004e-05,
      "loss": 0.2418,
      "step": 1121
    },
    {
      "bce_eff_w": 0.3083,
      "bce_loss": -0.4587884545326233,
      "epoch": 5.605,
      "step": 1121
    },
    {
      "epoch": 5.61,
      "grad_norm": 30.242963790893555,
      "learning_rate": 2.746875e-05,
      "loss": 1.1016,
      "step": 1122
    },
    {
      "bce_eff_w": 0.30845,
      "bce_loss": -0.4554414451122284,
      "epoch": 5.61,
      "step": 1122
    },
    {
      "epoch": 5.615,
      "grad_norm": 17.49446678161621,
      "learning_rate": 2.74375e-05,
      "loss": 0.7122,
      "step": 1123
    },
    {
      "bce_eff_w": 0.3086,
      "bce_loss": -0.4525754749774933,
      "epoch": 5.615,
      "step": 1123
    },
    {
      "epoch": 5.62,
      "grad_norm": 10.275832176208496,
      "learning_rate": 2.740625e-05,
      "loss": 0.2365,
      "step": 1124
    },
    {
      "bce_eff_w": 0.30875,
      "bce_loss": -0.45422011613845825,
      "epoch": 5.62,
      "step": 1124
    },
    {
      "epoch": 5.625,
      "grad_norm": 21.78944969177246,
      "learning_rate": 2.7375e-05,
      "loss": 2.0414,
      "step": 1125
    },
    {
      "bce_eff_w": 0.3089,
      "bce_loss": -0.46291038393974304,
      "epoch": 5.625,
      "step": 1125
    },
    {
      "epoch": 5.63,
      "grad_norm": 5.582113742828369,
      "learning_rate": 2.734375e-05,
      "loss": -0.0427,
      "step": 1126
    },
    {
      "bce_eff_w": 0.30905,
      "bce_loss": -0.46034711599349976,
      "epoch": 5.63,
      "step": 1126
    },
    {
      "epoch": 5.635,
      "grad_norm": 39.5920524597168,
      "learning_rate": 2.7312500000000003e-05,
      "loss": 0.9843,
      "step": 1127
    },
    {
      "bce_eff_w": 0.30920000000000003,
      "bce_loss": -0.46011441946029663,
      "epoch": 5.635,
      "step": 1127
    },
    {
      "epoch": 5.64,
      "grad_norm": 16.978633880615234,
      "learning_rate": 2.7281250000000002e-05,
      "loss": 0.1685,
      "step": 1128
    },
    {
      "bce_eff_w": 0.30935,
      "bce_loss": -0.4630180299282074,
      "epoch": 5.64,
      "step": 1128
    },
    {
      "epoch": 5.645,
      "grad_norm": 26.64903450012207,
      "learning_rate": 2.725e-05,
      "loss": 0.0691,
      "step": 1129
    },
    {
      "bce_eff_w": 0.3095,
      "bce_loss": -0.4645494818687439,
      "epoch": 5.645,
      "step": 1129
    },
    {
      "epoch": 5.65,
      "grad_norm": 7.757429599761963,
      "learning_rate": 2.7218750000000004e-05,
      "loss": -0.0341,
      "step": 1130
    },
    {
      "bce_eff_w": 0.30965,
      "bce_loss": -0.4609355032444,
      "epoch": 5.65,
      "step": 1130
    },
    {
      "epoch": 5.655,
      "grad_norm": 20.586774826049805,
      "learning_rate": 2.71875e-05,
      "loss": 0.9388,
      "step": 1131
    },
    {
      "bce_eff_w": 0.3098,
      "bce_loss": -0.45728686451911926,
      "epoch": 5.655,
      "step": 1131
    },
    {
      "epoch": 5.66,
      "grad_norm": 51.32563781738281,
      "learning_rate": 2.715625e-05,
      "loss": 1.3204,
      "step": 1132
    },
    {
      "bce_eff_w": 0.30995,
      "bce_loss": -0.45754027366638184,
      "epoch": 5.66,
      "step": 1132
    },
    {
      "epoch": 5.665,
      "grad_norm": 49.8332633972168,
      "learning_rate": 2.7125000000000002e-05,
      "loss": 0.5931,
      "step": 1133
    },
    {
      "bce_eff_w": 0.3101,
      "bce_loss": -0.463358998298645,
      "epoch": 5.665,
      "step": 1133
    },
    {
      "epoch": 5.67,
      "grad_norm": 19.160120010375977,
      "learning_rate": 2.709375e-05,
      "loss": 1.1767,
      "step": 1134
    },
    {
      "bce_eff_w": 0.31025,
      "bce_loss": -0.46215254068374634,
      "epoch": 5.67,
      "step": 1134
    },
    {
      "epoch": 5.675,
      "grad_norm": 13.406264305114746,
      "learning_rate": 2.70625e-05,
      "loss": 1.5456,
      "step": 1135
    },
    {
      "bce_eff_w": 0.3104,
      "bce_loss": -0.4603475332260132,
      "epoch": 5.675,
      "step": 1135
    },
    {
      "epoch": 5.68,
      "grad_norm": 11.563220977783203,
      "learning_rate": 2.7031250000000003e-05,
      "loss": 0.0342,
      "step": 1136
    },
    {
      "bce_eff_w": 0.31055,
      "bce_loss": -0.45301297307014465,
      "epoch": 5.68,
      "step": 1136
    },
    {
      "epoch": 5.6850000000000005,
      "grad_norm": 13.14977741241455,
      "learning_rate": 2.7000000000000002e-05,
      "loss": 0.3349,
      "step": 1137
    },
    {
      "bce_eff_w": 0.3107,
      "bce_loss": -0.46325698494911194,
      "epoch": 5.6850000000000005,
      "step": 1137
    },
    {
      "epoch": 5.6899999999999995,
      "grad_norm": 24.146921157836914,
      "learning_rate": 2.6968750000000005e-05,
      "loss": 1.4471,
      "step": 1138
    },
    {
      "bce_eff_w": 0.31085,
      "bce_loss": -0.46235817670822144,
      "epoch": 5.6899999999999995,
      "step": 1138
    },
    {
      "epoch": 5.695,
      "grad_norm": 17.163768768310547,
      "learning_rate": 2.6937499999999997e-05,
      "loss": 3.2534,
      "step": 1139
    },
    {
      "bce_eff_w": 0.311,
      "bce_loss": -0.46107131242752075,
      "epoch": 5.695,
      "step": 1139
    },
    {
      "epoch": 5.7,
      "grad_norm": 19.47953224182129,
      "learning_rate": 2.690625e-05,
      "loss": 1.1424,
      "step": 1140
    },
    {
      "bce_eff_w": 0.31115000000000004,
      "bce_loss": -0.46393322944641113,
      "epoch": 5.7,
      "step": 1140
    },
    {
      "epoch": 5.705,
      "grad_norm": 16.936246871948242,
      "learning_rate": 2.6875e-05,
      "loss": 1.0874,
      "step": 1141
    },
    {
      "bce_eff_w": 0.3113,
      "bce_loss": -0.4585276246070862,
      "epoch": 5.705,
      "step": 1141
    },
    {
      "epoch": 5.71,
      "grad_norm": 26.653507232666016,
      "learning_rate": 2.6843750000000002e-05,
      "loss": 2.2935,
      "step": 1142
    },
    {
      "bce_eff_w": 0.31145,
      "bce_loss": -0.461887001991272,
      "epoch": 5.71,
      "step": 1142
    },
    {
      "epoch": 5.715,
      "grad_norm": 24.121479034423828,
      "learning_rate": 2.68125e-05,
      "loss": 1.1283,
      "step": 1143
    },
    {
      "bce_eff_w": 0.3116,
      "bce_loss": -0.4585367441177368,
      "epoch": 5.715,
      "step": 1143
    },
    {
      "epoch": 5.72,
      "grad_norm": 14.682851791381836,
      "learning_rate": 2.6781250000000004e-05,
      "loss": 2.8269,
      "step": 1144
    },
    {
      "bce_eff_w": 0.31175,
      "bce_loss": -0.4636043608188629,
      "epoch": 5.72,
      "step": 1144
    },
    {
      "epoch": 5.725,
      "grad_norm": 20.45460319519043,
      "learning_rate": 2.6750000000000003e-05,
      "loss": 0.4165,
      "step": 1145
    },
    {
      "bce_eff_w": 0.3119,
      "bce_loss": -0.4601573050022125,
      "epoch": 5.725,
      "step": 1145
    },
    {
      "epoch": 5.73,
      "grad_norm": 31.423236846923828,
      "learning_rate": 2.6718750000000002e-05,
      "loss": 0.7184,
      "step": 1146
    },
    {
      "bce_eff_w": 0.31205,
      "bce_loss": -0.45920225977897644,
      "epoch": 5.73,
      "step": 1146
    },
    {
      "epoch": 5.735,
      "grad_norm": 19.096071243286133,
      "learning_rate": 2.6687499999999998e-05,
      "loss": 0.9191,
      "step": 1147
    },
    {
      "bce_eff_w": 0.31220000000000003,
      "bce_loss": -0.46111828088760376,
      "epoch": 5.735,
      "step": 1147
    },
    {
      "epoch": 5.74,
      "grad_norm": 24.036794662475586,
      "learning_rate": 2.665625e-05,
      "loss": 1.1685,
      "step": 1148
    },
    {
      "bce_eff_w": 0.31235,
      "bce_loss": -0.46243882179260254,
      "epoch": 5.74,
      "step": 1148
    },
    {
      "epoch": 5.745,
      "grad_norm": 19.358165740966797,
      "learning_rate": 2.6625e-05,
      "loss": 2.3014,
      "step": 1149
    },
    {
      "bce_eff_w": 0.3125,
      "bce_loss": -0.4644416868686676,
      "epoch": 5.745,
      "step": 1149
    },
    {
      "epoch": 5.75,
      "grad_norm": 28.335050582885742,
      "learning_rate": 2.659375e-05,
      "loss": 0.7931,
      "step": 1150
    },
    {
      "bce_eff_w": 0.31265,
      "bce_loss": -0.46190375089645386,
      "epoch": 5.75,
      "step": 1150
    },
    {
      "epoch": 5.755,
      "grad_norm": 15.48788070678711,
      "learning_rate": 2.6562500000000002e-05,
      "loss": 1.3862,
      "step": 1151
    },
    {
      "bce_eff_w": 0.3128,
      "bce_loss": -0.4617585241794586,
      "epoch": 5.755,
      "step": 1151
    },
    {
      "epoch": 5.76,
      "grad_norm": 18.916324615478516,
      "learning_rate": 2.653125e-05,
      "loss": 0.6972,
      "step": 1152
    },
    {
      "bce_eff_w": 0.31295,
      "bce_loss": -0.4615875482559204,
      "epoch": 5.76,
      "step": 1152
    },
    {
      "epoch": 5.765,
      "grad_norm": 15.261423110961914,
      "learning_rate": 2.6500000000000004e-05,
      "loss": 0.2639,
      "step": 1153
    },
    {
      "bce_eff_w": 0.3131,
      "bce_loss": -0.4620262682437897,
      "epoch": 5.765,
      "step": 1153
    },
    {
      "epoch": 5.77,
      "grad_norm": 17.68335723876953,
      "learning_rate": 2.6468750000000003e-05,
      "loss": 1.2725,
      "step": 1154
    },
    {
      "bce_eff_w": 0.31325000000000003,
      "bce_loss": -0.4617922306060791,
      "epoch": 5.77,
      "step": 1154
    },
    {
      "epoch": 5.775,
      "grad_norm": 60.94717025756836,
      "learning_rate": 2.6437500000000002e-05,
      "loss": 1.993,
      "step": 1155
    },
    {
      "bce_eff_w": 0.3134,
      "bce_loss": -0.46406853199005127,
      "epoch": 5.775,
      "step": 1155
    },
    {
      "epoch": 5.78,
      "grad_norm": 22.231407165527344,
      "learning_rate": 2.6406249999999998e-05,
      "loss": 0.8608,
      "step": 1156
    },
    {
      "bce_eff_w": 0.31355,
      "bce_loss": -0.4627564549446106,
      "epoch": 5.78,
      "step": 1156
    },
    {
      "epoch": 5.785,
      "grad_norm": 19.38126564025879,
      "learning_rate": 2.6375e-05,
      "loss": 1.0144,
      "step": 1157
    },
    {
      "bce_eff_w": 0.3137,
      "bce_loss": -0.4493940472602844,
      "epoch": 5.785,
      "step": 1157
    },
    {
      "epoch": 5.79,
      "grad_norm": 21.916879653930664,
      "learning_rate": 2.634375e-05,
      "loss": 0.6754,
      "step": 1158
    },
    {
      "bce_eff_w": 0.31385,
      "bce_loss": -0.4603600800037384,
      "epoch": 5.79,
      "step": 1158
    },
    {
      "epoch": 5.795,
      "grad_norm": 17.38397979736328,
      "learning_rate": 2.6312500000000003e-05,
      "loss": 1.0685,
      "step": 1159
    },
    {
      "bce_eff_w": 0.314,
      "bce_loss": -0.46257612109184265,
      "epoch": 5.795,
      "step": 1159
    },
    {
      "epoch": 5.8,
      "grad_norm": 15.91598892211914,
      "learning_rate": 2.6281250000000002e-05,
      "loss": 0.4785,
      "step": 1160
    },
    {
      "bce_eff_w": 0.31415000000000004,
      "bce_loss": -0.4583650827407837,
      "epoch": 5.8,
      "step": 1160
    },
    {
      "epoch": 5.805,
      "grad_norm": 15.213325500488281,
      "learning_rate": 2.625e-05,
      "loss": 0.528,
      "step": 1161
    },
    {
      "bce_eff_w": 0.3143,
      "bce_loss": -0.4614594876766205,
      "epoch": 5.805,
      "step": 1161
    },
    {
      "epoch": 5.8100000000000005,
      "grad_norm": 21.29552459716797,
      "learning_rate": 2.6218750000000004e-05,
      "loss": 0.1968,
      "step": 1162
    },
    {
      "bce_eff_w": 0.31445,
      "bce_loss": -0.4599108099937439,
      "epoch": 5.8100000000000005,
      "step": 1162
    },
    {
      "epoch": 5.8149999999999995,
      "grad_norm": 18.265066146850586,
      "learning_rate": 2.6187500000000003e-05,
      "loss": 0.6077,
      "step": 1163
    },
    {
      "bce_eff_w": 0.3146,
      "bce_loss": -0.45823585987091064,
      "epoch": 5.8149999999999995,
      "step": 1163
    },
    {
      "epoch": 5.82,
      "grad_norm": 24.34492301940918,
      "learning_rate": 2.615625e-05,
      "loss": 0.8559,
      "step": 1164
    },
    {
      "bce_eff_w": 0.31475,
      "bce_loss": -0.45858505368232727,
      "epoch": 5.82,
      "step": 1164
    },
    {
      "epoch": 5.825,
      "grad_norm": 16.36997413635254,
      "learning_rate": 2.6124999999999998e-05,
      "loss": 0.4272,
      "step": 1165
    },
    {
      "bce_eff_w": 0.3149,
      "bce_loss": -0.46280473470687866,
      "epoch": 5.825,
      "step": 1165
    },
    {
      "epoch": 5.83,
      "grad_norm": 24.33158302307129,
      "learning_rate": 2.609375e-05,
      "loss": 0.3961,
      "step": 1166
    },
    {
      "bce_eff_w": 0.31505,
      "bce_loss": -0.4579576551914215,
      "epoch": 5.83,
      "step": 1166
    },
    {
      "epoch": 5.835,
      "grad_norm": 20.012470245361328,
      "learning_rate": 2.60625e-05,
      "loss": 1.3345,
      "step": 1167
    },
    {
      "bce_eff_w": 0.31520000000000004,
      "bce_loss": -0.4590223431587219,
      "epoch": 5.835,
      "step": 1167
    },
    {
      "epoch": 5.84,
      "grad_norm": 15.734560012817383,
      "learning_rate": 2.6031250000000003e-05,
      "loss": 2.7293,
      "step": 1168
    },
    {
      "bce_eff_w": 0.31535,
      "bce_loss": -0.4587230980396271,
      "epoch": 5.84,
      "step": 1168
    },
    {
      "epoch": 5.845,
      "grad_norm": 36.86611557006836,
      "learning_rate": 2.6000000000000002e-05,
      "loss": 3.0096,
      "step": 1169
    },
    {
      "bce_eff_w": 0.3155,
      "bce_loss": -0.46404826641082764,
      "epoch": 5.845,
      "step": 1169
    },
    {
      "epoch": 5.85,
      "grad_norm": 26.423742294311523,
      "learning_rate": 2.5968750000000004e-05,
      "loss": 0.6342,
      "step": 1170
    },
    {
      "bce_eff_w": 0.31565,
      "bce_loss": -0.45773231983184814,
      "epoch": 5.85,
      "step": 1170
    },
    {
      "epoch": 5.855,
      "grad_norm": 30.748126983642578,
      "learning_rate": 2.5937500000000004e-05,
      "loss": 0.8785,
      "step": 1171
    },
    {
      "bce_eff_w": 0.3158,
      "bce_loss": -0.4626008868217468,
      "epoch": 5.855,
      "step": 1171
    },
    {
      "epoch": 5.86,
      "grad_norm": 17.685338973999023,
      "learning_rate": 2.590625e-05,
      "loss": 1.0688,
      "step": 1172
    },
    {
      "bce_eff_w": 0.31595,
      "bce_loss": -0.4628775119781494,
      "epoch": 5.86,
      "step": 1172
    },
    {
      "epoch": 5.865,
      "grad_norm": 23.487516403198242,
      "learning_rate": 2.5875e-05,
      "loss": 0.8398,
      "step": 1173
    },
    {
      "bce_eff_w": 0.3161,
      "bce_loss": -0.45201534032821655,
      "epoch": 5.865,
      "step": 1173
    },
    {
      "epoch": 5.87,
      "grad_norm": 11.07646656036377,
      "learning_rate": 2.584375e-05,
      "loss": 0.156,
      "step": 1174
    },
    {
      "bce_eff_w": 0.31625000000000003,
      "bce_loss": -0.462603896856308,
      "epoch": 5.87,
      "step": 1174
    },
    {
      "epoch": 5.875,
      "grad_norm": 20.76801300048828,
      "learning_rate": 2.58125e-05,
      "loss": 1.7268,
      "step": 1175
    },
    {
      "bce_eff_w": 0.3164,
      "bce_loss": -0.46265795826911926,
      "epoch": 5.875,
      "step": 1175
    },
    {
      "epoch": 5.88,
      "grad_norm": 13.42664909362793,
      "learning_rate": 2.578125e-05,
      "loss": 0.1461,
      "step": 1176
    },
    {
      "bce_eff_w": 0.31655,
      "bce_loss": -0.4557550549507141,
      "epoch": 5.88,
      "step": 1176
    },
    {
      "epoch": 5.885,
      "grad_norm": 21.27577781677246,
      "learning_rate": 2.5750000000000002e-05,
      "loss": 0.7692,
      "step": 1177
    },
    {
      "bce_eff_w": 0.3167,
      "bce_loss": -0.45955732464790344,
      "epoch": 5.885,
      "step": 1177
    },
    {
      "epoch": 5.89,
      "grad_norm": 23.176198959350586,
      "learning_rate": 2.5718750000000002e-05,
      "loss": 1.4677,
      "step": 1178
    },
    {
      "bce_eff_w": 0.31685,
      "bce_loss": -0.4593594968318939,
      "epoch": 5.89,
      "step": 1178
    },
    {
      "epoch": 5.895,
      "grad_norm": 17.103290557861328,
      "learning_rate": 2.5687500000000004e-05,
      "loss": 0.9798,
      "step": 1179
    },
    {
      "bce_eff_w": 0.317,
      "bce_loss": -0.4631092846393585,
      "epoch": 5.895,
      "step": 1179
    },
    {
      "epoch": 5.9,
      "grad_norm": 15.242185592651367,
      "learning_rate": 2.5656250000000004e-05,
      "loss": 0.1488,
      "step": 1180
    },
    {
      "bce_eff_w": 0.31715000000000004,
      "bce_loss": -0.46076279878616333,
      "epoch": 5.9,
      "step": 1180
    },
    {
      "epoch": 5.905,
      "grad_norm": 23.045434951782227,
      "learning_rate": 2.5625e-05,
      "loss": 1.5351,
      "step": 1181
    },
    {
      "bce_eff_w": 0.3173,
      "bce_loss": -0.4583205580711365,
      "epoch": 5.905,
      "step": 1181
    },
    {
      "epoch": 5.91,
      "grad_norm": 21.813684463500977,
      "learning_rate": 2.559375e-05,
      "loss": 0.8474,
      "step": 1182
    },
    {
      "bce_eff_w": 0.31745,
      "bce_loss": -0.4558904469013214,
      "epoch": 5.91,
      "step": 1182
    },
    {
      "epoch": 5.915,
      "grad_norm": 18.091489791870117,
      "learning_rate": 2.55625e-05,
      "loss": 0.952,
      "step": 1183
    },
    {
      "bce_eff_w": 0.3176,
      "bce_loss": -0.4604743421077728,
      "epoch": 5.915,
      "step": 1183
    },
    {
      "epoch": 5.92,
      "grad_norm": 16.866422653198242,
      "learning_rate": 2.553125e-05,
      "loss": 1.0542,
      "step": 1184
    },
    {
      "bce_eff_w": 0.31775,
      "bce_loss": -0.4632774591445923,
      "epoch": 5.92,
      "step": 1184
    },
    {
      "epoch": 5.925,
      "grad_norm": 18.101661682128906,
      "learning_rate": 2.5500000000000003e-05,
      "loss": 0.8805,
      "step": 1185
    },
    {
      "bce_eff_w": 0.3179,
      "bce_loss": -0.46217021346092224,
      "epoch": 5.925,
      "step": 1185
    },
    {
      "epoch": 5.93,
      "grad_norm": 22.301664352416992,
      "learning_rate": 2.5468750000000002e-05,
      "loss": 1.2912,
      "step": 1186
    },
    {
      "bce_eff_w": 0.31805,
      "bce_loss": -0.45687001943588257,
      "epoch": 5.93,
      "step": 1186
    },
    {
      "epoch": 5.9350000000000005,
      "grad_norm": 25.917633056640625,
      "learning_rate": 2.54375e-05,
      "loss": 0.3435,
      "step": 1187
    },
    {
      "bce_eff_w": 0.31820000000000004,
      "bce_loss": -0.45535027980804443,
      "epoch": 5.9350000000000005,
      "step": 1187
    },
    {
      "epoch": 5.9399999999999995,
      "grad_norm": 22.98390007019043,
      "learning_rate": 2.5406250000000004e-05,
      "loss": 2.3945,
      "step": 1188
    },
    {
      "bce_eff_w": 0.31835,
      "bce_loss": -0.45469701290130615,
      "epoch": 5.9399999999999995,
      "step": 1188
    },
    {
      "epoch": 5.945,
      "grad_norm": 11.038466453552246,
      "learning_rate": 2.5375e-05,
      "loss": 0.098,
      "step": 1189
    },
    {
      "bce_eff_w": 0.3185,
      "bce_loss": -0.46291524171829224,
      "epoch": 5.945,
      "step": 1189
    },
    {
      "epoch": 5.95,
      "grad_norm": 30.2611026763916,
      "learning_rate": 2.534375e-05,
      "loss": 0.8055,
      "step": 1190
    },
    {
      "bce_eff_w": 0.31865,
      "bce_loss": -0.4625350832939148,
      "epoch": 5.95,
      "step": 1190
    },
    {
      "epoch": 5.955,
      "grad_norm": 25.064794540405273,
      "learning_rate": 2.53125e-05,
      "loss": 0.7007,
      "step": 1191
    },
    {
      "bce_eff_w": 0.31880000000000003,
      "bce_loss": -0.46069034934043884,
      "epoch": 5.955,
      "step": 1191
    },
    {
      "epoch": 5.96,
      "grad_norm": 18.419139862060547,
      "learning_rate": 2.528125e-05,
      "loss": 0.2145,
      "step": 1192
    },
    {
      "bce_eff_w": 0.31895,
      "bce_loss": -0.4494103491306305,
      "epoch": 5.96,
      "step": 1192
    },
    {
      "epoch": 5.965,
      "grad_norm": 27.038774490356445,
      "learning_rate": 2.525e-05,
      "loss": 1.5826,
      "step": 1193
    },
    {
      "bce_eff_w": 0.3191,
      "bce_loss": -0.46378061175346375,
      "epoch": 5.965,
      "step": 1193
    },
    {
      "epoch": 5.97,
      "grad_norm": 17.001752853393555,
      "learning_rate": 2.5218750000000003e-05,
      "loss": 0.1759,
      "step": 1194
    },
    {
      "bce_eff_w": 0.31925000000000003,
      "bce_loss": -0.4628618657588959,
      "epoch": 5.97,
      "step": 1194
    },
    {
      "epoch": 5.975,
      "grad_norm": 16.650033950805664,
      "learning_rate": 2.5187500000000002e-05,
      "loss": 0.9004,
      "step": 1195
    },
    {
      "bce_eff_w": 0.3194,
      "bce_loss": -0.4634218215942383,
      "epoch": 5.975,
      "step": 1195
    },
    {
      "epoch": 5.98,
      "grad_norm": 21.6290340423584,
      "learning_rate": 2.5156250000000005e-05,
      "loss": 0.4343,
      "step": 1196
    },
    {
      "bce_eff_w": 0.31955,
      "bce_loss": -0.46183133125305176,
      "epoch": 5.98,
      "step": 1196
    },
    {
      "epoch": 5.985,
      "grad_norm": 25.343524932861328,
      "learning_rate": 2.5124999999999997e-05,
      "loss": 0.5211,
      "step": 1197
    },
    {
      "bce_eff_w": 0.3197,
      "bce_loss": -0.45741137862205505,
      "epoch": 5.985,
      "step": 1197
    },
    {
      "epoch": 5.99,
      "grad_norm": 15.911298751831055,
      "learning_rate": 2.509375e-05,
      "loss": 2.3451,
      "step": 1198
    },
    {
      "bce_eff_w": 0.31985,
      "bce_loss": -0.4636456370353699,
      "epoch": 5.99,
      "step": 1198
    },
    {
      "epoch": 5.995,
      "grad_norm": 15.853507995605469,
      "learning_rate": 2.50625e-05,
      "loss": 0.0793,
      "step": 1199
    },
    {
      "bce_eff_w": 0.32,
      "bce_loss": -0.4636547863483429,
      "epoch": 5.995,
      "step": 1199
    },
    {
      "epoch": 6.0,
      "grad_norm": 26.481468200683594,
      "learning_rate": 2.5031250000000002e-05,
      "loss": 1.0506,
      "step": 1200
    },
    {
      "bce_eff_w": 0.32015000000000005,
      "bce_loss": -0.46177083253860474,
      "epoch": 6.0,
      "step": 1200
    },
    {
      "epoch": 6.005,
      "grad_norm": 23.31121253967285,
      "learning_rate": 2.5e-05,
      "loss": 1.413,
      "step": 1201
    },
    {
      "bce_eff_w": 0.32030000000000003,
      "bce_loss": -0.4638046622276306,
      "epoch": 6.005,
      "step": 1201
    },
    {
      "epoch": 6.01,
      "grad_norm": 10.709754943847656,
      "learning_rate": 2.496875e-05,
      "loss": 0.0533,
      "step": 1202
    },
    {
      "bce_eff_w": 0.32045,
      "bce_loss": -0.45805954933166504,
      "epoch": 6.01,
      "step": 1202
    },
    {
      "epoch": 6.015,
      "grad_norm": 17.0111141204834,
      "learning_rate": 2.4937500000000003e-05,
      "loss": 0.3914,
      "step": 1203
    },
    {
      "bce_eff_w": 0.3206,
      "bce_loss": -0.4631248712539673,
      "epoch": 6.015,
      "step": 1203
    },
    {
      "epoch": 6.02,
      "grad_norm": 4.694472312927246,
      "learning_rate": 2.490625e-05,
      "loss": -0.0666,
      "step": 1204
    },
    {
      "bce_eff_w": 0.32075,
      "bce_loss": -0.46322202682495117,
      "epoch": 6.02,
      "step": 1204
    },
    {
      "epoch": 6.025,
      "grad_norm": 26.878381729125977,
      "learning_rate": 2.4875e-05,
      "loss": 1.0604,
      "step": 1205
    },
    {
      "bce_eff_w": 0.3209,
      "bce_loss": -0.4602217674255371,
      "epoch": 6.025,
      "step": 1205
    },
    {
      "epoch": 6.03,
      "grad_norm": 16.689865112304688,
      "learning_rate": 2.484375e-05,
      "loss": 0.2249,
      "step": 1206
    },
    {
      "bce_eff_w": 0.32105,
      "bce_loss": -0.46313533186912537,
      "epoch": 6.03,
      "step": 1206
    },
    {
      "epoch": 6.035,
      "grad_norm": 8.803056716918945,
      "learning_rate": 2.4812500000000003e-05,
      "loss": -0.0428,
      "step": 1207
    },
    {
      "bce_eff_w": 0.32120000000000004,
      "bce_loss": -0.4633704423904419,
      "epoch": 6.035,
      "step": 1207
    },
    {
      "epoch": 6.04,
      "grad_norm": 19.20229148864746,
      "learning_rate": 2.478125e-05,
      "loss": 1.1628,
      "step": 1208
    },
    {
      "bce_eff_w": 0.32135,
      "bce_loss": -0.45689767599105835,
      "epoch": 6.04,
      "step": 1208
    },
    {
      "epoch": 6.045,
      "grad_norm": 13.00823974609375,
      "learning_rate": 2.4750000000000002e-05,
      "loss": 2.36,
      "step": 1209
    },
    {
      "bce_eff_w": 0.3215,
      "bce_loss": -0.460744172334671,
      "epoch": 6.045,
      "step": 1209
    },
    {
      "epoch": 6.05,
      "grad_norm": 31.6943416595459,
      "learning_rate": 2.471875e-05,
      "loss": 0.6934,
      "step": 1210
    },
    {
      "bce_eff_w": 0.32165,
      "bce_loss": -0.46211132407188416,
      "epoch": 6.05,
      "step": 1210
    },
    {
      "epoch": 6.055,
      "grad_norm": 27.121023178100586,
      "learning_rate": 2.4687500000000004e-05,
      "loss": 0.6501,
      "step": 1211
    },
    {
      "bce_eff_w": 0.32180000000000003,
      "bce_loss": -0.46115919947624207,
      "epoch": 6.055,
      "step": 1211
    },
    {
      "epoch": 6.06,
      "grad_norm": 14.76822566986084,
      "learning_rate": 2.465625e-05,
      "loss": 2.2962,
      "step": 1212
    },
    {
      "bce_eff_w": 0.32195,
      "bce_loss": -0.46350422501564026,
      "epoch": 6.06,
      "step": 1212
    },
    {
      "epoch": 6.065,
      "grad_norm": 25.907560348510742,
      "learning_rate": 2.4625000000000002e-05,
      "loss": 0.531,
      "step": 1213
    },
    {
      "bce_eff_w": 0.3221,
      "bce_loss": -0.46314606070518494,
      "epoch": 6.065,
      "step": 1213
    },
    {
      "epoch": 6.07,
      "grad_norm": 14.383981704711914,
      "learning_rate": 2.459375e-05,
      "loss": 1.6845,
      "step": 1214
    },
    {
      "bce_eff_w": 0.32225,
      "bce_loss": -0.46388474106788635,
      "epoch": 6.07,
      "step": 1214
    },
    {
      "epoch": 6.075,
      "grad_norm": 20.675277709960938,
      "learning_rate": 2.45625e-05,
      "loss": 0.6314,
      "step": 1215
    },
    {
      "bce_eff_w": 0.3224,
      "bce_loss": -0.46036118268966675,
      "epoch": 6.075,
      "step": 1215
    },
    {
      "epoch": 6.08,
      "grad_norm": 16.844772338867188,
      "learning_rate": 2.453125e-05,
      "loss": 2.5162,
      "step": 1216
    },
    {
      "bce_eff_w": 0.32255,
      "bce_loss": -0.4593316614627838,
      "epoch": 6.08,
      "step": 1216
    },
    {
      "epoch": 6.085,
      "grad_norm": 19.82378387451172,
      "learning_rate": 2.45e-05,
      "loss": 1.0405,
      "step": 1217
    },
    {
      "bce_eff_w": 0.3227,
      "bce_loss": -0.46186161041259766,
      "epoch": 6.085,
      "step": 1217
    },
    {
      "epoch": 6.09,
      "grad_norm": 16.437759399414062,
      "learning_rate": 2.4468750000000002e-05,
      "loss": 0.5779,
      "step": 1218
    },
    {
      "bce_eff_w": 0.32284999999999997,
      "bce_loss": -0.4613896906375885,
      "epoch": 6.09,
      "step": 1218
    },
    {
      "epoch": 6.095,
      "grad_norm": 18.608118057250977,
      "learning_rate": 2.44375e-05,
      "loss": 0.1947,
      "step": 1219
    },
    {
      "bce_eff_w": 0.323,
      "bce_loss": -0.46167412400245667,
      "epoch": 6.095,
      "step": 1219
    },
    {
      "epoch": 6.1,
      "grad_norm": 21.222307205200195,
      "learning_rate": 2.440625e-05,
      "loss": 1.4512,
      "step": 1220
    },
    {
      "bce_eff_w": 0.32315,
      "bce_loss": -0.4623585343360901,
      "epoch": 6.1,
      "step": 1220
    },
    {
      "epoch": 6.105,
      "grad_norm": 15.927189826965332,
      "learning_rate": 2.4375e-05,
      "loss": 0.6369,
      "step": 1221
    },
    {
      "bce_eff_w": 0.32330000000000003,
      "bce_loss": -0.4596119523048401,
      "epoch": 6.105,
      "step": 1221
    },
    {
      "epoch": 6.11,
      "grad_norm": 14.700310707092285,
      "learning_rate": 2.4343750000000002e-05,
      "loss": 0.4527,
      "step": 1222
    },
    {
      "bce_eff_w": 0.32345,
      "bce_loss": -0.4586237967014313,
      "epoch": 6.11,
      "step": 1222
    },
    {
      "epoch": 6.115,
      "grad_norm": 20.581758499145508,
      "learning_rate": 2.43125e-05,
      "loss": 0.6654,
      "step": 1223
    },
    {
      "bce_eff_w": 0.3236,
      "bce_loss": -0.4643399715423584,
      "epoch": 6.115,
      "step": 1223
    },
    {
      "epoch": 6.12,
      "grad_norm": 22.978546142578125,
      "learning_rate": 2.428125e-05,
      "loss": 1.4761,
      "step": 1224
    },
    {
      "bce_eff_w": 0.32375,
      "bce_loss": -0.46129828691482544,
      "epoch": 6.12,
      "step": 1224
    },
    {
      "epoch": 6.125,
      "grad_norm": 17.792552947998047,
      "learning_rate": 2.425e-05,
      "loss": 0.8587,
      "step": 1225
    },
    {
      "bce_eff_w": 0.32389999999999997,
      "bce_loss": -0.464377224445343,
      "epoch": 6.125,
      "step": 1225
    },
    {
      "epoch": 6.13,
      "grad_norm": 9.117396354675293,
      "learning_rate": 2.4218750000000003e-05,
      "loss": 0.0698,
      "step": 1226
    },
    {
      "bce_eff_w": 0.32405,
      "bce_loss": -0.45993953943252563,
      "epoch": 6.13,
      "step": 1226
    },
    {
      "epoch": 6.135,
      "grad_norm": 23.39590072631836,
      "learning_rate": 2.4187500000000002e-05,
      "loss": 2.4659,
      "step": 1227
    },
    {
      "bce_eff_w": 0.3242,
      "bce_loss": -0.4622868001461029,
      "epoch": 6.135,
      "step": 1227
    },
    {
      "epoch": 6.14,
      "grad_norm": 31.251501083374023,
      "learning_rate": 2.415625e-05,
      "loss": 0.3885,
      "step": 1228
    },
    {
      "bce_eff_w": 0.32435,
      "bce_loss": -0.4562462568283081,
      "epoch": 6.14,
      "step": 1228
    },
    {
      "epoch": 6.145,
      "grad_norm": 18.09807014465332,
      "learning_rate": 2.4125e-05,
      "loss": 2.2053,
      "step": 1229
    },
    {
      "bce_eff_w": 0.3245,
      "bce_loss": -0.4604507386684418,
      "epoch": 6.145,
      "step": 1229
    },
    {
      "epoch": 6.15,
      "grad_norm": 27.398880004882812,
      "learning_rate": 2.409375e-05,
      "loss": 0.6244,
      "step": 1230
    },
    {
      "bce_eff_w": 0.32465,
      "bce_loss": -0.46287989616394043,
      "epoch": 6.15,
      "step": 1230
    },
    {
      "epoch": 6.155,
      "grad_norm": 11.681328773498535,
      "learning_rate": 2.4062500000000002e-05,
      "loss": 0.0357,
      "step": 1231
    },
    {
      "bce_eff_w": 0.3248,
      "bce_loss": -0.4641496539115906,
      "epoch": 6.155,
      "step": 1231
    },
    {
      "epoch": 6.16,
      "grad_norm": 17.71007537841797,
      "learning_rate": 2.403125e-05,
      "loss": 0.2835,
      "step": 1232
    },
    {
      "bce_eff_w": 0.32495,
      "bce_loss": -0.46235164999961853,
      "epoch": 6.16,
      "step": 1232
    },
    {
      "epoch": 6.165,
      "grad_norm": 13.608434677124023,
      "learning_rate": 2.4e-05,
      "loss": 0.1173,
      "step": 1233
    },
    {
      "bce_eff_w": 0.3251,
      "bce_loss": -0.4640524387359619,
      "epoch": 6.165,
      "step": 1233
    },
    {
      "epoch": 6.17,
      "grad_norm": 19.573596954345703,
      "learning_rate": 2.396875e-05,
      "loss": 0.5349,
      "step": 1234
    },
    {
      "bce_eff_w": 0.32525000000000004,
      "bce_loss": -0.4509957730770111,
      "epoch": 6.17,
      "step": 1234
    },
    {
      "epoch": 6.175,
      "grad_norm": 14.043363571166992,
      "learning_rate": 2.3937500000000002e-05,
      "loss": 2.8463,
      "step": 1235
    },
    {
      "bce_eff_w": 0.3254,
      "bce_loss": -0.4634650647640228,
      "epoch": 6.175,
      "step": 1235
    },
    {
      "epoch": 6.18,
      "grad_norm": 52.25577163696289,
      "learning_rate": 2.3906250000000002e-05,
      "loss": 2.5771,
      "step": 1236
    },
    {
      "bce_eff_w": 0.32555,
      "bce_loss": -0.46474599838256836,
      "epoch": 6.18,
      "step": 1236
    },
    {
      "epoch": 6.185,
      "grad_norm": 17.738683700561523,
      "learning_rate": 2.3875e-05,
      "loss": 0.3305,
      "step": 1237
    },
    {
      "bce_eff_w": 0.3257,
      "bce_loss": -0.4600183665752411,
      "epoch": 6.185,
      "step": 1237
    },
    {
      "epoch": 6.19,
      "grad_norm": 11.415931701660156,
      "learning_rate": 2.384375e-05,
      "loss": 0.1308,
      "step": 1238
    },
    {
      "bce_eff_w": 0.32585,
      "bce_loss": -0.4595644772052765,
      "epoch": 6.19,
      "step": 1238
    },
    {
      "epoch": 6.195,
      "grad_norm": 19.361587524414062,
      "learning_rate": 2.3812500000000003e-05,
      "loss": 0.26,
      "step": 1239
    },
    {
      "bce_eff_w": 0.326,
      "bce_loss": -0.45097169280052185,
      "epoch": 6.195,
      "step": 1239
    },
    {
      "epoch": 6.2,
      "grad_norm": 19.025157928466797,
      "learning_rate": 2.3781250000000002e-05,
      "loss": 1.0062,
      "step": 1240
    },
    {
      "bce_eff_w": 0.32615,
      "bce_loss": -0.46279260516166687,
      "epoch": 6.2,
      "step": 1240
    },
    {
      "epoch": 6.205,
      "grad_norm": 19.489553451538086,
      "learning_rate": 2.375e-05,
      "loss": 0.9306,
      "step": 1241
    },
    {
      "bce_eff_w": 0.32630000000000003,
      "bce_loss": -0.4632895886898041,
      "epoch": 6.205,
      "step": 1241
    },
    {
      "epoch": 6.21,
      "grad_norm": 64.83441925048828,
      "learning_rate": 2.371875e-05,
      "loss": 1.7444,
      "step": 1242
    },
    {
      "bce_eff_w": 0.32645,
      "bce_loss": -0.46252408623695374,
      "epoch": 6.21,
      "step": 1242
    },
    {
      "epoch": 6.215,
      "grad_norm": 24.17519187927246,
      "learning_rate": 2.36875e-05,
      "loss": 0.5337,
      "step": 1243
    },
    {
      "bce_eff_w": 0.3266,
      "bce_loss": -0.46224623918533325,
      "epoch": 6.215,
      "step": 1243
    },
    {
      "epoch": 6.22,
      "grad_norm": 21.80051612854004,
      "learning_rate": 2.3656250000000002e-05,
      "loss": 0.567,
      "step": 1244
    },
    {
      "bce_eff_w": 0.32675,
      "bce_loss": -0.4553433060646057,
      "epoch": 6.22,
      "step": 1244
    },
    {
      "epoch": 6.225,
      "grad_norm": 25.122957229614258,
      "learning_rate": 2.3624999999999998e-05,
      "loss": 2.1933,
      "step": 1245
    },
    {
      "bce_eff_w": 0.32689999999999997,
      "bce_loss": -0.45814040303230286,
      "epoch": 6.225,
      "step": 1245
    },
    {
      "epoch": 6.23,
      "grad_norm": 16.821664810180664,
      "learning_rate": 2.359375e-05,
      "loss": 0.6366,
      "step": 1246
    },
    {
      "bce_eff_w": 0.32705,
      "bce_loss": -0.46246618032455444,
      "epoch": 6.23,
      "step": 1246
    },
    {
      "epoch": 6.235,
      "grad_norm": 19.07891273498535,
      "learning_rate": 2.35625e-05,
      "loss": 1.2753,
      "step": 1247
    },
    {
      "bce_eff_w": 0.3272,
      "bce_loss": -0.4630230665206909,
      "epoch": 6.235,
      "step": 1247
    },
    {
      "epoch": 6.24,
      "grad_norm": 24.551589965820312,
      "learning_rate": 2.3531250000000003e-05,
      "loss": 0.5172,
      "step": 1248
    },
    {
      "bce_eff_w": 0.32735000000000003,
      "bce_loss": -0.46209996938705444,
      "epoch": 6.24,
      "step": 1248
    },
    {
      "epoch": 6.245,
      "grad_norm": 31.009492874145508,
      "learning_rate": 2.35e-05,
      "loss": 1.0117,
      "step": 1249
    },
    {
      "bce_eff_w": 0.3275,
      "bce_loss": -0.4626379609107971,
      "epoch": 6.245,
      "step": 1249
    },
    {
      "epoch": 6.25,
      "grad_norm": 19.974401473999023,
      "learning_rate": 2.346875e-05,
      "loss": 0.5326,
      "step": 1250
    },
    {
      "bce_eff_w": 0.32765,
      "bce_loss": -0.4611433744430542,
      "epoch": 6.25,
      "step": 1250
    },
    {
      "epoch": 6.255,
      "grad_norm": 16.25477409362793,
      "learning_rate": 2.34375e-05,
      "loss": 1.2353,
      "step": 1251
    },
    {
      "bce_eff_w": 0.3278,
      "bce_loss": -0.46188482642173767,
      "epoch": 6.255,
      "step": 1251
    },
    {
      "epoch": 6.26,
      "grad_norm": 24.323991775512695,
      "learning_rate": 2.3406250000000003e-05,
      "loss": 1.168,
      "step": 1252
    },
    {
      "bce_eff_w": 0.32794999999999996,
      "bce_loss": -0.46031248569488525,
      "epoch": 6.26,
      "step": 1252
    },
    {
      "epoch": 6.265,
      "grad_norm": 21.49642562866211,
      "learning_rate": 2.3375000000000002e-05,
      "loss": 0.9084,
      "step": 1253
    },
    {
      "bce_eff_w": 0.3281,
      "bce_loss": -0.4564485549926758,
      "epoch": 6.265,
      "step": 1253
    },
    {
      "epoch": 6.27,
      "grad_norm": 20.147327423095703,
      "learning_rate": 2.334375e-05,
      "loss": 1.616,
      "step": 1254
    },
    {
      "bce_eff_w": 0.32825000000000004,
      "bce_loss": -0.45795485377311707,
      "epoch": 6.27,
      "step": 1254
    },
    {
      "epoch": 6.275,
      "grad_norm": 7.624711036682129,
      "learning_rate": 2.33125e-05,
      "loss": -0.0383,
      "step": 1255
    },
    {
      "bce_eff_w": 0.3284,
      "bce_loss": -0.46204039454460144,
      "epoch": 6.275,
      "step": 1255
    },
    {
      "epoch": 6.28,
      "grad_norm": 16.786834716796875,
      "learning_rate": 2.328125e-05,
      "loss": 0.13,
      "step": 1256
    },
    {
      "bce_eff_w": 0.32855,
      "bce_loss": -0.46146437525749207,
      "epoch": 6.28,
      "step": 1256
    },
    {
      "epoch": 6.285,
      "grad_norm": 17.266904830932617,
      "learning_rate": 2.3250000000000003e-05,
      "loss": 1.6051,
      "step": 1257
    },
    {
      "bce_eff_w": 0.3287,
      "bce_loss": -0.4583001434803009,
      "epoch": 6.285,
      "step": 1257
    },
    {
      "epoch": 6.29,
      "grad_norm": 12.836160659790039,
      "learning_rate": 2.321875e-05,
      "loss": 2.2461,
      "step": 1258
    },
    {
      "bce_eff_w": 0.32885,
      "bce_loss": -0.4607899785041809,
      "epoch": 6.29,
      "step": 1258
    },
    {
      "epoch": 6.295,
      "grad_norm": 18.272789001464844,
      "learning_rate": 2.31875e-05,
      "loss": 1.2747,
      "step": 1259
    },
    {
      "bce_eff_w": 0.329,
      "bce_loss": -0.46279096603393555,
      "epoch": 6.295,
      "step": 1259
    },
    {
      "epoch": 6.3,
      "grad_norm": 20.100181579589844,
      "learning_rate": 2.315625e-05,
      "loss": 1.146,
      "step": 1260
    },
    {
      "bce_eff_w": 0.32915,
      "bce_loss": -0.4606306850910187,
      "epoch": 6.3,
      "step": 1260
    },
    {
      "epoch": 6.305,
      "grad_norm": 11.919116020202637,
      "learning_rate": 2.3125000000000003e-05,
      "loss": 0.0256,
      "step": 1261
    },
    {
      "bce_eff_w": 0.32930000000000004,
      "bce_loss": -0.45996859669685364,
      "epoch": 6.305,
      "step": 1261
    },
    {
      "epoch": 6.31,
      "grad_norm": 7.463357448577881,
      "learning_rate": 2.309375e-05,
      "loss": -0.0666,
      "step": 1262
    },
    {
      "bce_eff_w": 0.32945,
      "bce_loss": -0.46393918991088867,
      "epoch": 6.31,
      "step": 1262
    },
    {
      "epoch": 6.315,
      "grad_norm": 15.207343101501465,
      "learning_rate": 2.30625e-05,
      "loss": 0.0286,
      "step": 1263
    },
    {
      "bce_eff_w": 0.3296,
      "bce_loss": -0.4646192193031311,
      "epoch": 6.315,
      "step": 1263
    },
    {
      "epoch": 6.32,
      "grad_norm": 34.84949493408203,
      "learning_rate": 2.303125e-05,
      "loss": 0.6556,
      "step": 1264
    },
    {
      "bce_eff_w": 0.32975,
      "bce_loss": -0.4609123170375824,
      "epoch": 6.32,
      "step": 1264
    },
    {
      "epoch": 6.325,
      "grad_norm": 23.911739349365234,
      "learning_rate": 2.3000000000000003e-05,
      "loss": 0.9828,
      "step": 1265
    },
    {
      "bce_eff_w": 0.32989999999999997,
      "bce_loss": -0.4562373757362366,
      "epoch": 6.325,
      "step": 1265
    },
    {
      "epoch": 6.33,
      "grad_norm": 15.358818054199219,
      "learning_rate": 2.296875e-05,
      "loss": 0.2203,
      "step": 1266
    },
    {
      "bce_eff_w": 0.33005,
      "bce_loss": -0.4596807360649109,
      "epoch": 6.33,
      "step": 1266
    },
    {
      "epoch": 6.335,
      "grad_norm": 12.37845230102539,
      "learning_rate": 2.2937500000000002e-05,
      "loss": 2.2927,
      "step": 1267
    },
    {
      "bce_eff_w": 0.3302,
      "bce_loss": -0.4531974494457245,
      "epoch": 6.335,
      "step": 1267
    },
    {
      "epoch": 6.34,
      "grad_norm": 12.791742324829102,
      "learning_rate": 2.290625e-05,
      "loss": 0.2788,
      "step": 1268
    },
    {
      "bce_eff_w": 0.33035000000000003,
      "bce_loss": -0.45234543085098267,
      "epoch": 6.34,
      "step": 1268
    },
    {
      "epoch": 6.345,
      "grad_norm": 20.21497917175293,
      "learning_rate": 2.2875e-05,
      "loss": 0.8033,
      "step": 1269
    },
    {
      "bce_eff_w": 0.3305,
      "bce_loss": -0.4593009054660797,
      "epoch": 6.345,
      "step": 1269
    },
    {
      "epoch": 6.35,
      "grad_norm": 23.501720428466797,
      "learning_rate": 2.284375e-05,
      "loss": 0.8016,
      "step": 1270
    },
    {
      "bce_eff_w": 0.33065,
      "bce_loss": -0.46183836460113525,
      "epoch": 6.35,
      "step": 1270
    },
    {
      "epoch": 6.355,
      "grad_norm": 17.83440589904785,
      "learning_rate": 2.28125e-05,
      "loss": 0.6654,
      "step": 1271
    },
    {
      "bce_eff_w": 0.3308,
      "bce_loss": -0.46199095249176025,
      "epoch": 6.355,
      "step": 1271
    },
    {
      "epoch": 6.36,
      "grad_norm": 19.414987564086914,
      "learning_rate": 2.278125e-05,
      "loss": 0.5065,
      "step": 1272
    },
    {
      "bce_eff_w": 0.33094999999999997,
      "bce_loss": -0.46212252974510193,
      "epoch": 6.36,
      "step": 1272
    },
    {
      "epoch": 6.365,
      "grad_norm": 21.553258895874023,
      "learning_rate": 2.275e-05,
      "loss": 0.0241,
      "step": 1273
    },
    {
      "bce_eff_w": 0.3311,
      "bce_loss": -0.4612988829612732,
      "epoch": 6.365,
      "step": 1273
    },
    {
      "epoch": 6.37,
      "grad_norm": 23.700170516967773,
      "learning_rate": 2.271875e-05,
      "loss": 0.2545,
      "step": 1274
    },
    {
      "bce_eff_w": 0.33125000000000004,
      "bce_loss": -0.4568130373954773,
      "epoch": 6.37,
      "step": 1274
    },
    {
      "epoch": 6.375,
      "grad_norm": 24.404972076416016,
      "learning_rate": 2.26875e-05,
      "loss": 2.3469,
      "step": 1275
    },
    {
      "bce_eff_w": 0.33140000000000003,
      "bce_loss": -0.46359601616859436,
      "epoch": 6.375,
      "step": 1275
    },
    {
      "epoch": 6.38,
      "grad_norm": 17.58086585998535,
      "learning_rate": 2.2656250000000002e-05,
      "loss": 1.7158,
      "step": 1276
    },
    {
      "bce_eff_w": 0.33155,
      "bce_loss": -0.4649145007133484,
      "epoch": 6.38,
      "step": 1276
    },
    {
      "epoch": 6.385,
      "grad_norm": 15.546937942504883,
      "learning_rate": 2.2625e-05,
      "loss": 1.7792,
      "step": 1277
    },
    {
      "bce_eff_w": 0.3317,
      "bce_loss": -0.46340638399124146,
      "epoch": 6.385,
      "step": 1277
    },
    {
      "epoch": 6.39,
      "grad_norm": 40.5220832824707,
      "learning_rate": 2.2593750000000004e-05,
      "loss": 0.5718,
      "step": 1278
    },
    {
      "bce_eff_w": 0.33185,
      "bce_loss": -0.4627721607685089,
      "epoch": 6.39,
      "step": 1278
    },
    {
      "epoch": 6.395,
      "grad_norm": 17.404611587524414,
      "learning_rate": 2.25625e-05,
      "loss": 1.0372,
      "step": 1279
    },
    {
      "bce_eff_w": 0.332,
      "bce_loss": -0.4611656665802002,
      "epoch": 6.395,
      "step": 1279
    },
    {
      "epoch": 6.4,
      "grad_norm": 19.094078063964844,
      "learning_rate": 2.2531250000000002e-05,
      "loss": 0.5185,
      "step": 1280
    },
    {
      "bce_eff_w": 0.33215,
      "bce_loss": -0.4636743664741516,
      "epoch": 6.4,
      "step": 1280
    },
    {
      "epoch": 6.405,
      "grad_norm": 14.254986763000488,
      "learning_rate": 2.25e-05,
      "loss": 1.9635,
      "step": 1281
    },
    {
      "bce_eff_w": 0.33230000000000004,
      "bce_loss": -0.4492107033729553,
      "epoch": 6.405,
      "step": 1281
    },
    {
      "epoch": 6.41,
      "grad_norm": 11.053956031799316,
      "learning_rate": 2.246875e-05,
      "loss": 0.19,
      "step": 1282
    },
    {
      "bce_eff_w": 0.33245,
      "bce_loss": -0.4584428369998932,
      "epoch": 6.41,
      "step": 1282
    },
    {
      "epoch": 6.415,
      "grad_norm": 18.6158504486084,
      "learning_rate": 2.24375e-05,
      "loss": 0.2577,
      "step": 1283
    },
    {
      "bce_eff_w": 0.3326,
      "bce_loss": -0.4584856331348419,
      "epoch": 6.415,
      "step": 1283
    },
    {
      "epoch": 6.42,
      "grad_norm": 36.1575927734375,
      "learning_rate": 2.2406250000000003e-05,
      "loss": 1.8236,
      "step": 1284
    },
    {
      "bce_eff_w": 0.33275,
      "bce_loss": -0.46240511536598206,
      "epoch": 6.42,
      "step": 1284
    },
    {
      "epoch": 6.425,
      "grad_norm": 11.593395233154297,
      "learning_rate": 2.2375000000000002e-05,
      "loss": 0.032,
      "step": 1285
    },
    {
      "bce_eff_w": 0.3329,
      "bce_loss": -0.46003779768943787,
      "epoch": 6.425,
      "step": 1285
    },
    {
      "epoch": 6.43,
      "grad_norm": 35.03461456298828,
      "learning_rate": 2.234375e-05,
      "loss": 2.3386,
      "step": 1286
    },
    {
      "bce_eff_w": 0.33305,
      "bce_loss": -0.46275442838668823,
      "epoch": 6.43,
      "step": 1286
    },
    {
      "epoch": 6.435,
      "grad_norm": 19.29187774658203,
      "learning_rate": 2.23125e-05,
      "loss": 1.5242,
      "step": 1287
    },
    {
      "bce_eff_w": 0.3332,
      "bce_loss": -0.4642026424407959,
      "epoch": 6.435,
      "step": 1287
    },
    {
      "epoch": 6.44,
      "grad_norm": 16.3630313873291,
      "learning_rate": 2.228125e-05,
      "loss": 1.6099,
      "step": 1288
    },
    {
      "bce_eff_w": 0.33335000000000004,
      "bce_loss": -0.46369668841362,
      "epoch": 6.44,
      "step": 1288
    },
    {
      "epoch": 6.445,
      "grad_norm": 24.04191017150879,
      "learning_rate": 2.2250000000000002e-05,
      "loss": 0.6972,
      "step": 1289
    },
    {
      "bce_eff_w": 0.3335,
      "bce_loss": -0.4579971730709076,
      "epoch": 6.445,
      "step": 1289
    },
    {
      "epoch": 6.45,
      "grad_norm": 2.4841275215148926,
      "learning_rate": 2.221875e-05,
      "loss": -0.1173,
      "step": 1290
    },
    {
      "bce_eff_w": 0.33365,
      "bce_loss": -0.4568440914154053,
      "epoch": 6.45,
      "step": 1290
    },
    {
      "epoch": 6.455,
      "grad_norm": 24.66532325744629,
      "learning_rate": 2.21875e-05,
      "loss": 1.1867,
      "step": 1291
    },
    {
      "bce_eff_w": 0.3338,
      "bce_loss": -0.45975354313850403,
      "epoch": 6.455,
      "step": 1291
    },
    {
      "epoch": 6.46,
      "grad_norm": 22.662841796875,
      "learning_rate": 2.215625e-05,
      "loss": 0.4337,
      "step": 1292
    },
    {
      "bce_eff_w": 0.33394999999999997,
      "bce_loss": -0.4628731310367584,
      "epoch": 6.46,
      "step": 1292
    },
    {
      "epoch": 6.465,
      "grad_norm": 7.119626522064209,
      "learning_rate": 2.2125000000000002e-05,
      "loss": -0.053,
      "step": 1293
    },
    {
      "bce_eff_w": 0.3341,
      "bce_loss": -0.456686794757843,
      "epoch": 6.465,
      "step": 1293
    },
    {
      "epoch": 6.47,
      "grad_norm": 16.46917152404785,
      "learning_rate": 2.2093750000000002e-05,
      "loss": 0.5133,
      "step": 1294
    },
    {
      "bce_eff_w": 0.33425000000000005,
      "bce_loss": -0.4593207836151123,
      "epoch": 6.47,
      "step": 1294
    },
    {
      "epoch": 6.475,
      "grad_norm": 27.45411491394043,
      "learning_rate": 2.20625e-05,
      "loss": 0.555,
      "step": 1295
    },
    {
      "bce_eff_w": 0.33440000000000003,
      "bce_loss": -0.4534764885902405,
      "epoch": 6.475,
      "step": 1295
    },
    {
      "epoch": 6.48,
      "grad_norm": 25.268917083740234,
      "learning_rate": 2.203125e-05,
      "loss": 0.7418,
      "step": 1296
    },
    {
      "bce_eff_w": 0.33455,
      "bce_loss": -0.45895129442214966,
      "epoch": 6.48,
      "step": 1296
    },
    {
      "epoch": 6.485,
      "grad_norm": 14.683990478515625,
      "learning_rate": 2.2000000000000003e-05,
      "loss": 0.2466,
      "step": 1297
    },
    {
      "bce_eff_w": 0.3347,
      "bce_loss": -0.46025288105010986,
      "epoch": 6.485,
      "step": 1297
    },
    {
      "epoch": 6.49,
      "grad_norm": 21.898744583129883,
      "learning_rate": 2.1968750000000002e-05,
      "loss": 0.6484,
      "step": 1298
    },
    {
      "bce_eff_w": 0.33485,
      "bce_loss": -0.4612261652946472,
      "epoch": 6.49,
      "step": 1298
    },
    {
      "epoch": 6.495,
      "grad_norm": 10.40153694152832,
      "learning_rate": 2.19375e-05,
      "loss": 0.0573,
      "step": 1299
    },
    {
      "bce_eff_w": 0.335,
      "bce_loss": -0.46165555715560913,
      "epoch": 6.495,
      "step": 1299
    },
    {
      "epoch": 6.5,
      "grad_norm": 15.701967239379883,
      "learning_rate": 2.190625e-05,
      "loss": 0.4219,
      "step": 1300
    },
    {
      "bce_eff_w": 0.33515,
      "bce_loss": -0.46028396487236023,
      "epoch": 6.5,
      "step": 1300
    },
    {
      "epoch": 6.505,
      "grad_norm": 2.298780918121338,
      "learning_rate": 2.1875e-05,
      "loss": -0.1249,
      "step": 1301
    },
    {
      "bce_eff_w": 0.33530000000000004,
      "bce_loss": -0.45758339762687683,
      "epoch": 6.505,
      "step": 1301
    },
    {
      "epoch": 6.51,
      "grad_norm": 17.025835037231445,
      "learning_rate": 2.1843750000000002e-05,
      "loss": 0.3399,
      "step": 1302
    },
    {
      "bce_eff_w": 0.33545,
      "bce_loss": -0.463888555765152,
      "epoch": 6.51,
      "step": 1302
    },
    {
      "epoch": 6.515,
      "grad_norm": 15.867875099182129,
      "learning_rate": 2.18125e-05,
      "loss": 0.4493,
      "step": 1303
    },
    {
      "bce_eff_w": 0.3356,
      "bce_loss": -0.4613584578037262,
      "epoch": 6.515,
      "step": 1303
    },
    {
      "epoch": 6.52,
      "grad_norm": 20.23706817626953,
      "learning_rate": 2.178125e-05,
      "loss": 0.6224,
      "step": 1304
    },
    {
      "bce_eff_w": 0.33575,
      "bce_loss": -0.4610001742839813,
      "epoch": 6.52,
      "step": 1304
    },
    {
      "epoch": 6.525,
      "grad_norm": 18.651187896728516,
      "learning_rate": 2.175e-05,
      "loss": 2.0704,
      "step": 1305
    },
    {
      "bce_eff_w": 0.3359,
      "bce_loss": -0.4605759382247925,
      "epoch": 6.525,
      "step": 1305
    },
    {
      "epoch": 6.53,
      "grad_norm": 51.139034271240234,
      "learning_rate": 2.1718750000000003e-05,
      "loss": 0.4637,
      "step": 1306
    },
    {
      "bce_eff_w": 0.33605,
      "bce_loss": -0.4641016721725464,
      "epoch": 6.53,
      "step": 1306
    },
    {
      "epoch": 6.535,
      "grad_norm": 15.32381820678711,
      "learning_rate": 2.1687500000000002e-05,
      "loss": 1.1653,
      "step": 1307
    },
    {
      "bce_eff_w": 0.3362,
      "bce_loss": -0.46261200308799744,
      "epoch": 6.535,
      "step": 1307
    },
    {
      "epoch": 6.54,
      "grad_norm": 20.000905990600586,
      "learning_rate": 2.165625e-05,
      "loss": 0.5087,
      "step": 1308
    },
    {
      "bce_eff_w": 0.33635000000000004,
      "bce_loss": -0.46075379848480225,
      "epoch": 6.54,
      "step": 1308
    },
    {
      "epoch": 6.545,
      "grad_norm": 8.805665016174316,
      "learning_rate": 2.1625e-05,
      "loss": -0.0251,
      "step": 1309
    },
    {
      "bce_eff_w": 0.3365,
      "bce_loss": -0.46201178431510925,
      "epoch": 6.545,
      "step": 1309
    },
    {
      "epoch": 6.55,
      "grad_norm": 17.352657318115234,
      "learning_rate": 2.1593750000000003e-05,
      "loss": 0.158,
      "step": 1310
    },
    {
      "bce_eff_w": 0.33665,
      "bce_loss": -0.46370944380760193,
      "epoch": 6.55,
      "step": 1310
    },
    {
      "epoch": 6.555,
      "grad_norm": 23.055601119995117,
      "learning_rate": 2.1562500000000002e-05,
      "loss": 0.4542,
      "step": 1311
    },
    {
      "bce_eff_w": 0.3368,
      "bce_loss": -0.45280349254608154,
      "epoch": 6.555,
      "step": 1311
    },
    {
      "epoch": 6.5600000000000005,
      "grad_norm": 12.207321166992188,
      "learning_rate": 2.153125e-05,
      "loss": 0.3318,
      "step": 1312
    },
    {
      "bce_eff_w": 0.33694999999999997,
      "bce_loss": -0.4632914364337921,
      "epoch": 6.5600000000000005,
      "step": 1312
    },
    {
      "epoch": 6.5649999999999995,
      "grad_norm": 26.155162811279297,
      "learning_rate": 2.15e-05,
      "loss": 0.6632,
      "step": 1313
    },
    {
      "bce_eff_w": 0.3371,
      "bce_loss": -0.4573806822299957,
      "epoch": 6.5649999999999995,
      "step": 1313
    },
    {
      "epoch": 6.57,
      "grad_norm": 21.37012481689453,
      "learning_rate": 2.146875e-05,
      "loss": 0.7794,
      "step": 1314
    },
    {
      "bce_eff_w": 0.33725000000000005,
      "bce_loss": -0.4630374610424042,
      "epoch": 6.57,
      "step": 1314
    },
    {
      "epoch": 6.575,
      "grad_norm": 23.364770889282227,
      "learning_rate": 2.1437500000000003e-05,
      "loss": 1.0871,
      "step": 1315
    },
    {
      "bce_eff_w": 0.33740000000000003,
      "bce_loss": -0.45726945996284485,
      "epoch": 6.575,
      "step": 1315
    },
    {
      "epoch": 6.58,
      "grad_norm": 34.15113830566406,
      "learning_rate": 2.140625e-05,
      "loss": 1.8696,
      "step": 1316
    },
    {
      "bce_eff_w": 0.33755,
      "bce_loss": -0.4578261375427246,
      "epoch": 6.58,
      "step": 1316
    },
    {
      "epoch": 6.585,
      "grad_norm": 16.555156707763672,
      "learning_rate": 2.1375e-05,
      "loss": 1.0043,
      "step": 1317
    },
    {
      "bce_eff_w": 0.3377,
      "bce_loss": -0.45927178859710693,
      "epoch": 6.585,
      "step": 1317
    },
    {
      "epoch": 6.59,
      "grad_norm": 11.967850685119629,
      "learning_rate": 2.134375e-05,
      "loss": -0.0702,
      "step": 1318
    },
    {
      "bce_eff_w": 0.33785,
      "bce_loss": -0.46073609590530396,
      "epoch": 6.59,
      "step": 1318
    },
    {
      "epoch": 6.595,
      "grad_norm": 19.100643157958984,
      "learning_rate": 2.1312500000000003e-05,
      "loss": 1.215,
      "step": 1319
    },
    {
      "bce_eff_w": 0.338,
      "bce_loss": -0.45639148354530334,
      "epoch": 6.595,
      "step": 1319
    },
    {
      "epoch": 6.6,
      "grad_norm": 13.335641860961914,
      "learning_rate": 2.128125e-05,
      "loss": 0.241,
      "step": 1320
    },
    {
      "bce_eff_w": 0.33815,
      "bce_loss": -0.4624740779399872,
      "epoch": 6.6,
      "step": 1320
    },
    {
      "epoch": 6.605,
      "grad_norm": 22.632139205932617,
      "learning_rate": 2.125e-05,
      "loss": 0.9709,
      "step": 1321
    },
    {
      "bce_eff_w": 0.33830000000000005,
      "bce_loss": -0.46056655049324036,
      "epoch": 6.605,
      "step": 1321
    },
    {
      "epoch": 6.61,
      "grad_norm": 26.05342674255371,
      "learning_rate": 2.121875e-05,
      "loss": 0.4994,
      "step": 1322
    },
    {
      "bce_eff_w": 0.33845000000000003,
      "bce_loss": -0.4625720977783203,
      "epoch": 6.61,
      "step": 1322
    },
    {
      "epoch": 6.615,
      "grad_norm": 18.310344696044922,
      "learning_rate": 2.1187500000000003e-05,
      "loss": 0.8681,
      "step": 1323
    },
    {
      "bce_eff_w": 0.3386,
      "bce_loss": -0.4602679908275604,
      "epoch": 6.615,
      "step": 1323
    },
    {
      "epoch": 6.62,
      "grad_norm": 27.714147567749023,
      "learning_rate": 2.115625e-05,
      "loss": 1.2727,
      "step": 1324
    },
    {
      "bce_eff_w": 0.33875,
      "bce_loss": -0.45999109745025635,
      "epoch": 6.62,
      "step": 1324
    },
    {
      "epoch": 6.625,
      "grad_norm": 19.14484977722168,
      "learning_rate": 2.1125000000000002e-05,
      "loss": 1.0568,
      "step": 1325
    },
    {
      "bce_eff_w": 0.3389,
      "bce_loss": -0.4610270857810974,
      "epoch": 6.625,
      "step": 1325
    },
    {
      "epoch": 6.63,
      "grad_norm": 18.702861785888672,
      "learning_rate": 2.109375e-05,
      "loss": 0.3567,
      "step": 1326
    },
    {
      "bce_eff_w": 0.33905,
      "bce_loss": -0.45819151401519775,
      "epoch": 6.63,
      "step": 1326
    },
    {
      "epoch": 6.635,
      "grad_norm": 17.21250343322754,
      "learning_rate": 2.10625e-05,
      "loss": 1.2281,
      "step": 1327
    },
    {
      "bce_eff_w": 0.3392,
      "bce_loss": -0.46341225504875183,
      "epoch": 6.635,
      "step": 1327
    },
    {
      "epoch": 6.64,
      "grad_norm": 21.446544647216797,
      "learning_rate": 2.1031250000000003e-05,
      "loss": 0.4656,
      "step": 1328
    },
    {
      "bce_eff_w": 0.33935000000000004,
      "bce_loss": -0.4628221392631531,
      "epoch": 6.64,
      "step": 1328
    },
    {
      "epoch": 6.645,
      "grad_norm": 23.6538143157959,
      "learning_rate": 2.1e-05,
      "loss": 1.6274,
      "step": 1329
    },
    {
      "bce_eff_w": 0.3395,
      "bce_loss": -0.45599937438964844,
      "epoch": 6.645,
      "step": 1329
    },
    {
      "epoch": 6.65,
      "grad_norm": 12.303586959838867,
      "learning_rate": 2.096875e-05,
      "loss": 0.0312,
      "step": 1330
    },
    {
      "bce_eff_w": 0.33965,
      "bce_loss": -0.4621606469154358,
      "epoch": 6.65,
      "step": 1330
    },
    {
      "epoch": 6.655,
      "grad_norm": 22.89303207397461,
      "learning_rate": 2.09375e-05,
      "loss": 0.4263,
      "step": 1331
    },
    {
      "bce_eff_w": 0.3398,
      "bce_loss": -0.46405231952667236,
      "epoch": 6.655,
      "step": 1331
    },
    {
      "epoch": 6.66,
      "grad_norm": 14.234845161437988,
      "learning_rate": 2.0906250000000003e-05,
      "loss": 1.0962,
      "step": 1332
    },
    {
      "bce_eff_w": 0.33995,
      "bce_loss": -0.4591275751590729,
      "epoch": 6.66,
      "step": 1332
    },
    {
      "epoch": 6.665,
      "grad_norm": 16.415029525756836,
      "learning_rate": 2.0875e-05,
      "loss": 1.7291,
      "step": 1333
    },
    {
      "bce_eff_w": 0.3401,
      "bce_loss": -0.46181634068489075,
      "epoch": 6.665,
      "step": 1333
    },
    {
      "epoch": 6.67,
      "grad_norm": 22.856599807739258,
      "learning_rate": 2.0843750000000002e-05,
      "loss": 1.3269,
      "step": 1334
    },
    {
      "bce_eff_w": 0.34025000000000005,
      "bce_loss": -0.44946232438087463,
      "epoch": 6.67,
      "step": 1334
    },
    {
      "epoch": 6.675,
      "grad_norm": 14.068549156188965,
      "learning_rate": 2.08125e-05,
      "loss": 0.3928,
      "step": 1335
    },
    {
      "bce_eff_w": 0.34040000000000004,
      "bce_loss": -0.46138468384742737,
      "epoch": 6.675,
      "step": 1335
    },
    {
      "epoch": 6.68,
      "grad_norm": 18.801618576049805,
      "learning_rate": 2.0781250000000004e-05,
      "loss": 0.8761,
      "step": 1336
    },
    {
      "bce_eff_w": 0.34055,
      "bce_loss": -0.46032553911209106,
      "epoch": 6.68,
      "step": 1336
    },
    {
      "epoch": 6.6850000000000005,
      "grad_norm": 17.032257080078125,
      "learning_rate": 2.075e-05,
      "loss": 1.0991,
      "step": 1337
    },
    {
      "bce_eff_w": 0.3407,
      "bce_loss": -0.45886850357055664,
      "epoch": 6.6850000000000005,
      "step": 1337
    },
    {
      "epoch": 6.6899999999999995,
      "grad_norm": 15.372579574584961,
      "learning_rate": 2.0718750000000002e-05,
      "loss": 0.7481,
      "step": 1338
    },
    {
      "bce_eff_w": 0.34085,
      "bce_loss": -0.4491376280784607,
      "epoch": 6.6899999999999995,
      "step": 1338
    },
    {
      "epoch": 6.695,
      "grad_norm": 23.784809112548828,
      "learning_rate": 2.06875e-05,
      "loss": 1.196,
      "step": 1339
    },
    {
      "bce_eff_w": 0.34099999999999997,
      "bce_loss": -0.4596249461174011,
      "epoch": 6.695,
      "step": 1339
    },
    {
      "epoch": 6.7,
      "grad_norm": 38.44112014770508,
      "learning_rate": 2.065625e-05,
      "loss": 1.2858,
      "step": 1340
    },
    {
      "bce_eff_w": 0.34115,
      "bce_loss": -0.4604965150356293,
      "epoch": 6.7,
      "step": 1340
    },
    {
      "epoch": 6.705,
      "grad_norm": 29.193714141845703,
      "learning_rate": 2.0625e-05,
      "loss": 0.8872,
      "step": 1341
    },
    {
      "bce_eff_w": 0.3413,
      "bce_loss": -0.46446841955184937,
      "epoch": 6.705,
      "step": 1341
    },
    {
      "epoch": 6.71,
      "grad_norm": 16.32566261291504,
      "learning_rate": 2.059375e-05,
      "loss": 0.2754,
      "step": 1342
    },
    {
      "bce_eff_w": 0.34145000000000003,
      "bce_loss": -0.45578229427337646,
      "epoch": 6.71,
      "step": 1342
    },
    {
      "epoch": 6.715,
      "grad_norm": 25.614778518676758,
      "learning_rate": 2.0562500000000002e-05,
      "loss": 0.6892,
      "step": 1343
    },
    {
      "bce_eff_w": 0.3416,
      "bce_loss": -0.4635515809059143,
      "epoch": 6.715,
      "step": 1343
    },
    {
      "epoch": 6.72,
      "grad_norm": 29.12864112854004,
      "learning_rate": 2.053125e-05,
      "loss": 0.2957,
      "step": 1344
    },
    {
      "bce_eff_w": 0.34175,
      "bce_loss": -0.4591998755931854,
      "epoch": 6.72,
      "step": 1344
    },
    {
      "epoch": 6.725,
      "grad_norm": 19.290241241455078,
      "learning_rate": 2.05e-05,
      "loss": 0.1829,
      "step": 1345
    },
    {
      "bce_eff_w": 0.3419,
      "bce_loss": -0.4615859389305115,
      "epoch": 6.725,
      "step": 1345
    },
    {
      "epoch": 6.73,
      "grad_norm": 18.427186965942383,
      "learning_rate": 2.046875e-05,
      "loss": 0.4356,
      "step": 1346
    },
    {
      "bce_eff_w": 0.34204999999999997,
      "bce_loss": -0.4631803333759308,
      "epoch": 6.73,
      "step": 1346
    },
    {
      "epoch": 6.735,
      "grad_norm": 19.229768753051758,
      "learning_rate": 2.0437500000000002e-05,
      "loss": 1.3744,
      "step": 1347
    },
    {
      "bce_eff_w": 0.3422,
      "bce_loss": -0.44713911414146423,
      "epoch": 6.735,
      "step": 1347
    },
    {
      "epoch": 6.74,
      "grad_norm": 6.404155254364014,
      "learning_rate": 2.040625e-05,
      "loss": -0.019,
      "step": 1348
    },
    {
      "bce_eff_w": 0.34235,
      "bce_loss": -0.461148738861084,
      "epoch": 6.74,
      "step": 1348
    },
    {
      "epoch": 6.745,
      "grad_norm": 17.950992584228516,
      "learning_rate": 2.0375e-05,
      "loss": 0.9899,
      "step": 1349
    },
    {
      "bce_eff_w": 0.3425,
      "bce_loss": -0.4608304798603058,
      "epoch": 6.745,
      "step": 1349
    },
    {
      "epoch": 6.75,
      "grad_norm": 51.60941696166992,
      "learning_rate": 2.034375e-05,
      "loss": 1.2086,
      "step": 1350
    },
    {
      "bce_eff_w": 0.34265,
      "bce_loss": -0.4586946368217468,
      "epoch": 6.75,
      "step": 1350
    },
    {
      "epoch": 6.755,
      "grad_norm": 10.19853687286377,
      "learning_rate": 2.0312500000000002e-05,
      "loss": -0.0222,
      "step": 1351
    },
    {
      "bce_eff_w": 0.3428,
      "bce_loss": -0.4597477316856384,
      "epoch": 6.755,
      "step": 1351
    },
    {
      "epoch": 6.76,
      "grad_norm": 16.621606826782227,
      "learning_rate": 2.0281250000000002e-05,
      "loss": 0.5213,
      "step": 1352
    },
    {
      "bce_eff_w": 0.34295,
      "bce_loss": -0.4642251431941986,
      "epoch": 6.76,
      "step": 1352
    },
    {
      "epoch": 6.765,
      "grad_norm": 19.477571487426758,
      "learning_rate": 2.025e-05,
      "loss": 0.206,
      "step": 1353
    },
    {
      "bce_eff_w": 0.34309999999999996,
      "bce_loss": -0.45858368277549744,
      "epoch": 6.765,
      "step": 1353
    },
    {
      "epoch": 6.77,
      "grad_norm": 21.269641876220703,
      "learning_rate": 2.021875e-05,
      "loss": 1.4776,
      "step": 1354
    },
    {
      "bce_eff_w": 0.34325,
      "bce_loss": -0.45791107416152954,
      "epoch": 6.77,
      "step": 1354
    },
    {
      "epoch": 6.775,
      "grad_norm": 22.060640335083008,
      "learning_rate": 2.01875e-05,
      "loss": 0.8171,
      "step": 1355
    },
    {
      "bce_eff_w": 0.34340000000000004,
      "bce_loss": -0.46204593777656555,
      "epoch": 6.775,
      "step": 1355
    },
    {
      "epoch": 6.78,
      "grad_norm": 19.417129516601562,
      "learning_rate": 2.0156250000000002e-05,
      "loss": 1.4114,
      "step": 1356
    },
    {
      "bce_eff_w": 0.34355,
      "bce_loss": -0.46341338753700256,
      "epoch": 6.78,
      "step": 1356
    },
    {
      "epoch": 6.785,
      "grad_norm": 22.267818450927734,
      "learning_rate": 2.0125e-05,
      "loss": 0.3668,
      "step": 1357
    },
    {
      "bce_eff_w": 0.3437,
      "bce_loss": -0.46173471212387085,
      "epoch": 6.785,
      "step": 1357
    },
    {
      "epoch": 6.79,
      "grad_norm": 22.58807945251465,
      "learning_rate": 2.009375e-05,
      "loss": 0.5143,
      "step": 1358
    },
    {
      "bce_eff_w": 0.34385,
      "bce_loss": -0.4625699818134308,
      "epoch": 6.79,
      "step": 1358
    },
    {
      "epoch": 6.795,
      "grad_norm": 17.18545913696289,
      "learning_rate": 2.00625e-05,
      "loss": 0.8316,
      "step": 1359
    },
    {
      "bce_eff_w": 0.344,
      "bce_loss": -0.46324220299720764,
      "epoch": 6.795,
      "step": 1359
    },
    {
      "epoch": 6.8,
      "grad_norm": 16.649253845214844,
      "learning_rate": 2.0031250000000002e-05,
      "loss": 0.6426,
      "step": 1360
    },
    {
      "bce_eff_w": 0.34415,
      "bce_loss": -0.45767414569854736,
      "epoch": 6.8,
      "step": 1360
    },
    {
      "epoch": 6.805,
      "grad_norm": 28.176225662231445,
      "learning_rate": 2e-05,
      "loss": 0.701,
      "step": 1361
    },
    {
      "bce_eff_w": 0.3443,
      "bce_loss": -0.46322304010391235,
      "epoch": 6.805,
      "step": 1361
    },
    {
      "epoch": 6.8100000000000005,
      "grad_norm": 20.124134063720703,
      "learning_rate": 1.996875e-05,
      "loss": 0.5369,
      "step": 1362
    },
    {
      "bce_eff_w": 0.34445000000000003,
      "bce_loss": -0.4631945788860321,
      "epoch": 6.8100000000000005,
      "step": 1362
    },
    {
      "epoch": 6.8149999999999995,
      "grad_norm": 17.768020629882812,
      "learning_rate": 1.99375e-05,
      "loss": 1.4894,
      "step": 1363
    },
    {
      "bce_eff_w": 0.3446,
      "bce_loss": -0.4597187638282776,
      "epoch": 6.8149999999999995,
      "step": 1363
    },
    {
      "epoch": 6.82,
      "grad_norm": 15.125493049621582,
      "learning_rate": 1.9906250000000003e-05,
      "loss": 2.4214,
      "step": 1364
    },
    {
      "bce_eff_w": 0.34475,
      "bce_loss": -0.46229010820388794,
      "epoch": 6.82,
      "step": 1364
    },
    {
      "epoch": 6.825,
      "grad_norm": 18.873849868774414,
      "learning_rate": 1.9875000000000002e-05,
      "loss": 0.2366,
      "step": 1365
    },
    {
      "bce_eff_w": 0.3449,
      "bce_loss": -0.45915260910987854,
      "epoch": 6.825,
      "step": 1365
    },
    {
      "epoch": 6.83,
      "grad_norm": 27.40498924255371,
      "learning_rate": 1.984375e-05,
      "loss": 0.8711,
      "step": 1366
    },
    {
      "bce_eff_w": 0.34504999999999997,
      "bce_loss": -0.4576593041419983,
      "epoch": 6.83,
      "step": 1366
    },
    {
      "epoch": 6.835,
      "grad_norm": 16.09658432006836,
      "learning_rate": 1.98125e-05,
      "loss": 0.4839,
      "step": 1367
    },
    {
      "bce_eff_w": 0.3452,
      "bce_loss": -0.46202680468559265,
      "epoch": 6.835,
      "step": 1367
    },
    {
      "epoch": 6.84,
      "grad_norm": 19.891088485717773,
      "learning_rate": 1.978125e-05,
      "loss": 1.8157,
      "step": 1368
    },
    {
      "bce_eff_w": 0.34535,
      "bce_loss": -0.4523499011993408,
      "epoch": 6.84,
      "step": 1368
    },
    {
      "epoch": 6.845,
      "grad_norm": 11.421292304992676,
      "learning_rate": 1.9750000000000002e-05,
      "loss": 0.2,
      "step": 1369
    },
    {
      "bce_eff_w": 0.34550000000000003,
      "bce_loss": -0.46366894245147705,
      "epoch": 6.845,
      "step": 1369
    },
    {
      "epoch": 6.85,
      "grad_norm": 13.898343086242676,
      "learning_rate": 1.9718749999999998e-05,
      "loss": -0.0248,
      "step": 1370
    },
    {
      "bce_eff_w": 0.34565,
      "bce_loss": -0.4559950530529022,
      "epoch": 6.85,
      "step": 1370
    },
    {
      "epoch": 6.855,
      "grad_norm": 17.46758270263672,
      "learning_rate": 1.96875e-05,
      "loss": 0.4391,
      "step": 1371
    },
    {
      "bce_eff_w": 0.3458,
      "bce_loss": -0.4617471694946289,
      "epoch": 6.855,
      "step": 1371
    },
    {
      "epoch": 6.86,
      "grad_norm": 20.47684097290039,
      "learning_rate": 1.965625e-05,
      "loss": 0.8383,
      "step": 1372
    },
    {
      "bce_eff_w": 0.34595,
      "bce_loss": -0.45786258578300476,
      "epoch": 6.86,
      "step": 1372
    },
    {
      "epoch": 6.865,
      "grad_norm": 20.329683303833008,
      "learning_rate": 1.9625000000000003e-05,
      "loss": 0.1501,
      "step": 1373
    },
    {
      "bce_eff_w": 0.34609999999999996,
      "bce_loss": -0.46119242906570435,
      "epoch": 6.865,
      "step": 1373
    },
    {
      "epoch": 6.87,
      "grad_norm": 20.526260375976562,
      "learning_rate": 1.959375e-05,
      "loss": 0.7637,
      "step": 1374
    },
    {
      "bce_eff_w": 0.34625,
      "bce_loss": -0.4630356729030609,
      "epoch": 6.87,
      "step": 1374
    },
    {
      "epoch": 6.875,
      "grad_norm": 26.28287696838379,
      "learning_rate": 1.95625e-05,
      "loss": 0.2004,
      "step": 1375
    },
    {
      "bce_eff_w": 0.34640000000000004,
      "bce_loss": -0.4635776877403259,
      "epoch": 6.875,
      "step": 1375
    },
    {
      "epoch": 6.88,
      "grad_norm": 25.906002044677734,
      "learning_rate": 1.953125e-05,
      "loss": 1.0644,
      "step": 1376
    },
    {
      "bce_eff_w": 0.34655,
      "bce_loss": -0.4615701138973236,
      "epoch": 6.88,
      "step": 1376
    },
    {
      "epoch": 6.885,
      "grad_norm": 11.359502792358398,
      "learning_rate": 1.9500000000000003e-05,
      "loss": -0.0132,
      "step": 1377
    },
    {
      "bce_eff_w": 0.3467,
      "bce_loss": -0.4601401686668396,
      "epoch": 6.885,
      "step": 1377
    },
    {
      "epoch": 6.89,
      "grad_norm": 29.37994956970215,
      "learning_rate": 1.9468750000000002e-05,
      "loss": 1.1395,
      "step": 1378
    },
    {
      "bce_eff_w": 0.34685,
      "bce_loss": -0.46361416578292847,
      "epoch": 6.89,
      "step": 1378
    },
    {
      "epoch": 6.895,
      "grad_norm": 16.611587524414062,
      "learning_rate": 1.94375e-05,
      "loss": 0.8208,
      "step": 1379
    },
    {
      "bce_eff_w": 0.347,
      "bce_loss": -0.4624418616294861,
      "epoch": 6.895,
      "step": 1379
    },
    {
      "epoch": 6.9,
      "grad_norm": 26.597835540771484,
      "learning_rate": 1.940625e-05,
      "loss": 1.1799,
      "step": 1380
    },
    {
      "bce_eff_w": 0.34715,
      "bce_loss": -0.45378628373146057,
      "epoch": 6.9,
      "step": 1380
    },
    {
      "epoch": 6.905,
      "grad_norm": 17.16836929321289,
      "learning_rate": 1.9375e-05,
      "loss": 0.215,
      "step": 1381
    },
    {
      "bce_eff_w": 0.3473,
      "bce_loss": -0.45434391498565674,
      "epoch": 6.905,
      "step": 1381
    },
    {
      "epoch": 6.91,
      "grad_norm": 11.752737045288086,
      "learning_rate": 1.9343750000000003e-05,
      "loss": 0.0382,
      "step": 1382
    },
    {
      "bce_eff_w": 0.34745000000000004,
      "bce_loss": -0.46101635694503784,
      "epoch": 6.91,
      "step": 1382
    },
    {
      "epoch": 6.915,
      "grad_norm": 41.44852066040039,
      "learning_rate": 1.93125e-05,
      "loss": 0.6362,
      "step": 1383
    },
    {
      "bce_eff_w": 0.3476,
      "bce_loss": -0.4645296037197113,
      "epoch": 6.915,
      "step": 1383
    },
    {
      "epoch": 6.92,
      "grad_norm": 15.99800968170166,
      "learning_rate": 1.928125e-05,
      "loss": 1.0482,
      "step": 1384
    },
    {
      "bce_eff_w": 0.34775,
      "bce_loss": -0.46163713932037354,
      "epoch": 6.92,
      "step": 1384
    },
    {
      "epoch": 6.925,
      "grad_norm": 31.281110763549805,
      "learning_rate": 1.925e-05,
      "loss": 0.6886,
      "step": 1385
    },
    {
      "bce_eff_w": 0.3479,
      "bce_loss": -0.46272024512290955,
      "epoch": 6.925,
      "step": 1385
    },
    {
      "epoch": 6.93,
      "grad_norm": 21.603479385375977,
      "learning_rate": 1.9218750000000003e-05,
      "loss": 0.5946,
      "step": 1386
    },
    {
      "bce_eff_w": 0.34804999999999997,
      "bce_loss": -0.45704570412635803,
      "epoch": 6.93,
      "step": 1386
    },
    {
      "epoch": 6.9350000000000005,
      "grad_norm": 19.07032585144043,
      "learning_rate": 1.91875e-05,
      "loss": 0.2723,
      "step": 1387
    },
    {
      "bce_eff_w": 0.3482,
      "bce_loss": -0.4632906913757324,
      "epoch": 6.9350000000000005,
      "step": 1387
    },
    {
      "epoch": 6.9399999999999995,
      "grad_norm": 17.01536750793457,
      "learning_rate": 1.915625e-05,
      "loss": 1.1685,
      "step": 1388
    },
    {
      "bce_eff_w": 0.34835,
      "bce_loss": -0.46092164516448975,
      "epoch": 6.9399999999999995,
      "step": 1388
    },
    {
      "epoch": 6.945,
      "grad_norm": 13.588726043701172,
      "learning_rate": 1.9125e-05,
      "loss": 0.0292,
      "step": 1389
    },
    {
      "bce_eff_w": 0.34850000000000003,
      "bce_loss": -0.4615131616592407,
      "epoch": 6.945,
      "step": 1389
    },
    {
      "epoch": 6.95,
      "grad_norm": 18.397109985351562,
      "learning_rate": 1.9093750000000003e-05,
      "loss": 0.0943,
      "step": 1390
    },
    {
      "bce_eff_w": 0.34865,
      "bce_loss": -0.4552185833454132,
      "epoch": 6.95,
      "step": 1390
    },
    {
      "epoch": 6.955,
      "grad_norm": 22.01930809020996,
      "learning_rate": 1.90625e-05,
      "loss": 1.9036,
      "step": 1391
    },
    {
      "bce_eff_w": 0.3488,
      "bce_loss": -0.45759183168411255,
      "epoch": 6.955,
      "step": 1391
    },
    {
      "epoch": 6.96,
      "grad_norm": 17.071504592895508,
      "learning_rate": 1.9031250000000002e-05,
      "loss": 0.4588,
      "step": 1392
    },
    {
      "bce_eff_w": 0.34895,
      "bce_loss": -0.4625627398490906,
      "epoch": 6.96,
      "step": 1392
    },
    {
      "epoch": 6.965,
      "grad_norm": 23.968231201171875,
      "learning_rate": 1.9e-05,
      "loss": 1.5698,
      "step": 1393
    },
    {
      "bce_eff_w": 0.34909999999999997,
      "bce_loss": -0.46366438269615173,
      "epoch": 6.965,
      "step": 1393
    },
    {
      "epoch": 6.97,
      "grad_norm": 17.03564453125,
      "learning_rate": 1.896875e-05,
      "loss": 1.08,
      "step": 1394
    },
    {
      "bce_eff_w": 0.34925,
      "bce_loss": -0.45239049196243286,
      "epoch": 6.97,
      "step": 1394
    },
    {
      "epoch": 6.975,
      "grad_norm": 19.51877212524414,
      "learning_rate": 1.89375e-05,
      "loss": 0.5347,
      "step": 1395
    },
    {
      "bce_eff_w": 0.34940000000000004,
      "bce_loss": -0.4562147557735443,
      "epoch": 6.975,
      "step": 1395
    },
    {
      "epoch": 6.98,
      "grad_norm": 26.4887638092041,
      "learning_rate": 1.890625e-05,
      "loss": 1.5135,
      "step": 1396
    },
    {
      "bce_eff_w": 0.34955,
      "bce_loss": -0.4574502408504486,
      "epoch": 6.98,
      "step": 1396
    },
    {
      "epoch": 6.985,
      "grad_norm": 19.401933670043945,
      "learning_rate": 1.8875e-05,
      "loss": 0.253,
      "step": 1397
    },
    {
      "bce_eff_w": 0.3497,
      "bce_loss": -0.4609503448009491,
      "epoch": 6.985,
      "step": 1397
    },
    {
      "epoch": 6.99,
      "grad_norm": 17.1055908203125,
      "learning_rate": 1.884375e-05,
      "loss": 0.2026,
      "step": 1398
    },
    {
      "bce_eff_w": 0.34985,
      "bce_loss": -0.459278404712677,
      "epoch": 6.99,
      "step": 1398
    },
    {
      "epoch": 6.995,
      "grad_norm": 22.138954162597656,
      "learning_rate": 1.88125e-05,
      "loss": 0.6911,
      "step": 1399
    },
    {
      "bce_eff_w": 0.35,
      "bce_loss": -0.4603746831417084,
      "epoch": 6.995,
      "step": 1399
    },
    {
      "epoch": 7.0,
      "grad_norm": 13.59615707397461,
      "learning_rate": 1.878125e-05,
      "loss": 2.3877,
      "step": 1400
    },
    {
      "bce_eff_w": 0.35014999999999996,
      "bce_loss": -0.4596910774707794,
      "epoch": 7.0,
      "step": 1400
    },
    {
      "epoch": 7.005,
      "grad_norm": 9.075958251953125,
      "learning_rate": 1.8750000000000002e-05,
      "loss": -0.0748,
      "step": 1401
    },
    {
      "bce_eff_w": 0.3503,
      "bce_loss": -0.4615735411643982,
      "epoch": 7.005,
      "step": 1401
    },
    {
      "epoch": 7.01,
      "grad_norm": 16.800243377685547,
      "learning_rate": 1.871875e-05,
      "loss": 0.2257,
      "step": 1402
    },
    {
      "bce_eff_w": 0.35045,
      "bce_loss": -0.4601303040981293,
      "epoch": 7.01,
      "step": 1402
    },
    {
      "epoch": 7.015,
      "grad_norm": 1.1450707912445068,
      "learning_rate": 1.8687500000000004e-05,
      "loss": -0.1497,
      "step": 1403
    },
    {
      "bce_eff_w": 0.3506,
      "bce_loss": -0.458800345659256,
      "epoch": 7.015,
      "step": 1403
    },
    {
      "epoch": 7.02,
      "grad_norm": 17.87783432006836,
      "learning_rate": 1.865625e-05,
      "loss": 0.5245,
      "step": 1404
    },
    {
      "bce_eff_w": 0.35075,
      "bce_loss": -0.45793452858924866,
      "epoch": 7.02,
      "step": 1404
    },
    {
      "epoch": 7.025,
      "grad_norm": 15.964210510253906,
      "learning_rate": 1.8625000000000002e-05,
      "loss": 0.4273,
      "step": 1405
    },
    {
      "bce_eff_w": 0.3509,
      "bce_loss": -0.4621596932411194,
      "epoch": 7.025,
      "step": 1405
    },
    {
      "epoch": 7.03,
      "grad_norm": 17.651418685913086,
      "learning_rate": 1.859375e-05,
      "loss": 1.2244,
      "step": 1406
    },
    {
      "bce_eff_w": 0.35105,
      "bce_loss": -0.461712509393692,
      "epoch": 7.03,
      "step": 1406
    },
    {
      "epoch": 7.035,
      "grad_norm": 19.994461059570312,
      "learning_rate": 1.85625e-05,
      "loss": 0.5371,
      "step": 1407
    },
    {
      "bce_eff_w": 0.3512,
      "bce_loss": -0.4604359269142151,
      "epoch": 7.035,
      "step": 1407
    },
    {
      "epoch": 7.04,
      "grad_norm": 24.00853157043457,
      "learning_rate": 1.853125e-05,
      "loss": 1.5518,
      "step": 1408
    },
    {
      "bce_eff_w": 0.35135,
      "bce_loss": -0.46145522594451904,
      "epoch": 7.04,
      "step": 1408
    },
    {
      "epoch": 7.045,
      "grad_norm": 10.430869102478027,
      "learning_rate": 1.85e-05,
      "loss": 0.0034,
      "step": 1409
    },
    {
      "bce_eff_w": 0.35150000000000003,
      "bce_loss": -0.4613398611545563,
      "epoch": 7.045,
      "step": 1409
    },
    {
      "epoch": 7.05,
      "grad_norm": 9.517243385314941,
      "learning_rate": 1.846875e-05,
      "loss": -0.0427,
      "step": 1410
    },
    {
      "bce_eff_w": 0.35165,
      "bce_loss": -0.45789748430252075,
      "epoch": 7.05,
      "step": 1410
    },
    {
      "epoch": 7.055,
      "grad_norm": 25.124670028686523,
      "learning_rate": 1.84375e-05,
      "loss": 0.5065,
      "step": 1411
    },
    {
      "bce_eff_w": 0.3518,
      "bce_loss": -0.46042850613594055,
      "epoch": 7.055,
      "step": 1411
    },
    {
      "epoch": 7.06,
      "grad_norm": 6.084343433380127,
      "learning_rate": 1.840625e-05,
      "loss": -0.0573,
      "step": 1412
    },
    {
      "bce_eff_w": 0.35195,
      "bce_loss": -0.46388524770736694,
      "epoch": 7.06,
      "step": 1412
    },
    {
      "epoch": 7.065,
      "grad_norm": 6.811134338378906,
      "learning_rate": 1.8375e-05,
      "loss": -0.036,
      "step": 1413
    },
    {
      "bce_eff_w": 0.35209999999999997,
      "bce_loss": -0.4584014117717743,
      "epoch": 7.065,
      "step": 1413
    },
    {
      "epoch": 7.07,
      "grad_norm": 20.273284912109375,
      "learning_rate": 1.8343750000000002e-05,
      "loss": 1.1158,
      "step": 1414
    },
    {
      "bce_eff_w": 0.35224999999999995,
      "bce_loss": -0.46034902334213257,
      "epoch": 7.07,
      "step": 1414
    },
    {
      "epoch": 7.075,
      "grad_norm": 12.409748077392578,
      "learning_rate": 1.83125e-05,
      "loss": -0.0088,
      "step": 1415
    },
    {
      "bce_eff_w": 0.35240000000000005,
      "bce_loss": -0.4617816209793091,
      "epoch": 7.075,
      "step": 1415
    },
    {
      "epoch": 7.08,
      "grad_norm": 10.460954666137695,
      "learning_rate": 1.828125e-05,
      "loss": 0.0252,
      "step": 1416
    },
    {
      "bce_eff_w": 0.35255000000000003,
      "bce_loss": -0.4573342800140381,
      "epoch": 7.08,
      "step": 1416
    },
    {
      "epoch": 7.085,
      "grad_norm": 19.205413818359375,
      "learning_rate": 1.825e-05,
      "loss": 0.6962,
      "step": 1417
    },
    {
      "bce_eff_w": 0.3527,
      "bce_loss": -0.46095913648605347,
      "epoch": 7.085,
      "step": 1417
    },
    {
      "epoch": 7.09,
      "grad_norm": 7.453756332397461,
      "learning_rate": 1.8218750000000002e-05,
      "loss": -0.046,
      "step": 1418
    },
    {
      "bce_eff_w": 0.35285,
      "bce_loss": -0.4644896686077118,
      "epoch": 7.09,
      "step": 1418
    },
    {
      "epoch": 7.095,
      "grad_norm": 18.196720123291016,
      "learning_rate": 1.81875e-05,
      "loss": 1.0733,
      "step": 1419
    },
    {
      "bce_eff_w": 0.353,
      "bce_loss": -0.46185341477394104,
      "epoch": 7.095,
      "step": 1419
    },
    {
      "epoch": 7.1,
      "grad_norm": 15.631611824035645,
      "learning_rate": 1.815625e-05,
      "loss": 1.2653,
      "step": 1420
    },
    {
      "bce_eff_w": 0.35314999999999996,
      "bce_loss": -0.4604418873786926,
      "epoch": 7.1,
      "step": 1420
    },
    {
      "epoch": 7.105,
      "grad_norm": 21.788496017456055,
      "learning_rate": 1.8125e-05,
      "loss": 0.7508,
      "step": 1421
    },
    {
      "bce_eff_w": 0.3533,
      "bce_loss": -0.460035502910614,
      "epoch": 7.105,
      "step": 1421
    },
    {
      "epoch": 7.11,
      "grad_norm": 13.842945098876953,
      "learning_rate": 1.809375e-05,
      "loss": 0.3166,
      "step": 1422
    },
    {
      "bce_eff_w": 0.35345,
      "bce_loss": -0.4579825699329376,
      "epoch": 7.11,
      "step": 1422
    },
    {
      "epoch": 7.115,
      "grad_norm": 26.405536651611328,
      "learning_rate": 1.8062500000000002e-05,
      "loss": 0.4872,
      "step": 1423
    },
    {
      "bce_eff_w": 0.3536,
      "bce_loss": -0.46307119727134705,
      "epoch": 7.115,
      "step": 1423
    },
    {
      "epoch": 7.12,
      "grad_norm": 5.7461466789245605,
      "learning_rate": 1.803125e-05,
      "loss": -0.0645,
      "step": 1424
    },
    {
      "bce_eff_w": 0.35375,
      "bce_loss": -0.45883339643478394,
      "epoch": 7.12,
      "step": 1424
    },
    {
      "epoch": 7.125,
      "grad_norm": 16.15097999572754,
      "learning_rate": 1.8e-05,
      "loss": 0.5128,
      "step": 1425
    },
    {
      "bce_eff_w": 0.3539,
      "bce_loss": -0.45683589577674866,
      "epoch": 7.125,
      "step": 1425
    },
    {
      "epoch": 7.13,
      "grad_norm": 13.1885404586792,
      "learning_rate": 1.796875e-05,
      "loss": 0.1423,
      "step": 1426
    },
    {
      "bce_eff_w": 0.35405,
      "bce_loss": -0.462132066488266,
      "epoch": 7.13,
      "step": 1426
    },
    {
      "epoch": 7.135,
      "grad_norm": 20.321054458618164,
      "learning_rate": 1.7937500000000002e-05,
      "loss": 1.3396,
      "step": 1427
    },
    {
      "bce_eff_w": 0.3542,
      "bce_loss": -0.46210676431655884,
      "epoch": 7.135,
      "step": 1427
    },
    {
      "epoch": 7.14,
      "grad_norm": 14.355317115783691,
      "learning_rate": 1.790625e-05,
      "loss": 0.1105,
      "step": 1428
    },
    {
      "bce_eff_w": 0.35435,
      "bce_loss": -0.46034401655197144,
      "epoch": 7.14,
      "step": 1428
    },
    {
      "epoch": 7.145,
      "grad_norm": 18.71769905090332,
      "learning_rate": 1.7875e-05,
      "loss": 0.5178,
      "step": 1429
    },
    {
      "bce_eff_w": 0.35450000000000004,
      "bce_loss": -0.4628830850124359,
      "epoch": 7.145,
      "step": 1429
    },
    {
      "epoch": 7.15,
      "grad_norm": 20.056312561035156,
      "learning_rate": 1.784375e-05,
      "loss": 0.2969,
      "step": 1430
    },
    {
      "bce_eff_w": 0.35465,
      "bce_loss": -0.4620831310749054,
      "epoch": 7.15,
      "step": 1430
    },
    {
      "epoch": 7.155,
      "grad_norm": 16.512924194335938,
      "learning_rate": 1.7812500000000003e-05,
      "loss": 0.5636,
      "step": 1431
    },
    {
      "bce_eff_w": 0.3548,
      "bce_loss": -0.45448148250579834,
      "epoch": 7.155,
      "step": 1431
    },
    {
      "epoch": 7.16,
      "grad_norm": 20.62710189819336,
      "learning_rate": 1.7781250000000002e-05,
      "loss": 0.7851,
      "step": 1432
    },
    {
      "bce_eff_w": 0.35495,
      "bce_loss": -0.45542898774147034,
      "epoch": 7.16,
      "step": 1432
    },
    {
      "epoch": 7.165,
      "grad_norm": 22.832666397094727,
      "learning_rate": 1.775e-05,
      "loss": 1.2413,
      "step": 1433
    },
    {
      "bce_eff_w": 0.35509999999999997,
      "bce_loss": -0.46258988976478577,
      "epoch": 7.165,
      "step": 1433
    },
    {
      "epoch": 7.17,
      "grad_norm": 4.103535175323486,
      "learning_rate": 1.771875e-05,
      "loss": -0.1296,
      "step": 1434
    },
    {
      "bce_eff_w": 0.35524999999999995,
      "bce_loss": -0.46264833211898804,
      "epoch": 7.17,
      "step": 1434
    },
    {
      "epoch": 7.175,
      "grad_norm": 16.992942810058594,
      "learning_rate": 1.76875e-05,
      "loss": 0.1415,
      "step": 1435
    },
    {
      "bce_eff_w": 0.35540000000000005,
      "bce_loss": -0.46245110034942627,
      "epoch": 7.175,
      "step": 1435
    },
    {
      "epoch": 7.18,
      "grad_norm": 32.247859954833984,
      "learning_rate": 1.7656250000000002e-05,
      "loss": 1.0211,
      "step": 1436
    },
    {
      "bce_eff_w": 0.35555000000000003,
      "bce_loss": -0.4628341495990753,
      "epoch": 7.18,
      "step": 1436
    },
    {
      "epoch": 7.185,
      "grad_norm": 15.426881790161133,
      "learning_rate": 1.7625e-05,
      "loss": 0.8848,
      "step": 1437
    },
    {
      "bce_eff_w": 0.3557,
      "bce_loss": -0.45434752106666565,
      "epoch": 7.185,
      "step": 1437
    },
    {
      "epoch": 7.19,
      "grad_norm": 36.333919525146484,
      "learning_rate": 1.759375e-05,
      "loss": 0.4688,
      "step": 1438
    },
    {
      "bce_eff_w": 0.35585,
      "bce_loss": -0.46151140332221985,
      "epoch": 7.19,
      "step": 1438
    },
    {
      "epoch": 7.195,
      "grad_norm": 16.372400283813477,
      "learning_rate": 1.75625e-05,
      "loss": 0.3946,
      "step": 1439
    },
    {
      "bce_eff_w": 0.356,
      "bce_loss": -0.4559025168418884,
      "epoch": 7.195,
      "step": 1439
    },
    {
      "epoch": 7.2,
      "grad_norm": 23.03294563293457,
      "learning_rate": 1.7531250000000003e-05,
      "loss": 0.2922,
      "step": 1440
    },
    {
      "bce_eff_w": 0.35614999999999997,
      "bce_loss": -0.4592648446559906,
      "epoch": 7.2,
      "step": 1440
    },
    {
      "epoch": 7.205,
      "grad_norm": 22.714601516723633,
      "learning_rate": 1.75e-05,
      "loss": 0.8137,
      "step": 1441
    },
    {
      "bce_eff_w": 0.3563,
      "bce_loss": -0.4582717716693878,
      "epoch": 7.205,
      "step": 1441
    },
    {
      "epoch": 7.21,
      "grad_norm": 14.735245704650879,
      "learning_rate": 1.746875e-05,
      "loss": 0.1633,
      "step": 1442
    },
    {
      "bce_eff_w": 0.35645,
      "bce_loss": -0.46251773834228516,
      "epoch": 7.21,
      "step": 1442
    },
    {
      "epoch": 7.215,
      "grad_norm": 20.818748474121094,
      "learning_rate": 1.74375e-05,
      "loss": 1.1834,
      "step": 1443
    },
    {
      "bce_eff_w": 0.35660000000000003,
      "bce_loss": -0.45764872431755066,
      "epoch": 7.215,
      "step": 1443
    },
    {
      "epoch": 7.22,
      "grad_norm": 15.160791397094727,
      "learning_rate": 1.7406250000000003e-05,
      "loss": 0.4525,
      "step": 1444
    },
    {
      "bce_eff_w": 0.35675,
      "bce_loss": -0.4618503749370575,
      "epoch": 7.22,
      "step": 1444
    },
    {
      "epoch": 7.225,
      "grad_norm": 27.928159713745117,
      "learning_rate": 1.7375e-05,
      "loss": 1.0995,
      "step": 1445
    },
    {
      "bce_eff_w": 0.3569,
      "bce_loss": -0.46114903688430786,
      "epoch": 7.225,
      "step": 1445
    },
    {
      "epoch": 7.23,
      "grad_norm": 25.168272018432617,
      "learning_rate": 1.734375e-05,
      "loss": 0.343,
      "step": 1446
    },
    {
      "bce_eff_w": 0.35705,
      "bce_loss": -0.46279940009117126,
      "epoch": 7.23,
      "step": 1446
    },
    {
      "epoch": 7.235,
      "grad_norm": 19.672311782836914,
      "learning_rate": 1.73125e-05,
      "loss": 1.2898,
      "step": 1447
    },
    {
      "bce_eff_w": 0.3572,
      "bce_loss": -0.45669859647750854,
      "epoch": 7.235,
      "step": 1447
    },
    {
      "epoch": 7.24,
      "grad_norm": 16.242835998535156,
      "learning_rate": 1.728125e-05,
      "loss": 1.1815,
      "step": 1448
    },
    {
      "bce_eff_w": 0.35735,
      "bce_loss": -0.4548819661140442,
      "epoch": 7.24,
      "step": 1448
    },
    {
      "epoch": 7.245,
      "grad_norm": 29.403682708740234,
      "learning_rate": 1.725e-05,
      "loss": 2.1833,
      "step": 1449
    },
    {
      "bce_eff_w": 0.35750000000000004,
      "bce_loss": -0.46319863200187683,
      "epoch": 7.245,
      "step": 1449
    },
    {
      "epoch": 7.25,
      "grad_norm": 17.076072692871094,
      "learning_rate": 1.7218750000000002e-05,
      "loss": 1.3764,
      "step": 1450
    },
    {
      "bce_eff_w": 0.35765,
      "bce_loss": -0.45828330516815186,
      "epoch": 7.25,
      "step": 1450
    },
    {
      "epoch": 7.255,
      "grad_norm": 1.7632975578308105,
      "learning_rate": 1.71875e-05,
      "loss": -0.1415,
      "step": 1451
    },
    {
      "bce_eff_w": 0.3578,
      "bce_loss": -0.4640061855316162,
      "epoch": 7.255,
      "step": 1451
    },
    {
      "epoch": 7.26,
      "grad_norm": 20.132734298706055,
      "learning_rate": 1.715625e-05,
      "loss": 0.2317,
      "step": 1452
    },
    {
      "bce_eff_w": 0.35795,
      "bce_loss": -0.46023082733154297,
      "epoch": 7.26,
      "step": 1452
    },
    {
      "epoch": 7.265,
      "grad_norm": 16.287633895874023,
      "learning_rate": 1.7125000000000003e-05,
      "loss": 0.3444,
      "step": 1453
    },
    {
      "bce_eff_w": 0.3581,
      "bce_loss": -0.4592955708503723,
      "epoch": 7.265,
      "step": 1453
    },
    {
      "epoch": 7.27,
      "grad_norm": 18.82167625427246,
      "learning_rate": 1.709375e-05,
      "loss": 2.5224,
      "step": 1454
    },
    {
      "bce_eff_w": 0.35824999999999996,
      "bce_loss": -0.46182557940483093,
      "epoch": 7.27,
      "step": 1454
    },
    {
      "epoch": 7.275,
      "grad_norm": 25.64784049987793,
      "learning_rate": 1.70625e-05,
      "loss": 1.4751,
      "step": 1455
    },
    {
      "bce_eff_w": 0.35840000000000005,
      "bce_loss": -0.4637952744960785,
      "epoch": 7.275,
      "step": 1455
    },
    {
      "epoch": 7.28,
      "grad_norm": 19.056377410888672,
      "learning_rate": 1.703125e-05,
      "loss": 0.4142,
      "step": 1456
    },
    {
      "bce_eff_w": 0.35855000000000004,
      "bce_loss": -0.46279820799827576,
      "epoch": 7.28,
      "step": 1456
    },
    {
      "epoch": 7.285,
      "grad_norm": 32.214935302734375,
      "learning_rate": 1.7000000000000003e-05,
      "loss": 0.6427,
      "step": 1457
    },
    {
      "bce_eff_w": 0.3587,
      "bce_loss": -0.46086248755455017,
      "epoch": 7.285,
      "step": 1457
    },
    {
      "epoch": 7.29,
      "grad_norm": 21.031673431396484,
      "learning_rate": 1.696875e-05,
      "loss": 0.6423,
      "step": 1458
    },
    {
      "bce_eff_w": 0.35885,
      "bce_loss": -0.4626599848270416,
      "epoch": 7.29,
      "step": 1458
    },
    {
      "epoch": 7.295,
      "grad_norm": 27.48521614074707,
      "learning_rate": 1.6937500000000002e-05,
      "loss": 0.8044,
      "step": 1459
    },
    {
      "bce_eff_w": 0.359,
      "bce_loss": -0.45912620425224304,
      "epoch": 7.295,
      "step": 1459
    },
    {
      "epoch": 7.3,
      "grad_norm": 14.559219360351562,
      "learning_rate": 1.690625e-05,
      "loss": 2.1792,
      "step": 1460
    },
    {
      "bce_eff_w": 0.35914999999999997,
      "bce_loss": -0.4612111449241638,
      "epoch": 7.3,
      "step": 1460
    },
    {
      "epoch": 7.305,
      "grad_norm": 23.383108139038086,
      "learning_rate": 1.6875000000000004e-05,
      "loss": 1.0852,
      "step": 1461
    },
    {
      "bce_eff_w": 0.3593,
      "bce_loss": -0.4458017647266388,
      "epoch": 7.305,
      "step": 1461
    },
    {
      "epoch": 7.31,
      "grad_norm": 20.44137191772461,
      "learning_rate": 1.684375e-05,
      "loss": 1.7566,
      "step": 1462
    },
    {
      "bce_eff_w": 0.35945,
      "bce_loss": -0.45490553975105286,
      "epoch": 7.31,
      "step": 1462
    },
    {
      "epoch": 7.315,
      "grad_norm": 61.40187454223633,
      "learning_rate": 1.6812500000000002e-05,
      "loss": 0.2603,
      "step": 1463
    },
    {
      "bce_eff_w": 0.35960000000000003,
      "bce_loss": -0.4559290111064911,
      "epoch": 7.315,
      "step": 1463
    },
    {
      "epoch": 7.32,
      "grad_norm": 20.629785537719727,
      "learning_rate": 1.678125e-05,
      "loss": 0.5377,
      "step": 1464
    },
    {
      "bce_eff_w": 0.35975,
      "bce_loss": -0.46360206604003906,
      "epoch": 7.32,
      "step": 1464
    },
    {
      "epoch": 7.325,
      "grad_norm": 22.04056739807129,
      "learning_rate": 1.675e-05,
      "loss": 1.1507,
      "step": 1465
    },
    {
      "bce_eff_w": 0.3599,
      "bce_loss": -0.46008777618408203,
      "epoch": 7.325,
      "step": 1465
    },
    {
      "epoch": 7.33,
      "grad_norm": 33.82942199707031,
      "learning_rate": 1.671875e-05,
      "loss": 0.3057,
      "step": 1466
    },
    {
      "bce_eff_w": 0.36005,
      "bce_loss": -0.46255427598953247,
      "epoch": 7.33,
      "step": 1466
    },
    {
      "epoch": 7.335,
      "grad_norm": 15.115700721740723,
      "learning_rate": 1.66875e-05,
      "loss": 0.7536,
      "step": 1467
    },
    {
      "bce_eff_w": 0.3602,
      "bce_loss": -0.4642496109008789,
      "epoch": 7.335,
      "step": 1467
    },
    {
      "epoch": 7.34,
      "grad_norm": 34.2384147644043,
      "learning_rate": 1.665625e-05,
      "loss": 0.73,
      "step": 1468
    },
    {
      "bce_eff_w": 0.36035,
      "bce_loss": -0.46268463134765625,
      "epoch": 7.34,
      "step": 1468
    },
    {
      "epoch": 7.345,
      "grad_norm": 15.420121192932129,
      "learning_rate": 1.6625e-05,
      "loss": 1.3061,
      "step": 1469
    },
    {
      "bce_eff_w": 0.36050000000000004,
      "bce_loss": -0.45669081807136536,
      "epoch": 7.345,
      "step": 1469
    },
    {
      "epoch": 7.35,
      "grad_norm": 18.692066192626953,
      "learning_rate": 1.659375e-05,
      "loss": 0.0343,
      "step": 1470
    },
    {
      "bce_eff_w": 0.36065,
      "bce_loss": -0.46180492639541626,
      "epoch": 7.35,
      "step": 1470
    },
    {
      "epoch": 7.355,
      "grad_norm": 31.789278030395508,
      "learning_rate": 1.65625e-05,
      "loss": 1.5015,
      "step": 1471
    },
    {
      "bce_eff_w": 0.3608,
      "bce_loss": -0.4627869725227356,
      "epoch": 7.355,
      "step": 1471
    },
    {
      "epoch": 7.36,
      "grad_norm": 18.796655654907227,
      "learning_rate": 1.6531250000000002e-05,
      "loss": 0.4813,
      "step": 1472
    },
    {
      "bce_eff_w": 0.36095,
      "bce_loss": -0.4625168740749359,
      "epoch": 7.36,
      "step": 1472
    },
    {
      "epoch": 7.365,
      "grad_norm": 18.36016273498535,
      "learning_rate": 1.65e-05,
      "loss": 1.0104,
      "step": 1473
    },
    {
      "bce_eff_w": 0.3611,
      "bce_loss": -0.45896634459495544,
      "epoch": 7.365,
      "step": 1473
    },
    {
      "epoch": 7.37,
      "grad_norm": 15.018534660339355,
      "learning_rate": 1.646875e-05,
      "loss": 2.0708,
      "step": 1474
    },
    {
      "bce_eff_w": 0.36124999999999996,
      "bce_loss": -0.4607234597206116,
      "epoch": 7.37,
      "step": 1474
    },
    {
      "epoch": 7.375,
      "grad_norm": 15.04145336151123,
      "learning_rate": 1.64375e-05,
      "loss": 2.4836,
      "step": 1475
    },
    {
      "bce_eff_w": 0.36140000000000005,
      "bce_loss": -0.46240273118019104,
      "epoch": 7.375,
      "step": 1475
    },
    {
      "epoch": 7.38,
      "grad_norm": 7.2368035316467285,
      "learning_rate": 1.6406250000000002e-05,
      "loss": -0.0526,
      "step": 1476
    },
    {
      "bce_eff_w": 0.36155000000000004,
      "bce_loss": -0.4603293240070343,
      "epoch": 7.38,
      "step": 1476
    },
    {
      "epoch": 7.385,
      "grad_norm": 46.38609313964844,
      "learning_rate": 1.6375e-05,
      "loss": 1.4141,
      "step": 1477
    },
    {
      "bce_eff_w": 0.3617,
      "bce_loss": -0.4612218737602234,
      "epoch": 7.385,
      "step": 1477
    },
    {
      "epoch": 7.39,
      "grad_norm": 19.756181716918945,
      "learning_rate": 1.634375e-05,
      "loss": 0.2917,
      "step": 1478
    },
    {
      "bce_eff_w": 0.36185,
      "bce_loss": -0.45295479893684387,
      "epoch": 7.39,
      "step": 1478
    },
    {
      "epoch": 7.395,
      "grad_norm": 18.443233489990234,
      "learning_rate": 1.63125e-05,
      "loss": 0.4387,
      "step": 1479
    },
    {
      "bce_eff_w": 0.362,
      "bce_loss": -0.45740270614624023,
      "epoch": 7.395,
      "step": 1479
    },
    {
      "epoch": 7.4,
      "grad_norm": 13.726513862609863,
      "learning_rate": 1.628125e-05,
      "loss": 2.0634,
      "step": 1480
    },
    {
      "bce_eff_w": 0.36214999999999997,
      "bce_loss": -0.4597046673297882,
      "epoch": 7.4,
      "step": 1480
    },
    {
      "epoch": 7.405,
      "grad_norm": 11.936567306518555,
      "learning_rate": 1.6250000000000002e-05,
      "loss": 0.268,
      "step": 1481
    },
    {
      "bce_eff_w": 0.3623,
      "bce_loss": -0.4618835151195526,
      "epoch": 7.405,
      "step": 1481
    },
    {
      "epoch": 7.41,
      "grad_norm": 24.755699157714844,
      "learning_rate": 1.621875e-05,
      "loss": 2.5276,
      "step": 1482
    },
    {
      "bce_eff_w": 0.36245,
      "bce_loss": -0.4605598449707031,
      "epoch": 7.41,
      "step": 1482
    },
    {
      "epoch": 7.415,
      "grad_norm": 13.436500549316406,
      "learning_rate": 1.61875e-05,
      "loss": -0.0189,
      "step": 1483
    },
    {
      "bce_eff_w": 0.36260000000000003,
      "bce_loss": -0.45596832036972046,
      "epoch": 7.415,
      "step": 1483
    },
    {
      "epoch": 7.42,
      "grad_norm": 17.768484115600586,
      "learning_rate": 1.615625e-05,
      "loss": 2.1596,
      "step": 1484
    },
    {
      "bce_eff_w": 0.36275,
      "bce_loss": -0.4628993570804596,
      "epoch": 7.42,
      "step": 1484
    },
    {
      "epoch": 7.425,
      "grad_norm": 30.72830581665039,
      "learning_rate": 1.6125000000000002e-05,
      "loss": 0.2114,
      "step": 1485
    },
    {
      "bce_eff_w": 0.3629,
      "bce_loss": -0.4572444558143616,
      "epoch": 7.425,
      "step": 1485
    },
    {
      "epoch": 7.43,
      "grad_norm": 18.94526481628418,
      "learning_rate": 1.609375e-05,
      "loss": 0.6698,
      "step": 1486
    },
    {
      "bce_eff_w": 0.36305,
      "bce_loss": -0.4643491506576538,
      "epoch": 7.43,
      "step": 1486
    },
    {
      "epoch": 7.435,
      "grad_norm": 28.23480796813965,
      "learning_rate": 1.60625e-05,
      "loss": 0.7605,
      "step": 1487
    },
    {
      "bce_eff_w": 0.3632,
      "bce_loss": -0.45760321617126465,
      "epoch": 7.435,
      "step": 1487
    },
    {
      "epoch": 7.44,
      "grad_norm": 8.659979820251465,
      "learning_rate": 1.603125e-05,
      "loss": -0.0313,
      "step": 1488
    },
    {
      "bce_eff_w": 0.36335,
      "bce_loss": -0.46239960193634033,
      "epoch": 7.44,
      "step": 1488
    },
    {
      "epoch": 7.445,
      "grad_norm": 12.662672996520996,
      "learning_rate": 1.6000000000000003e-05,
      "loss": 0.1135,
      "step": 1489
    },
    {
      "bce_eff_w": 0.36350000000000005,
      "bce_loss": -0.46011021733283997,
      "epoch": 7.445,
      "step": 1489
    },
    {
      "epoch": 7.45,
      "grad_norm": 17.733970642089844,
      "learning_rate": 1.5968750000000002e-05,
      "loss": 1.0815,
      "step": 1490
    },
    {
      "bce_eff_w": 0.36365000000000003,
      "bce_loss": -0.458336740732193,
      "epoch": 7.45,
      "step": 1490
    },
    {
      "epoch": 7.455,
      "grad_norm": 29.59294891357422,
      "learning_rate": 1.59375e-05,
      "loss": 0.1212,
      "step": 1491
    },
    {
      "bce_eff_w": 0.3638,
      "bce_loss": -0.45620888471603394,
      "epoch": 7.455,
      "step": 1491
    },
    {
      "epoch": 7.46,
      "grad_norm": 35.29865264892578,
      "learning_rate": 1.590625e-05,
      "loss": 2.6581,
      "step": 1492
    },
    {
      "bce_eff_w": 0.36395,
      "bce_loss": -0.4593649208545685,
      "epoch": 7.46,
      "step": 1492
    },
    {
      "epoch": 7.465,
      "grad_norm": 26.35713768005371,
      "learning_rate": 1.5875e-05,
      "loss": 0.8626,
      "step": 1493
    },
    {
      "bce_eff_w": 0.3641,
      "bce_loss": -0.455532431602478,
      "epoch": 7.465,
      "step": 1493
    },
    {
      "epoch": 7.47,
      "grad_norm": 18.77716064453125,
      "learning_rate": 1.5843750000000002e-05,
      "loss": 0.988,
      "step": 1494
    },
    {
      "bce_eff_w": 0.36424999999999996,
      "bce_loss": -0.4597415626049042,
      "epoch": 7.47,
      "step": 1494
    },
    {
      "epoch": 7.475,
      "grad_norm": 26.53694725036621,
      "learning_rate": 1.5812499999999998e-05,
      "loss": 2.1224,
      "step": 1495
    },
    {
      "bce_eff_w": 0.36440000000000006,
      "bce_loss": -0.46460628509521484,
      "epoch": 7.475,
      "step": 1495
    },
    {
      "epoch": 7.48,
      "grad_norm": 15.15616512298584,
      "learning_rate": 1.578125e-05,
      "loss": 1.6844,
      "step": 1496
    },
    {
      "bce_eff_w": 0.36455000000000004,
      "bce_loss": -0.46069732308387756,
      "epoch": 7.48,
      "step": 1496
    },
    {
      "epoch": 7.485,
      "grad_norm": 2.9368388652801514,
      "learning_rate": 1.575e-05,
      "loss": -0.14,
      "step": 1497
    },
    {
      "bce_eff_w": 0.3647,
      "bce_loss": -0.4642024636268616,
      "epoch": 7.485,
      "step": 1497
    },
    {
      "epoch": 7.49,
      "grad_norm": 23.99375343322754,
      "learning_rate": 1.5718750000000003e-05,
      "loss": 0.4245,
      "step": 1498
    },
    {
      "bce_eff_w": 0.36485,
      "bce_loss": -0.4585896134376526,
      "epoch": 7.49,
      "step": 1498
    },
    {
      "epoch": 7.495,
      "grad_norm": 14.66304874420166,
      "learning_rate": 1.56875e-05,
      "loss": 2.2534,
      "step": 1499
    },
    {
      "bce_eff_w": 0.365,
      "bce_loss": -0.46359390020370483,
      "epoch": 7.495,
      "step": 1499
    },
    {
      "epoch": 7.5,
      "grad_norm": 16.056087493896484,
      "learning_rate": 1.565625e-05,
      "loss": 0.9295,
      "step": 1500
    },
    {
      "bce_eff_w": 0.36515,
      "bce_loss": -0.46046438813209534,
      "epoch": 7.5,
      "step": 1500
    },
    {
      "epoch": 7.505,
      "grad_norm": 19.276954650878906,
      "learning_rate": 1.5625e-05,
      "loss": 0.7032,
      "step": 1501
    },
    {
      "bce_eff_w": 0.3653,
      "bce_loss": -0.45719483494758606,
      "epoch": 7.505,
      "step": 1501
    },
    {
      "epoch": 7.51,
      "grad_norm": 7.9247050285339355,
      "learning_rate": 1.5593750000000003e-05,
      "loss": -0.0712,
      "step": 1502
    },
    {
      "bce_eff_w": 0.36545,
      "bce_loss": -0.4601500928401947,
      "epoch": 7.51,
      "step": 1502
    },
    {
      "epoch": 7.515,
      "grad_norm": 19.45148277282715,
      "learning_rate": 1.5562500000000002e-05,
      "loss": 0.4371,
      "step": 1503
    },
    {
      "bce_eff_w": 0.36560000000000004,
      "bce_loss": -0.4610901176929474,
      "epoch": 7.515,
      "step": 1503
    },
    {
      "epoch": 7.52,
      "grad_norm": 15.786857604980469,
      "learning_rate": 1.553125e-05,
      "loss": 0.5346,
      "step": 1504
    },
    {
      "bce_eff_w": 0.36575,
      "bce_loss": -0.4577597379684448,
      "epoch": 7.52,
      "step": 1504
    },
    {
      "epoch": 7.525,
      "grad_norm": 13.526956558227539,
      "learning_rate": 1.55e-05,
      "loss": 0.6303,
      "step": 1505
    },
    {
      "bce_eff_w": 0.3659,
      "bce_loss": -0.4409189820289612,
      "epoch": 7.525,
      "step": 1505
    },
    {
      "epoch": 7.53,
      "grad_norm": 7.893453121185303,
      "learning_rate": 1.546875e-05,
      "loss": -0.0364,
      "step": 1506
    },
    {
      "bce_eff_w": 0.36605,
      "bce_loss": -0.4594407081604004,
      "epoch": 7.53,
      "step": 1506
    },
    {
      "epoch": 7.535,
      "grad_norm": 11.206762313842773,
      "learning_rate": 1.5437500000000003e-05,
      "loss": 0.0178,
      "step": 1507
    },
    {
      "bce_eff_w": 0.3662,
      "bce_loss": -0.46147316694259644,
      "epoch": 7.535,
      "step": 1507
    },
    {
      "epoch": 7.54,
      "grad_norm": 10.65157699584961,
      "learning_rate": 1.540625e-05,
      "loss": -0.0129,
      "step": 1508
    },
    {
      "bce_eff_w": 0.36635,
      "bce_loss": -0.45707520842552185,
      "epoch": 7.54,
      "step": 1508
    },
    {
      "epoch": 7.545,
      "grad_norm": 13.10901927947998,
      "learning_rate": 1.5375e-05,
      "loss": 0.3831,
      "step": 1509
    },
    {
      "bce_eff_w": 0.36650000000000005,
      "bce_loss": -0.4539147913455963,
      "epoch": 7.545,
      "step": 1509
    },
    {
      "epoch": 7.55,
      "grad_norm": 15.787128448486328,
      "learning_rate": 1.534375e-05,
      "loss": 0.3932,
      "step": 1510
    },
    {
      "bce_eff_w": 0.36665000000000003,
      "bce_loss": -0.46063050627708435,
      "epoch": 7.55,
      "step": 1510
    },
    {
      "epoch": 7.555,
      "grad_norm": 5.881115913391113,
      "learning_rate": 1.5312500000000003e-05,
      "loss": -0.101,
      "step": 1511
    },
    {
      "bce_eff_w": 0.3668,
      "bce_loss": -0.45752081274986267,
      "epoch": 7.555,
      "step": 1511
    },
    {
      "epoch": 7.5600000000000005,
      "grad_norm": 20.781780242919922,
      "learning_rate": 1.528125e-05,
      "loss": 0.2307,
      "step": 1512
    },
    {
      "bce_eff_w": 0.36695,
      "bce_loss": -0.45380765199661255,
      "epoch": 7.5600000000000005,
      "step": 1512
    },
    {
      "epoch": 7.5649999999999995,
      "grad_norm": 12.585209846496582,
      "learning_rate": 1.525e-05,
      "loss": 0.2017,
      "step": 1513
    },
    {
      "bce_eff_w": 0.3671,
      "bce_loss": -0.46181297302246094,
      "epoch": 7.5649999999999995,
      "step": 1513
    },
    {
      "epoch": 7.57,
      "grad_norm": 26.341697692871094,
      "learning_rate": 1.521875e-05,
      "loss": 0.1943,
      "step": 1514
    },
    {
      "bce_eff_w": 0.36724999999999997,
      "bce_loss": -0.46094027161598206,
      "epoch": 7.57,
      "step": 1514
    },
    {
      "epoch": 7.575,
      "grad_norm": 10.60838508605957,
      "learning_rate": 1.5187500000000002e-05,
      "loss": 0.029,
      "step": 1515
    },
    {
      "bce_eff_w": 0.36740000000000006,
      "bce_loss": -0.46081268787384033,
      "epoch": 7.575,
      "step": 1515
    },
    {
      "epoch": 7.58,
      "grad_norm": 22.669191360473633,
      "learning_rate": 1.5156249999999999e-05,
      "loss": 0.3003,
      "step": 1516
    },
    {
      "bce_eff_w": 0.36755000000000004,
      "bce_loss": -0.45826244354248047,
      "epoch": 7.58,
      "step": 1516
    },
    {
      "epoch": 7.585,
      "grad_norm": 16.021867752075195,
      "learning_rate": 1.5125e-05,
      "loss": 0.5263,
      "step": 1517
    },
    {
      "bce_eff_w": 0.3677,
      "bce_loss": -0.45514485239982605,
      "epoch": 7.585,
      "step": 1517
    },
    {
      "epoch": 7.59,
      "grad_norm": 28.247785568237305,
      "learning_rate": 1.5093750000000001e-05,
      "loss": 0.5549,
      "step": 1518
    },
    {
      "bce_eff_w": 0.36785,
      "bce_loss": -0.4623955488204956,
      "epoch": 7.59,
      "step": 1518
    },
    {
      "epoch": 7.595,
      "grad_norm": 19.434955596923828,
      "learning_rate": 1.5062500000000002e-05,
      "loss": 0.8465,
      "step": 1519
    },
    {
      "bce_eff_w": 0.368,
      "bce_loss": -0.45797252655029297,
      "epoch": 7.595,
      "step": 1519
    },
    {
      "epoch": 7.6,
      "grad_norm": 16.817760467529297,
      "learning_rate": 1.503125e-05,
      "loss": 0.1829,
      "step": 1520
    },
    {
      "bce_eff_w": 0.36815,
      "bce_loss": -0.46379563212394714,
      "epoch": 7.6,
      "step": 1520
    },
    {
      "epoch": 7.605,
      "grad_norm": 30.337759017944336,
      "learning_rate": 1.5e-05,
      "loss": 1.0344,
      "step": 1521
    },
    {
      "bce_eff_w": 0.3683,
      "bce_loss": -0.4640415608882904,
      "epoch": 7.605,
      "step": 1521
    },
    {
      "epoch": 7.61,
      "grad_norm": 15.558259010314941,
      "learning_rate": 1.4968750000000001e-05,
      "loss": 1.358,
      "step": 1522
    },
    {
      "bce_eff_w": 0.36845,
      "bce_loss": -0.45460236072540283,
      "epoch": 7.61,
      "step": 1522
    },
    {
      "epoch": 7.615,
      "grad_norm": 15.871256828308105,
      "learning_rate": 1.4937500000000002e-05,
      "loss": 0.176,
      "step": 1523
    },
    {
      "bce_eff_w": 0.36860000000000004,
      "bce_loss": -0.4617359936237335,
      "epoch": 7.615,
      "step": 1523
    },
    {
      "epoch": 7.62,
      "grad_norm": 19.45679473876953,
      "learning_rate": 1.490625e-05,
      "loss": 0.5327,
      "step": 1524
    },
    {
      "bce_eff_w": 0.36875,
      "bce_loss": -0.4635409712791443,
      "epoch": 7.62,
      "step": 1524
    },
    {
      "epoch": 7.625,
      "grad_norm": 13.802508354187012,
      "learning_rate": 1.4875e-05,
      "loss": 1.1005,
      "step": 1525
    },
    {
      "bce_eff_w": 0.3689,
      "bce_loss": -0.4551562964916229,
      "epoch": 7.625,
      "step": 1525
    },
    {
      "epoch": 7.63,
      "grad_norm": 9.857654571533203,
      "learning_rate": 1.484375e-05,
      "loss": 0.0863,
      "step": 1526
    },
    {
      "bce_eff_w": 0.36905,
      "bce_loss": -0.4548241198062897,
      "epoch": 7.63,
      "step": 1526
    },
    {
      "epoch": 7.635,
      "grad_norm": 21.446849822998047,
      "learning_rate": 1.4812500000000001e-05,
      "loss": 1.3816,
      "step": 1527
    },
    {
      "bce_eff_w": 0.3692,
      "bce_loss": -0.4556840658187866,
      "epoch": 7.635,
      "step": 1527
    },
    {
      "epoch": 7.64,
      "grad_norm": 23.96626853942871,
      "learning_rate": 1.4781250000000002e-05,
      "loss": 0.9888,
      "step": 1528
    },
    {
      "bce_eff_w": 0.36935,
      "bce_loss": -0.46061787009239197,
      "epoch": 7.64,
      "step": 1528
    },
    {
      "epoch": 7.645,
      "grad_norm": 16.491334915161133,
      "learning_rate": 1.475e-05,
      "loss": 1.7184,
      "step": 1529
    },
    {
      "bce_eff_w": 0.3695,
      "bce_loss": -0.4619826376438141,
      "epoch": 7.645,
      "step": 1529
    },
    {
      "epoch": 7.65,
      "grad_norm": 39.46528244018555,
      "learning_rate": 1.471875e-05,
      "loss": 0.606,
      "step": 1530
    },
    {
      "bce_eff_w": 0.36965000000000003,
      "bce_loss": -0.46306002140045166,
      "epoch": 7.65,
      "step": 1530
    },
    {
      "epoch": 7.655,
      "grad_norm": 20.7860164642334,
      "learning_rate": 1.4687500000000001e-05,
      "loss": 0.0433,
      "step": 1531
    },
    {
      "bce_eff_w": 0.3698,
      "bce_loss": -0.4625067114830017,
      "epoch": 7.655,
      "step": 1531
    },
    {
      "epoch": 7.66,
      "grad_norm": 18.780317306518555,
      "learning_rate": 1.4656250000000002e-05,
      "loss": 0.0566,
      "step": 1532
    },
    {
      "bce_eff_w": 0.36995,
      "bce_loss": -0.46248671412467957,
      "epoch": 7.66,
      "step": 1532
    },
    {
      "epoch": 7.665,
      "grad_norm": 12.052567481994629,
      "learning_rate": 1.4625e-05,
      "loss": 0.2468,
      "step": 1533
    },
    {
      "bce_eff_w": 0.3701,
      "bce_loss": -0.46132776141166687,
      "epoch": 7.665,
      "step": 1533
    },
    {
      "epoch": 7.67,
      "grad_norm": 33.948368072509766,
      "learning_rate": 1.459375e-05,
      "loss": 1.2568,
      "step": 1534
    },
    {
      "bce_eff_w": 0.37024999999999997,
      "bce_loss": -0.46046149730682373,
      "epoch": 7.67,
      "step": 1534
    },
    {
      "epoch": 7.675,
      "grad_norm": 19.682329177856445,
      "learning_rate": 1.4562500000000002e-05,
      "loss": 0.9886,
      "step": 1535
    },
    {
      "bce_eff_w": 0.37039999999999995,
      "bce_loss": -0.45906946063041687,
      "epoch": 7.675,
      "step": 1535
    },
    {
      "epoch": 7.68,
      "grad_norm": 24.351903915405273,
      "learning_rate": 1.4531250000000003e-05,
      "loss": 2.0034,
      "step": 1536
    },
    {
      "bce_eff_w": 0.37055000000000005,
      "bce_loss": -0.45998090505599976,
      "epoch": 7.68,
      "step": 1536
    },
    {
      "epoch": 7.6850000000000005,
      "grad_norm": 14.720638275146484,
      "learning_rate": 1.45e-05,
      "loss": 0.9136,
      "step": 1537
    },
    {
      "bce_eff_w": 0.37070000000000003,
      "bce_loss": -0.4597446024417877,
      "epoch": 7.6850000000000005,
      "step": 1537
    },
    {
      "epoch": 7.6899999999999995,
      "grad_norm": 23.166837692260742,
      "learning_rate": 1.4468750000000001e-05,
      "loss": 0.1985,
      "step": 1538
    },
    {
      "bce_eff_w": 0.37085,
      "bce_loss": -0.4444073438644409,
      "epoch": 7.6899999999999995,
      "step": 1538
    },
    {
      "epoch": 7.695,
      "grad_norm": 15.073145866394043,
      "learning_rate": 1.44375e-05,
      "loss": 0.5771,
      "step": 1539
    },
    {
      "bce_eff_w": 0.371,
      "bce_loss": -0.4619252383708954,
      "epoch": 7.695,
      "step": 1539
    },
    {
      "epoch": 7.7,
      "grad_norm": 21.248607635498047,
      "learning_rate": 1.4406250000000001e-05,
      "loss": 0.721,
      "step": 1540
    },
    {
      "bce_eff_w": 0.37115,
      "bce_loss": -0.46421244740486145,
      "epoch": 7.7,
      "step": 1540
    },
    {
      "epoch": 7.705,
      "grad_norm": 14.146224975585938,
      "learning_rate": 1.4374999999999999e-05,
      "loss": 1.7021,
      "step": 1541
    },
    {
      "bce_eff_w": 0.37129999999999996,
      "bce_loss": -0.4623348116874695,
      "epoch": 7.705,
      "step": 1541
    },
    {
      "epoch": 7.71,
      "grad_norm": 22.886871337890625,
      "learning_rate": 1.434375e-05,
      "loss": 0.4792,
      "step": 1542
    },
    {
      "bce_eff_w": 0.37145,
      "bce_loss": -0.46265438199043274,
      "epoch": 7.71,
      "step": 1542
    },
    {
      "epoch": 7.715,
      "grad_norm": 28.725893020629883,
      "learning_rate": 1.43125e-05,
      "loss": 0.9823,
      "step": 1543
    },
    {
      "bce_eff_w": 0.3716,
      "bce_loss": -0.46166661381721497,
      "epoch": 7.715,
      "step": 1543
    },
    {
      "epoch": 7.72,
      "grad_norm": 20.03183364868164,
      "learning_rate": 1.4281250000000002e-05,
      "loss": 0.1441,
      "step": 1544
    },
    {
      "bce_eff_w": 0.37175,
      "bce_loss": -0.46267732977867126,
      "epoch": 7.72,
      "step": 1544
    },
    {
      "epoch": 7.725,
      "grad_norm": 24.309612274169922,
      "learning_rate": 1.4249999999999999e-05,
      "loss": 0.6426,
      "step": 1545
    },
    {
      "bce_eff_w": 0.3719,
      "bce_loss": -0.4599009156227112,
      "epoch": 7.725,
      "step": 1545
    },
    {
      "epoch": 7.73,
      "grad_norm": 24.18040657043457,
      "learning_rate": 1.421875e-05,
      "loss": 1.3379,
      "step": 1546
    },
    {
      "bce_eff_w": 0.37205,
      "bce_loss": -0.4624546468257904,
      "epoch": 7.73,
      "step": 1546
    },
    {
      "epoch": 7.735,
      "grad_norm": 19.927780151367188,
      "learning_rate": 1.4187500000000001e-05,
      "loss": 1.7512,
      "step": 1547
    },
    {
      "bce_eff_w": 0.3722,
      "bce_loss": -0.4595228135585785,
      "epoch": 7.735,
      "step": 1547
    },
    {
      "epoch": 7.74,
      "grad_norm": 8.150622367858887,
      "learning_rate": 1.4156250000000002e-05,
      "loss": -0.0734,
      "step": 1548
    },
    {
      "bce_eff_w": 0.37235,
      "bce_loss": -0.4601997137069702,
      "epoch": 7.74,
      "step": 1548
    },
    {
      "epoch": 7.745,
      "grad_norm": 27.9736328125,
      "learning_rate": 1.4125e-05,
      "loss": 0.4235,
      "step": 1549
    },
    {
      "bce_eff_w": 0.3725,
      "bce_loss": -0.4594140648841858,
      "epoch": 7.745,
      "step": 1549
    },
    {
      "epoch": 7.75,
      "grad_norm": 20.60626220703125,
      "learning_rate": 1.409375e-05,
      "loss": 1.7407,
      "step": 1550
    },
    {
      "bce_eff_w": 0.37265000000000004,
      "bce_loss": -0.45964691042900085,
      "epoch": 7.75,
      "step": 1550
    },
    {
      "epoch": 7.755,
      "grad_norm": 16.571744918823242,
      "learning_rate": 1.4062500000000001e-05,
      "loss": 0.2329,
      "step": 1551
    },
    {
      "bce_eff_w": 0.3728,
      "bce_loss": -0.46162542700767517,
      "epoch": 7.755,
      "step": 1551
    },
    {
      "epoch": 7.76,
      "grad_norm": 20.879220962524414,
      "learning_rate": 1.403125e-05,
      "loss": 1.0399,
      "step": 1552
    },
    {
      "bce_eff_w": 0.37295,
      "bce_loss": -0.46213236451148987,
      "epoch": 7.76,
      "step": 1552
    },
    {
      "epoch": 7.765,
      "grad_norm": 18.068470001220703,
      "learning_rate": 1.4000000000000001e-05,
      "loss": 0.7203,
      "step": 1553
    },
    {
      "bce_eff_w": 0.3731,
      "bce_loss": -0.45993396639823914,
      "epoch": 7.765,
      "step": 1553
    },
    {
      "epoch": 7.77,
      "grad_norm": 15.286798477172852,
      "learning_rate": 1.396875e-05,
      "loss": 0.1182,
      "step": 1554
    },
    {
      "bce_eff_w": 0.37324999999999997,
      "bce_loss": -0.4611112177371979,
      "epoch": 7.77,
      "step": 1554
    },
    {
      "epoch": 7.775,
      "grad_norm": 23.38702392578125,
      "learning_rate": 1.39375e-05,
      "loss": 0.2798,
      "step": 1555
    },
    {
      "bce_eff_w": 0.37339999999999995,
      "bce_loss": -0.46199992299079895,
      "epoch": 7.775,
      "step": 1555
    },
    {
      "epoch": 7.78,
      "grad_norm": 20.53767967224121,
      "learning_rate": 1.3906250000000001e-05,
      "loss": 0.7416,
      "step": 1556
    },
    {
      "bce_eff_w": 0.37355000000000005,
      "bce_loss": -0.4602626860141754,
      "epoch": 7.78,
      "step": 1556
    },
    {
      "epoch": 7.785,
      "grad_norm": 16.062589645385742,
      "learning_rate": 1.3875000000000002e-05,
      "loss": 0.2126,
      "step": 1557
    },
    {
      "bce_eff_w": 0.37370000000000003,
      "bce_loss": -0.4606037437915802,
      "epoch": 7.785,
      "step": 1557
    },
    {
      "epoch": 7.79,
      "grad_norm": 23.40380859375,
      "learning_rate": 1.384375e-05,
      "loss": 0.981,
      "step": 1558
    },
    {
      "bce_eff_w": 0.37385,
      "bce_loss": -0.46089407801628113,
      "epoch": 7.79,
      "step": 1558
    },
    {
      "epoch": 7.795,
      "grad_norm": 22.04024314880371,
      "learning_rate": 1.38125e-05,
      "loss": 0.8418,
      "step": 1559
    },
    {
      "bce_eff_w": 0.374,
      "bce_loss": -0.46310803294181824,
      "epoch": 7.795,
      "step": 1559
    },
    {
      "epoch": 7.8,
      "grad_norm": 47.97246551513672,
      "learning_rate": 1.3781250000000001e-05,
      "loss": 0.7484,
      "step": 1560
    },
    {
      "bce_eff_w": 0.37415,
      "bce_loss": -0.46300655603408813,
      "epoch": 7.8,
      "step": 1560
    },
    {
      "epoch": 7.805,
      "grad_norm": 16.041732788085938,
      "learning_rate": 1.3750000000000002e-05,
      "loss": 1.2965,
      "step": 1561
    },
    {
      "bce_eff_w": 0.37429999999999997,
      "bce_loss": -0.46001508831977844,
      "epoch": 7.805,
      "step": 1561
    },
    {
      "epoch": 7.8100000000000005,
      "grad_norm": 9.423235893249512,
      "learning_rate": 1.371875e-05,
      "loss": 0.0106,
      "step": 1562
    },
    {
      "bce_eff_w": 0.37445,
      "bce_loss": -0.45977655053138733,
      "epoch": 7.8100000000000005,
      "step": 1562
    },
    {
      "epoch": 7.8149999999999995,
      "grad_norm": 32.71440887451172,
      "learning_rate": 1.36875e-05,
      "loss": 0.9198,
      "step": 1563
    },
    {
      "bce_eff_w": 0.3746,
      "bce_loss": -0.4608670473098755,
      "epoch": 7.8149999999999995,
      "step": 1563
    },
    {
      "epoch": 7.82,
      "grad_norm": 16.021827697753906,
      "learning_rate": 1.3656250000000002e-05,
      "loss": -0.0548,
      "step": 1564
    },
    {
      "bce_eff_w": 0.37475,
      "bce_loss": -0.45477527379989624,
      "epoch": 7.82,
      "step": 1564
    },
    {
      "epoch": 7.825,
      "grad_norm": 20.833948135375977,
      "learning_rate": 1.3625e-05,
      "loss": 1.659,
      "step": 1565
    },
    {
      "bce_eff_w": 0.3749,
      "bce_loss": -0.46234235167503357,
      "epoch": 7.825,
      "step": 1565
    },
    {
      "epoch": 7.83,
      "grad_norm": 21.290884017944336,
      "learning_rate": 1.359375e-05,
      "loss": 0.476,
      "step": 1566
    },
    {
      "bce_eff_w": 0.37505,
      "bce_loss": -0.4502980709075928,
      "epoch": 7.83,
      "step": 1566
    },
    {
      "epoch": 7.835,
      "grad_norm": 18.106033325195312,
      "learning_rate": 1.3562500000000001e-05,
      "loss": 0.6164,
      "step": 1567
    },
    {
      "bce_eff_w": 0.3752,
      "bce_loss": -0.4592582881450653,
      "epoch": 7.835,
      "step": 1567
    },
    {
      "epoch": 7.84,
      "grad_norm": 7.140481472015381,
      "learning_rate": 1.353125e-05,
      "loss": -0.0697,
      "step": 1568
    },
    {
      "bce_eff_w": 0.37535,
      "bce_loss": -0.4562035799026489,
      "epoch": 7.84,
      "step": 1568
    },
    {
      "epoch": 7.845,
      "grad_norm": 12.088715553283691,
      "learning_rate": 1.3500000000000001e-05,
      "loss": 0.0916,
      "step": 1569
    },
    {
      "bce_eff_w": 0.3755,
      "bce_loss": -0.46291661262512207,
      "epoch": 7.845,
      "step": 1569
    },
    {
      "epoch": 7.85,
      "grad_norm": 15.524286270141602,
      "learning_rate": 1.3468749999999999e-05,
      "loss": 1.3203,
      "step": 1570
    },
    {
      "bce_eff_w": 0.37565000000000004,
      "bce_loss": -0.4600889980792999,
      "epoch": 7.85,
      "step": 1570
    },
    {
      "epoch": 7.855,
      "grad_norm": 17.481584548950195,
      "learning_rate": 1.34375e-05,
      "loss": 0.9087,
      "step": 1571
    },
    {
      "bce_eff_w": 0.3758,
      "bce_loss": -0.46331194043159485,
      "epoch": 7.855,
      "step": 1571
    },
    {
      "epoch": 7.86,
      "grad_norm": 18.695722579956055,
      "learning_rate": 1.340625e-05,
      "loss": 0.9772,
      "step": 1572
    },
    {
      "bce_eff_w": 0.37595,
      "bce_loss": -0.46109119057655334,
      "epoch": 7.86,
      "step": 1572
    },
    {
      "epoch": 7.865,
      "grad_norm": 17.359649658203125,
      "learning_rate": 1.3375000000000002e-05,
      "loss": 1.1619,
      "step": 1573
    },
    {
      "bce_eff_w": 0.3761,
      "bce_loss": -0.460076242685318,
      "epoch": 7.865,
      "step": 1573
    },
    {
      "epoch": 7.87,
      "grad_norm": 51.270111083984375,
      "learning_rate": 1.3343749999999999e-05,
      "loss": 0.3149,
      "step": 1574
    },
    {
      "bce_eff_w": 0.37625,
      "bce_loss": -0.460071325302124,
      "epoch": 7.87,
      "step": 1574
    },
    {
      "epoch": 7.875,
      "grad_norm": 15.420125007629395,
      "learning_rate": 1.33125e-05,
      "loss": 0.5467,
      "step": 1575
    },
    {
      "bce_eff_w": 0.37639999999999996,
      "bce_loss": -0.4623945951461792,
      "epoch": 7.875,
      "step": 1575
    },
    {
      "epoch": 7.88,
      "grad_norm": 13.3951997756958,
      "learning_rate": 1.3281250000000001e-05,
      "loss": 0.1424,
      "step": 1576
    },
    {
      "bce_eff_w": 0.37655000000000005,
      "bce_loss": -0.46030887961387634,
      "epoch": 7.88,
      "step": 1576
    },
    {
      "epoch": 7.885,
      "grad_norm": 22.690011978149414,
      "learning_rate": 1.3250000000000002e-05,
      "loss": 0.3937,
      "step": 1577
    },
    {
      "bce_eff_w": 0.37670000000000003,
      "bce_loss": -0.45554623007774353,
      "epoch": 7.885,
      "step": 1577
    },
    {
      "epoch": 7.89,
      "grad_norm": 17.026657104492188,
      "learning_rate": 1.3218750000000001e-05,
      "loss": 1.95,
      "step": 1578
    },
    {
      "bce_eff_w": 0.37685,
      "bce_loss": -0.4646915793418884,
      "epoch": 7.89,
      "step": 1578
    },
    {
      "epoch": 7.895,
      "grad_norm": 11.587282180786133,
      "learning_rate": 1.31875e-05,
      "loss": 0.1503,
      "step": 1579
    },
    {
      "bce_eff_w": 0.377,
      "bce_loss": -0.45482003688812256,
      "epoch": 7.895,
      "step": 1579
    },
    {
      "epoch": 7.9,
      "grad_norm": 12.674398422241211,
      "learning_rate": 1.3156250000000001e-05,
      "loss": 2.3479,
      "step": 1580
    },
    {
      "bce_eff_w": 0.37715,
      "bce_loss": -0.4590068757534027,
      "epoch": 7.9,
      "step": 1580
    },
    {
      "epoch": 7.905,
      "grad_norm": 34.38052749633789,
      "learning_rate": 1.3125e-05,
      "loss": 0.5593,
      "step": 1581
    },
    {
      "bce_eff_w": 0.37729999999999997,
      "bce_loss": -0.46419787406921387,
      "epoch": 7.905,
      "step": 1581
    },
    {
      "epoch": 7.91,
      "grad_norm": 20.105648040771484,
      "learning_rate": 1.3093750000000001e-05,
      "loss": 1.3874,
      "step": 1582
    },
    {
      "bce_eff_w": 0.37745,
      "bce_loss": -0.46207261085510254,
      "epoch": 7.91,
      "step": 1582
    },
    {
      "epoch": 7.915,
      "grad_norm": 27.236289978027344,
      "learning_rate": 1.3062499999999999e-05,
      "loss": 0.7867,
      "step": 1583
    },
    {
      "bce_eff_w": 0.3776,
      "bce_loss": -0.46378639340400696,
      "epoch": 7.915,
      "step": 1583
    },
    {
      "epoch": 7.92,
      "grad_norm": 22.46437644958496,
      "learning_rate": 1.303125e-05,
      "loss": 0.2949,
      "step": 1584
    },
    {
      "bce_eff_w": 0.37775000000000003,
      "bce_loss": -0.45923754572868347,
      "epoch": 7.92,
      "step": 1584
    },
    {
      "epoch": 7.925,
      "grad_norm": 10.131956100463867,
      "learning_rate": 1.3000000000000001e-05,
      "loss": 0.0683,
      "step": 1585
    },
    {
      "bce_eff_w": 0.3779,
      "bce_loss": -0.452786922454834,
      "epoch": 7.925,
      "step": 1585
    },
    {
      "epoch": 7.93,
      "grad_norm": 22.917362213134766,
      "learning_rate": 1.2968750000000002e-05,
      "loss": 1.6289,
      "step": 1586
    },
    {
      "bce_eff_w": 0.37805,
      "bce_loss": -0.44769078493118286,
      "epoch": 7.93,
      "step": 1586
    },
    {
      "epoch": 7.9350000000000005,
      "grad_norm": 12.360389709472656,
      "learning_rate": 1.29375e-05,
      "loss": 0.0918,
      "step": 1587
    },
    {
      "bce_eff_w": 0.3782,
      "bce_loss": -0.46164876222610474,
      "epoch": 7.9350000000000005,
      "step": 1587
    },
    {
      "epoch": 7.9399999999999995,
      "grad_norm": 20.86354637145996,
      "learning_rate": 1.290625e-05,
      "loss": 0.4624,
      "step": 1588
    },
    {
      "bce_eff_w": 0.37835,
      "bce_loss": -0.4590623378753662,
      "epoch": 7.9399999999999995,
      "step": 1588
    },
    {
      "epoch": 7.945,
      "grad_norm": 19.494413375854492,
      "learning_rate": 1.2875000000000001e-05,
      "loss": 0.9764,
      "step": 1589
    },
    {
      "bce_eff_w": 0.3785,
      "bce_loss": -0.46468082070350647,
      "epoch": 7.945,
      "step": 1589
    },
    {
      "epoch": 7.95,
      "grad_norm": 22.020431518554688,
      "learning_rate": 1.2843750000000002e-05,
      "loss": 0.7478,
      "step": 1590
    },
    {
      "bce_eff_w": 0.37865000000000004,
      "bce_loss": -0.4632403552532196,
      "epoch": 7.95,
      "step": 1590
    },
    {
      "epoch": 7.955,
      "grad_norm": 19.150346755981445,
      "learning_rate": 1.28125e-05,
      "loss": 0.246,
      "step": 1591
    },
    {
      "bce_eff_w": 0.3788,
      "bce_loss": -0.46010822057724,
      "epoch": 7.955,
      "step": 1591
    },
    {
      "epoch": 7.96,
      "grad_norm": 24.973724365234375,
      "learning_rate": 1.278125e-05,
      "loss": 0.7064,
      "step": 1592
    },
    {
      "bce_eff_w": 0.37895,
      "bce_loss": -0.46184995770454407,
      "epoch": 7.96,
      "step": 1592
    },
    {
      "epoch": 7.965,
      "grad_norm": 4.0044050216674805,
      "learning_rate": 1.2750000000000002e-05,
      "loss": -0.1265,
      "step": 1593
    },
    {
      "bce_eff_w": 0.3791,
      "bce_loss": -0.45881038904190063,
      "epoch": 7.965,
      "step": 1593
    },
    {
      "epoch": 7.97,
      "grad_norm": 19.039871215820312,
      "learning_rate": 1.271875e-05,
      "loss": 1.1566,
      "step": 1594
    },
    {
      "bce_eff_w": 0.37925,
      "bce_loss": -0.4607284367084503,
      "epoch": 7.97,
      "step": 1594
    },
    {
      "epoch": 7.975,
      "grad_norm": 18.262907028198242,
      "learning_rate": 1.26875e-05,
      "loss": 0.6522,
      "step": 1595
    },
    {
      "bce_eff_w": 0.37939999999999996,
      "bce_loss": -0.46290627121925354,
      "epoch": 7.975,
      "step": 1595
    },
    {
      "epoch": 7.98,
      "grad_norm": 13.16336727142334,
      "learning_rate": 1.265625e-05,
      "loss": 1.2532,
      "step": 1596
    },
    {
      "bce_eff_w": 0.37955000000000005,
      "bce_loss": -0.4611184895038605,
      "epoch": 7.98,
      "step": 1596
    },
    {
      "epoch": 7.985,
      "grad_norm": 15.675764083862305,
      "learning_rate": 1.2625e-05,
      "loss": 1.2105,
      "step": 1597
    },
    {
      "bce_eff_w": 0.37970000000000004,
      "bce_loss": -0.4591158628463745,
      "epoch": 7.985,
      "step": 1597
    },
    {
      "epoch": 7.99,
      "grad_norm": 13.174713134765625,
      "learning_rate": 1.2593750000000001e-05,
      "loss": 0.7632,
      "step": 1598
    },
    {
      "bce_eff_w": 0.37985,
      "bce_loss": -0.46021050214767456,
      "epoch": 7.99,
      "step": 1598
    },
    {
      "epoch": 7.995,
      "grad_norm": 39.71651077270508,
      "learning_rate": 1.2562499999999999e-05,
      "loss": 1.1192,
      "step": 1599
    },
    {
      "bce_eff_w": 0.38,
      "bce_loss": -0.46077263355255127,
      "epoch": 7.995,
      "step": 1599
    },
    {
      "epoch": 8.0,
      "grad_norm": 36.1762580871582,
      "learning_rate": 1.253125e-05,
      "loss": 0.9523,
      "step": 1600
    },
    {
      "bce_eff_w": 0.38015,
      "bce_loss": -0.4619678556919098,
      "epoch": 8.0,
      "step": 1600
    },
    {
      "epoch": 8.005,
      "grad_norm": 19.908329010009766,
      "learning_rate": 1.25e-05,
      "loss": 0.168,
      "step": 1601
    },
    {
      "bce_eff_w": 0.38029999999999997,
      "bce_loss": -0.46185991168022156,
      "epoch": 8.005,
      "step": 1601
    },
    {
      "epoch": 8.01,
      "grad_norm": 15.356226921081543,
      "learning_rate": 1.2468750000000002e-05,
      "loss": 0.5727,
      "step": 1602
    },
    {
      "bce_eff_w": 0.38045,
      "bce_loss": -0.4620271623134613,
      "epoch": 8.01,
      "step": 1602
    },
    {
      "epoch": 8.015,
      "grad_norm": 19.388582229614258,
      "learning_rate": 1.24375e-05,
      "loss": 0.2511,
      "step": 1603
    },
    {
      "bce_eff_w": 0.3806,
      "bce_loss": -0.4641231596469879,
      "epoch": 8.015,
      "step": 1603
    },
    {
      "epoch": 8.02,
      "grad_norm": 18.80399513244629,
      "learning_rate": 1.2406250000000002e-05,
      "loss": 0.0765,
      "step": 1604
    },
    {
      "bce_eff_w": 0.38075000000000003,
      "bce_loss": -0.4631862938404083,
      "epoch": 8.02,
      "step": 1604
    },
    {
      "epoch": 8.025,
      "grad_norm": 16.168668746948242,
      "learning_rate": 1.2375000000000001e-05,
      "loss": 0.1972,
      "step": 1605
    },
    {
      "bce_eff_w": 0.3809,
      "bce_loss": -0.46251335740089417,
      "epoch": 8.025,
      "step": 1605
    },
    {
      "epoch": 8.03,
      "grad_norm": 12.963894844055176,
      "learning_rate": 1.2343750000000002e-05,
      "loss": 0.4259,
      "step": 1606
    },
    {
      "bce_eff_w": 0.38105,
      "bce_loss": -0.4587162733078003,
      "epoch": 8.03,
      "step": 1606
    },
    {
      "epoch": 8.035,
      "grad_norm": 20.425561904907227,
      "learning_rate": 1.2312500000000001e-05,
      "loss": 0.4294,
      "step": 1607
    },
    {
      "bce_eff_w": 0.3812,
      "bce_loss": -0.45810964703559875,
      "epoch": 8.035,
      "step": 1607
    },
    {
      "epoch": 8.04,
      "grad_norm": 21.88010597229004,
      "learning_rate": 1.228125e-05,
      "loss": 0.1967,
      "step": 1608
    },
    {
      "bce_eff_w": 0.38135,
      "bce_loss": -0.45924341678619385,
      "epoch": 8.04,
      "step": 1608
    },
    {
      "epoch": 8.045,
      "grad_norm": 20.39117431640625,
      "learning_rate": 1.225e-05,
      "loss": 0.3485,
      "step": 1609
    },
    {
      "bce_eff_w": 0.3815,
      "bce_loss": -0.4609333872795105,
      "epoch": 8.045,
      "step": 1609
    },
    {
      "epoch": 8.05,
      "grad_norm": 19.76703643798828,
      "learning_rate": 1.221875e-05,
      "loss": 2.3864,
      "step": 1610
    },
    {
      "bce_eff_w": 0.38165000000000004,
      "bce_loss": -0.46397045254707336,
      "epoch": 8.05,
      "step": 1610
    },
    {
      "epoch": 8.055,
      "grad_norm": 16.349395751953125,
      "learning_rate": 1.21875e-05,
      "loss": 0.6991,
      "step": 1611
    },
    {
      "bce_eff_w": 0.38180000000000003,
      "bce_loss": -0.45825374126434326,
      "epoch": 8.055,
      "step": 1611
    },
    {
      "epoch": 8.06,
      "grad_norm": 17.641109466552734,
      "learning_rate": 1.215625e-05,
      "loss": 0.4589,
      "step": 1612
    },
    {
      "bce_eff_w": 0.38195,
      "bce_loss": -0.45820337533950806,
      "epoch": 8.06,
      "step": 1612
    },
    {
      "epoch": 8.065,
      "grad_norm": 23.228975296020508,
      "learning_rate": 1.2125e-05,
      "loss": 1.6657,
      "step": 1613
    },
    {
      "bce_eff_w": 0.3821,
      "bce_loss": -0.4561994671821594,
      "epoch": 8.065,
      "step": 1613
    },
    {
      "epoch": 8.07,
      "grad_norm": 24.35920524597168,
      "learning_rate": 1.2093750000000001e-05,
      "loss": 1.1483,
      "step": 1614
    },
    {
      "bce_eff_w": 0.38225,
      "bce_loss": -0.46078529953956604,
      "epoch": 8.07,
      "step": 1614
    },
    {
      "epoch": 8.075,
      "grad_norm": 19.0605411529541,
      "learning_rate": 1.20625e-05,
      "loss": 0.1955,
      "step": 1615
    },
    {
      "bce_eff_w": 0.38239999999999996,
      "bce_loss": -0.46385741233825684,
      "epoch": 8.075,
      "step": 1615
    },
    {
      "epoch": 8.08,
      "grad_norm": 5.334596633911133,
      "learning_rate": 1.2031250000000001e-05,
      "loss": -0.1196,
      "step": 1616
    },
    {
      "bce_eff_w": 0.38255000000000006,
      "bce_loss": -0.46384212374687195,
      "epoch": 8.08,
      "step": 1616
    },
    {
      "epoch": 8.085,
      "grad_norm": 16.41745948791504,
      "learning_rate": 1.2e-05,
      "loss": 0.1857,
      "step": 1617
    },
    {
      "bce_eff_w": 0.38270000000000004,
      "bce_loss": -0.4610786736011505,
      "epoch": 8.085,
      "step": 1617
    },
    {
      "epoch": 8.09,
      "grad_norm": 17.02878761291504,
      "learning_rate": 1.1968750000000001e-05,
      "loss": 0.2184,
      "step": 1618
    },
    {
      "bce_eff_w": 0.38285,
      "bce_loss": -0.4597274959087372,
      "epoch": 8.09,
      "step": 1618
    },
    {
      "epoch": 8.095,
      "grad_norm": 17.787534713745117,
      "learning_rate": 1.19375e-05,
      "loss": 0.4031,
      "step": 1619
    },
    {
      "bce_eff_w": 0.383,
      "bce_loss": -0.46356186270713806,
      "epoch": 8.095,
      "step": 1619
    },
    {
      "epoch": 8.1,
      "grad_norm": 18.348541259765625,
      "learning_rate": 1.1906250000000001e-05,
      "loss": 0.4888,
      "step": 1620
    },
    {
      "bce_eff_w": 0.38315,
      "bce_loss": -0.4612775146961212,
      "epoch": 8.1,
      "step": 1620
    },
    {
      "epoch": 8.105,
      "grad_norm": 11.620516777038574,
      "learning_rate": 1.1875e-05,
      "loss": -0.0005,
      "step": 1621
    },
    {
      "bce_eff_w": 0.3833,
      "bce_loss": -0.4636518359184265,
      "epoch": 8.105,
      "step": 1621
    },
    {
      "epoch": 8.11,
      "grad_norm": 16.336130142211914,
      "learning_rate": 1.184375e-05,
      "loss": 1.2605,
      "step": 1622
    },
    {
      "bce_eff_w": 0.38345,
      "bce_loss": -0.4611894190311432,
      "epoch": 8.11,
      "step": 1622
    },
    {
      "epoch": 8.115,
      "grad_norm": 18.08296012878418,
      "learning_rate": 1.1812499999999999e-05,
      "loss": 1.1089,
      "step": 1623
    },
    {
      "bce_eff_w": 0.3836,
      "bce_loss": -0.46302756667137146,
      "epoch": 8.115,
      "step": 1623
    },
    {
      "epoch": 8.12,
      "grad_norm": 22.594696044921875,
      "learning_rate": 1.178125e-05,
      "loss": 0.2987,
      "step": 1624
    },
    {
      "bce_eff_w": 0.38375000000000004,
      "bce_loss": -0.460188627243042,
      "epoch": 8.12,
      "step": 1624
    },
    {
      "epoch": 8.125,
      "grad_norm": 22.96837615966797,
      "learning_rate": 1.175e-05,
      "loss": 0.0447,
      "step": 1625
    },
    {
      "bce_eff_w": 0.3839,
      "bce_loss": -0.4517137110233307,
      "epoch": 8.125,
      "step": 1625
    },
    {
      "epoch": 8.13,
      "grad_norm": 13.231547355651855,
      "learning_rate": 1.171875e-05,
      "loss": 2.1152,
      "step": 1626
    },
    {
      "bce_eff_w": 0.38405,
      "bce_loss": -0.4639304578304291,
      "epoch": 8.13,
      "step": 1626
    },
    {
      "epoch": 8.135,
      "grad_norm": 15.269669532775879,
      "learning_rate": 1.1687500000000001e-05,
      "loss": 0.7734,
      "step": 1627
    },
    {
      "bce_eff_w": 0.3842,
      "bce_loss": -0.45745617151260376,
      "epoch": 8.135,
      "step": 1627
    },
    {
      "epoch": 8.14,
      "grad_norm": 27.84762954711914,
      "learning_rate": 1.165625e-05,
      "loss": 0.8989,
      "step": 1628
    },
    {
      "bce_eff_w": 0.38435,
      "bce_loss": -0.46011486649513245,
      "epoch": 8.14,
      "step": 1628
    },
    {
      "epoch": 8.145,
      "grad_norm": 13.096786499023438,
      "learning_rate": 1.1625000000000001e-05,
      "loss": 2.0751,
      "step": 1629
    },
    {
      "bce_eff_w": 0.3845,
      "bce_loss": -0.4611547887325287,
      "epoch": 8.145,
      "step": 1629
    },
    {
      "epoch": 8.15,
      "grad_norm": 20.110010147094727,
      "learning_rate": 1.159375e-05,
      "loss": 0.2805,
      "step": 1630
    },
    {
      "bce_eff_w": 0.38465000000000005,
      "bce_loss": -0.4625930190086365,
      "epoch": 8.15,
      "step": 1630
    },
    {
      "epoch": 8.155,
      "grad_norm": 16.72382164001465,
      "learning_rate": 1.1562500000000002e-05,
      "loss": 0.1686,
      "step": 1631
    },
    {
      "bce_eff_w": 0.38480000000000003,
      "bce_loss": -0.46173718571662903,
      "epoch": 8.155,
      "step": 1631
    },
    {
      "epoch": 8.16,
      "grad_norm": 21.794208526611328,
      "learning_rate": 1.153125e-05,
      "loss": 0.1535,
      "step": 1632
    },
    {
      "bce_eff_w": 0.38495,
      "bce_loss": -0.45830535888671875,
      "epoch": 8.16,
      "step": 1632
    },
    {
      "epoch": 8.165,
      "grad_norm": 18.536218643188477,
      "learning_rate": 1.1500000000000002e-05,
      "loss": 0.2546,
      "step": 1633
    },
    {
      "bce_eff_w": 0.3851,
      "bce_loss": -0.4535323679447174,
      "epoch": 8.165,
      "step": 1633
    },
    {
      "epoch": 8.17,
      "grad_norm": 24.92622184753418,
      "learning_rate": 1.1468750000000001e-05,
      "loss": 0.1769,
      "step": 1634
    },
    {
      "bce_eff_w": 0.38525,
      "bce_loss": -0.4626806676387787,
      "epoch": 8.17,
      "step": 1634
    },
    {
      "epoch": 8.175,
      "grad_norm": 18.945505142211914,
      "learning_rate": 1.14375e-05,
      "loss": 0.6046,
      "step": 1635
    },
    {
      "bce_eff_w": 0.38539999999999996,
      "bce_loss": -0.46312305331230164,
      "epoch": 8.175,
      "step": 1635
    },
    {
      "epoch": 8.18,
      "grad_norm": 11.93294906616211,
      "learning_rate": 1.140625e-05,
      "loss": 1.5422,
      "step": 1636
    },
    {
      "bce_eff_w": 0.38555000000000006,
      "bce_loss": -0.45728224515914917,
      "epoch": 8.18,
      "step": 1636
    },
    {
      "epoch": 8.185,
      "grad_norm": 26.56401252746582,
      "learning_rate": 1.1375e-05,
      "loss": 0.2382,
      "step": 1637
    },
    {
      "bce_eff_w": 0.38570000000000004,
      "bce_loss": -0.46088093519210815,
      "epoch": 8.185,
      "step": 1637
    },
    {
      "epoch": 8.19,
      "grad_norm": 18.131122589111328,
      "learning_rate": 1.134375e-05,
      "loss": 1.7147,
      "step": 1638
    },
    {
      "bce_eff_w": 0.38585,
      "bce_loss": -0.4635219871997833,
      "epoch": 8.19,
      "step": 1638
    },
    {
      "epoch": 8.195,
      "grad_norm": 25.302776336669922,
      "learning_rate": 1.13125e-05,
      "loss": 1.019,
      "step": 1639
    },
    {
      "bce_eff_w": 0.386,
      "bce_loss": -0.4600883722305298,
      "epoch": 8.195,
      "step": 1639
    },
    {
      "epoch": 8.2,
      "grad_norm": 15.04467487335205,
      "learning_rate": 1.128125e-05,
      "loss": 2.1156,
      "step": 1640
    },
    {
      "bce_eff_w": 0.38615,
      "bce_loss": -0.4615021049976349,
      "epoch": 8.2,
      "step": 1640
    },
    {
      "epoch": 8.205,
      "grad_norm": 13.704519271850586,
      "learning_rate": 1.125e-05,
      "loss": -0.0913,
      "step": 1641
    },
    {
      "bce_eff_w": 0.3863,
      "bce_loss": -0.46262821555137634,
      "epoch": 8.205,
      "step": 1641
    },
    {
      "epoch": 8.21,
      "grad_norm": 25.22516441345215,
      "learning_rate": 1.121875e-05,
      "loss": 0.2213,
      "step": 1642
    },
    {
      "bce_eff_w": 0.38645,
      "bce_loss": -0.45983704924583435,
      "epoch": 8.21,
      "step": 1642
    },
    {
      "epoch": 8.215,
      "grad_norm": 18.732624053955078,
      "learning_rate": 1.1187500000000001e-05,
      "loss": 1.5363,
      "step": 1643
    },
    {
      "bce_eff_w": 0.3866,
      "bce_loss": -0.46200674772262573,
      "epoch": 8.215,
      "step": 1643
    },
    {
      "epoch": 8.22,
      "grad_norm": 18.842670440673828,
      "learning_rate": 1.115625e-05,
      "loss": 0.6717,
      "step": 1644
    },
    {
      "bce_eff_w": 0.38675000000000004,
      "bce_loss": -0.46174976229667664,
      "epoch": 8.22,
      "step": 1644
    },
    {
      "epoch": 8.225,
      "grad_norm": 19.737106323242188,
      "learning_rate": 1.1125000000000001e-05,
      "loss": 0.3913,
      "step": 1645
    },
    {
      "bce_eff_w": 0.3869,
      "bce_loss": -0.4602920711040497,
      "epoch": 8.225,
      "step": 1645
    },
    {
      "epoch": 8.23,
      "grad_norm": 21.3006534576416,
      "learning_rate": 1.109375e-05,
      "loss": 0.4482,
      "step": 1646
    },
    {
      "bce_eff_w": 0.38705,
      "bce_loss": -0.4625217020511627,
      "epoch": 8.23,
      "step": 1646
    },
    {
      "epoch": 8.235,
      "grad_norm": 12.887259483337402,
      "learning_rate": 1.1062500000000001e-05,
      "loss": 1.276,
      "step": 1647
    },
    {
      "bce_eff_w": 0.3872,
      "bce_loss": -0.4596376419067383,
      "epoch": 8.235,
      "step": 1647
    },
    {
      "epoch": 8.24,
      "grad_norm": 32.03740692138672,
      "learning_rate": 1.103125e-05,
      "loss": 0.5602,
      "step": 1648
    },
    {
      "bce_eff_w": 0.38735,
      "bce_loss": -0.4541166126728058,
      "epoch": 8.24,
      "step": 1648
    },
    {
      "epoch": 8.245,
      "grad_norm": 19.7945556640625,
      "learning_rate": 1.1000000000000001e-05,
      "loss": 1.0636,
      "step": 1649
    },
    {
      "bce_eff_w": 0.3875,
      "bce_loss": -0.4610762894153595,
      "epoch": 8.245,
      "step": 1649
    },
    {
      "epoch": 8.25,
      "grad_norm": 17.6866512298584,
      "learning_rate": 1.096875e-05,
      "loss": 0.8507,
      "step": 1650
    },
    {
      "bce_eff_w": 0.38765,
      "bce_loss": -0.46033933758735657,
      "epoch": 8.25,
      "step": 1650
    },
    {
      "epoch": 8.255,
      "grad_norm": 18.896562576293945,
      "learning_rate": 1.09375e-05,
      "loss": 0.8138,
      "step": 1651
    },
    {
      "bce_eff_w": 0.38780000000000003,
      "bce_loss": -0.45004281401634216,
      "epoch": 8.255,
      "step": 1651
    },
    {
      "epoch": 8.26,
      "grad_norm": 7.761762619018555,
      "learning_rate": 1.090625e-05,
      "loss": 0.0213,
      "step": 1652
    },
    {
      "bce_eff_w": 0.38795,
      "bce_loss": -0.4512011408805847,
      "epoch": 8.26,
      "step": 1652
    },
    {
      "epoch": 8.265,
      "grad_norm": 24.15253448486328,
      "learning_rate": 1.0875e-05,
      "loss": 0.5825,
      "step": 1653
    },
    {
      "bce_eff_w": 0.3881,
      "bce_loss": -0.45952901244163513,
      "epoch": 8.265,
      "step": 1653
    },
    {
      "epoch": 8.27,
      "grad_norm": 18.92736053466797,
      "learning_rate": 1.0843750000000001e-05,
      "loss": 1.2438,
      "step": 1654
    },
    {
      "bce_eff_w": 0.38825,
      "bce_loss": -0.4605133831501007,
      "epoch": 8.27,
      "step": 1654
    },
    {
      "epoch": 8.275,
      "grad_norm": 10.285524368286133,
      "learning_rate": 1.08125e-05,
      "loss": 0.0557,
      "step": 1655
    },
    {
      "bce_eff_w": 0.38839999999999997,
      "bce_loss": -0.45195886492729187,
      "epoch": 8.275,
      "step": 1655
    },
    {
      "epoch": 8.28,
      "grad_norm": 38.791316986083984,
      "learning_rate": 1.0781250000000001e-05,
      "loss": 1.7562,
      "step": 1656
    },
    {
      "bce_eff_w": 0.38854999999999995,
      "bce_loss": -0.46145838499069214,
      "epoch": 8.28,
      "step": 1656
    },
    {
      "epoch": 8.285,
      "grad_norm": 33.49607849121094,
      "learning_rate": 1.075e-05,
      "loss": 1.7906,
      "step": 1657
    },
    {
      "bce_eff_w": 0.38870000000000005,
      "bce_loss": -0.4521847665309906,
      "epoch": 8.285,
      "step": 1657
    },
    {
      "epoch": 8.29,
      "grad_norm": 59.773948669433594,
      "learning_rate": 1.0718750000000001e-05,
      "loss": 0.7938,
      "step": 1658
    },
    {
      "bce_eff_w": 0.38885000000000003,
      "bce_loss": -0.45947080850601196,
      "epoch": 8.29,
      "step": 1658
    },
    {
      "epoch": 8.295,
      "grad_norm": 13.391396522521973,
      "learning_rate": 1.06875e-05,
      "loss": 0.1529,
      "step": 1659
    },
    {
      "bce_eff_w": 0.389,
      "bce_loss": -0.4565325379371643,
      "epoch": 8.295,
      "step": 1659
    },
    {
      "epoch": 8.3,
      "grad_norm": 13.865943908691406,
      "learning_rate": 1.0656250000000002e-05,
      "loss": 1.9645,
      "step": 1660
    },
    {
      "bce_eff_w": 0.38915,
      "bce_loss": -0.45458361506462097,
      "epoch": 8.3,
      "step": 1660
    },
    {
      "epoch": 8.305,
      "grad_norm": 17.821609497070312,
      "learning_rate": 1.0625e-05,
      "loss": 0.8313,
      "step": 1661
    },
    {
      "bce_eff_w": 0.3893,
      "bce_loss": -0.46001946926116943,
      "epoch": 8.305,
      "step": 1661
    },
    {
      "epoch": 8.31,
      "grad_norm": 19.909086227416992,
      "learning_rate": 1.0593750000000002e-05,
      "loss": 0.9185,
      "step": 1662
    },
    {
      "bce_eff_w": 0.38944999999999996,
      "bce_loss": -0.4629479944705963,
      "epoch": 8.31,
      "step": 1662
    },
    {
      "epoch": 8.315,
      "grad_norm": 17.20311737060547,
      "learning_rate": 1.0562500000000001e-05,
      "loss": 1.4244,
      "step": 1663
    },
    {
      "bce_eff_w": 0.3896,
      "bce_loss": -0.4512978494167328,
      "epoch": 8.315,
      "step": 1663
    },
    {
      "epoch": 8.32,
      "grad_norm": 15.90184497833252,
      "learning_rate": 1.053125e-05,
      "loss": 1.8756,
      "step": 1664
    },
    {
      "bce_eff_w": 0.38975,
      "bce_loss": -0.46221011877059937,
      "epoch": 8.32,
      "step": 1664
    },
    {
      "epoch": 8.325,
      "grad_norm": 16.46419334411621,
      "learning_rate": 1.05e-05,
      "loss": 0.1541,
      "step": 1665
    },
    {
      "bce_eff_w": 0.3899,
      "bce_loss": -0.45808371901512146,
      "epoch": 8.325,
      "step": 1665
    },
    {
      "epoch": 8.33,
      "grad_norm": 23.750995635986328,
      "learning_rate": 1.046875e-05,
      "loss": 1.0123,
      "step": 1666
    },
    {
      "bce_eff_w": 0.39005,
      "bce_loss": -0.45831963419914246,
      "epoch": 8.33,
      "step": 1666
    },
    {
      "epoch": 8.335,
      "grad_norm": 14.092852592468262,
      "learning_rate": 1.04375e-05,
      "loss": 0.0456,
      "step": 1667
    },
    {
      "bce_eff_w": 0.3902,
      "bce_loss": -0.45494017004966736,
      "epoch": 8.335,
      "step": 1667
    },
    {
      "epoch": 8.34,
      "grad_norm": 21.53160285949707,
      "learning_rate": 1.040625e-05,
      "loss": 0.4331,
      "step": 1668
    },
    {
      "bce_eff_w": 0.39035,
      "bce_loss": -0.4622252881526947,
      "epoch": 8.34,
      "step": 1668
    },
    {
      "epoch": 8.345,
      "grad_norm": 18.10653305053711,
      "learning_rate": 1.0375e-05,
      "loss": 1.4387,
      "step": 1669
    },
    {
      "bce_eff_w": 0.3905,
      "bce_loss": -0.45822760462760925,
      "epoch": 8.345,
      "step": 1669
    },
    {
      "epoch": 8.35,
      "grad_norm": 20.776975631713867,
      "learning_rate": 1.034375e-05,
      "loss": 0.6905,
      "step": 1670
    },
    {
      "bce_eff_w": 0.39065,
      "bce_loss": -0.4621453285217285,
      "epoch": 8.35,
      "step": 1670
    },
    {
      "epoch": 8.355,
      "grad_norm": 13.127023696899414,
      "learning_rate": 1.03125e-05,
      "loss": 1.1232,
      "step": 1671
    },
    {
      "bce_eff_w": 0.39080000000000004,
      "bce_loss": -0.4626280665397644,
      "epoch": 8.355,
      "step": 1671
    },
    {
      "epoch": 8.36,
      "grad_norm": 13.948440551757812,
      "learning_rate": 1.0281250000000001e-05,
      "loss": 0.0668,
      "step": 1672
    },
    {
      "bce_eff_w": 0.39095,
      "bce_loss": -0.4613649547100067,
      "epoch": 8.36,
      "step": 1672
    },
    {
      "epoch": 8.365,
      "grad_norm": 17.07377815246582,
      "learning_rate": 1.025e-05,
      "loss": 0.1002,
      "step": 1673
    },
    {
      "bce_eff_w": 0.3911,
      "bce_loss": -0.46367523074150085,
      "epoch": 8.365,
      "step": 1673
    },
    {
      "epoch": 8.37,
      "grad_norm": 17.559837341308594,
      "learning_rate": 1.0218750000000001e-05,
      "loss": 0.4172,
      "step": 1674
    },
    {
      "bce_eff_w": 0.39125,
      "bce_loss": -0.4590359032154083,
      "epoch": 8.37,
      "step": 1674
    },
    {
      "epoch": 8.375,
      "grad_norm": 19.671178817749023,
      "learning_rate": 1.01875e-05,
      "loss": 0.2818,
      "step": 1675
    },
    {
      "bce_eff_w": 0.39139999999999997,
      "bce_loss": -0.46295109391212463,
      "epoch": 8.375,
      "step": 1675
    },
    {
      "epoch": 8.38,
      "grad_norm": 19.770416259765625,
      "learning_rate": 1.0156250000000001e-05,
      "loss": 0.7444,
      "step": 1676
    },
    {
      "bce_eff_w": 0.39154999999999995,
      "bce_loss": -0.46195319294929504,
      "epoch": 8.38,
      "step": 1676
    },
    {
      "epoch": 8.385,
      "grad_norm": 19.56478500366211,
      "learning_rate": 1.0125e-05,
      "loss": 1.248,
      "step": 1677
    },
    {
      "bce_eff_w": 0.39170000000000005,
      "bce_loss": -0.461791455745697,
      "epoch": 8.385,
      "step": 1677
    },
    {
      "epoch": 8.39,
      "grad_norm": 15.512916564941406,
      "learning_rate": 1.009375e-05,
      "loss": 0.5381,
      "step": 1678
    },
    {
      "bce_eff_w": 0.39185000000000003,
      "bce_loss": -0.4628656208515167,
      "epoch": 8.39,
      "step": 1678
    },
    {
      "epoch": 8.395,
      "grad_norm": 17.095617294311523,
      "learning_rate": 1.00625e-05,
      "loss": 0.4317,
      "step": 1679
    },
    {
      "bce_eff_w": 0.392,
      "bce_loss": -0.45147526264190674,
      "epoch": 8.395,
      "step": 1679
    },
    {
      "epoch": 8.4,
      "grad_norm": 18.83687400817871,
      "learning_rate": 1.003125e-05,
      "loss": 0.5662,
      "step": 1680
    },
    {
      "bce_eff_w": 0.39215,
      "bce_loss": -0.45515790581703186,
      "epoch": 8.4,
      "step": 1680
    },
    {
      "epoch": 8.405,
      "grad_norm": 18.940744400024414,
      "learning_rate": 1e-05,
      "loss": 1.6021,
      "step": 1681
    },
    {
      "bce_eff_w": 0.3923,
      "bce_loss": -0.44536659121513367,
      "epoch": 8.405,
      "step": 1681
    },
    {
      "epoch": 8.41,
      "grad_norm": 13.615339279174805,
      "learning_rate": 9.96875e-06,
      "loss": 0.2903,
      "step": 1682
    },
    {
      "bce_eff_w": 0.39244999999999997,
      "bce_loss": -0.45623913407325745,
      "epoch": 8.41,
      "step": 1682
    },
    {
      "epoch": 8.415,
      "grad_norm": 26.03141212463379,
      "learning_rate": 9.937500000000001e-06,
      "loss": 1.0437,
      "step": 1683
    },
    {
      "bce_eff_w": 0.3926,
      "bce_loss": -0.45218950510025024,
      "epoch": 8.415,
      "step": 1683
    },
    {
      "epoch": 8.42,
      "grad_norm": 22.732994079589844,
      "learning_rate": 9.90625e-06,
      "loss": 0.9136,
      "step": 1684
    },
    {
      "bce_eff_w": 0.39275,
      "bce_loss": -0.4636766314506531,
      "epoch": 8.42,
      "step": 1684
    },
    {
      "epoch": 8.425,
      "grad_norm": 15.528855323791504,
      "learning_rate": 9.875000000000001e-06,
      "loss": 0.5606,
      "step": 1685
    },
    {
      "bce_eff_w": 0.3929,
      "bce_loss": -0.45974287390708923,
      "epoch": 8.425,
      "step": 1685
    },
    {
      "epoch": 8.43,
      "grad_norm": 16.13234519958496,
      "learning_rate": 9.84375e-06,
      "loss": 0.8662,
      "step": 1686
    },
    {
      "bce_eff_w": 0.39305,
      "bce_loss": -0.4624682366847992,
      "epoch": 8.43,
      "step": 1686
    },
    {
      "epoch": 8.435,
      "grad_norm": 11.045869827270508,
      "learning_rate": 9.812500000000001e-06,
      "loss": -0.0724,
      "step": 1687
    },
    {
      "bce_eff_w": 0.3932,
      "bce_loss": -0.46217918395996094,
      "epoch": 8.435,
      "step": 1687
    },
    {
      "epoch": 8.44,
      "grad_norm": 20.408357620239258,
      "learning_rate": 9.78125e-06,
      "loss": 1.2316,
      "step": 1688
    },
    {
      "bce_eff_w": 0.39335,
      "bce_loss": -0.4596311151981354,
      "epoch": 8.44,
      "step": 1688
    },
    {
      "epoch": 8.445,
      "grad_norm": 18.415788650512695,
      "learning_rate": 9.750000000000002e-06,
      "loss": 1.63,
      "step": 1689
    },
    {
      "bce_eff_w": 0.3935,
      "bce_loss": -0.46388378739356995,
      "epoch": 8.445,
      "step": 1689
    },
    {
      "epoch": 8.45,
      "grad_norm": 12.29081916809082,
      "learning_rate": 9.71875e-06,
      "loss": 0.0697,
      "step": 1690
    },
    {
      "bce_eff_w": 0.39365,
      "bce_loss": -0.46192076802253723,
      "epoch": 8.45,
      "step": 1690
    },
    {
      "epoch": 8.455,
      "grad_norm": 22.462751388549805,
      "learning_rate": 9.6875e-06,
      "loss": 0.5409,
      "step": 1691
    },
    {
      "bce_eff_w": 0.39380000000000004,
      "bce_loss": -0.4600617587566376,
      "epoch": 8.455,
      "step": 1691
    },
    {
      "epoch": 8.46,
      "grad_norm": 14.083200454711914,
      "learning_rate": 9.65625e-06,
      "loss": 0.058,
      "step": 1692
    },
    {
      "bce_eff_w": 0.39395,
      "bce_loss": -0.46014758944511414,
      "epoch": 8.46,
      "step": 1692
    },
    {
      "epoch": 8.465,
      "grad_norm": 19.678213119506836,
      "learning_rate": 9.625e-06,
      "loss": 0.3331,
      "step": 1693
    },
    {
      "bce_eff_w": 0.3941,
      "bce_loss": -0.4628516137599945,
      "epoch": 8.465,
      "step": 1693
    },
    {
      "epoch": 8.47,
      "grad_norm": 41.533565521240234,
      "learning_rate": 9.59375e-06,
      "loss": 1.1663,
      "step": 1694
    },
    {
      "bce_eff_w": 0.39425,
      "bce_loss": -0.4595434367656708,
      "epoch": 8.47,
      "step": 1694
    },
    {
      "epoch": 8.475,
      "grad_norm": 14.372005462646484,
      "learning_rate": 9.5625e-06,
      "loss": 0.0968,
      "step": 1695
    },
    {
      "bce_eff_w": 0.3944,
      "bce_loss": -0.4625040888786316,
      "epoch": 8.475,
      "step": 1695
    },
    {
      "epoch": 8.48,
      "grad_norm": 22.48683738708496,
      "learning_rate": 9.53125e-06,
      "loss": 0.607,
      "step": 1696
    },
    {
      "bce_eff_w": 0.39454999999999996,
      "bce_loss": -0.46094101667404175,
      "epoch": 8.48,
      "step": 1696
    },
    {
      "epoch": 8.485,
      "grad_norm": 14.637262344360352,
      "learning_rate": 9.5e-06,
      "loss": 0.7541,
      "step": 1697
    },
    {
      "bce_eff_w": 0.39470000000000005,
      "bce_loss": -0.4614449441432953,
      "epoch": 8.485,
      "step": 1697
    },
    {
      "epoch": 8.49,
      "grad_norm": 21.503496170043945,
      "learning_rate": 9.46875e-06,
      "loss": 0.4117,
      "step": 1698
    },
    {
      "bce_eff_w": 0.39485000000000003,
      "bce_loss": -0.4567755162715912,
      "epoch": 8.49,
      "step": 1698
    },
    {
      "epoch": 8.495,
      "grad_norm": 17.006717681884766,
      "learning_rate": 9.4375e-06,
      "loss": 0.5531,
      "step": 1699
    },
    {
      "bce_eff_w": 0.395,
      "bce_loss": -0.46035686135292053,
      "epoch": 8.495,
      "step": 1699
    },
    {
      "epoch": 8.5,
      "grad_norm": 16.491601943969727,
      "learning_rate": 9.40625e-06,
      "loss": 0.6734,
      "step": 1700
    },
    {
      "bce_eff_w": 0.39515,
      "bce_loss": -0.4630424976348877,
      "epoch": 8.5,
      "step": 1700
    },
    {
      "epoch": 8.505,
      "grad_norm": 24.742643356323242,
      "learning_rate": 9.375000000000001e-06,
      "loss": 1.0777,
      "step": 1701
    },
    {
      "bce_eff_w": 0.3953,
      "bce_loss": -0.4618323743343353,
      "epoch": 8.505,
      "step": 1701
    },
    {
      "epoch": 8.51,
      "grad_norm": 30.367881774902344,
      "learning_rate": 9.343750000000002e-06,
      "loss": 1.1416,
      "step": 1702
    },
    {
      "bce_eff_w": 0.39544999999999997,
      "bce_loss": -0.4628250300884247,
      "epoch": 8.51,
      "step": 1702
    },
    {
      "epoch": 8.515,
      "grad_norm": 24.46126365661621,
      "learning_rate": 9.312500000000001e-06,
      "loss": 0.6529,
      "step": 1703
    },
    {
      "bce_eff_w": 0.3956,
      "bce_loss": -0.456597238779068,
      "epoch": 8.515,
      "step": 1703
    },
    {
      "epoch": 8.52,
      "grad_norm": 19.214366912841797,
      "learning_rate": 9.28125e-06,
      "loss": 0.34,
      "step": 1704
    },
    {
      "bce_eff_w": 0.39575,
      "bce_loss": -0.45932674407958984,
      "epoch": 8.52,
      "step": 1704
    },
    {
      "epoch": 8.525,
      "grad_norm": 10.709681510925293,
      "learning_rate": 9.25e-06,
      "loss": 0.095,
      "step": 1705
    },
    {
      "bce_eff_w": 0.39590000000000003,
      "bce_loss": -0.45578280091285706,
      "epoch": 8.525,
      "step": 1705
    },
    {
      "epoch": 8.53,
      "grad_norm": 24.830976486206055,
      "learning_rate": 9.21875e-06,
      "loss": 0.6714,
      "step": 1706
    },
    {
      "bce_eff_w": 0.39605,
      "bce_loss": -0.46339407563209534,
      "epoch": 8.53,
      "step": 1706
    },
    {
      "epoch": 8.535,
      "grad_norm": 20.54653549194336,
      "learning_rate": 9.1875e-06,
      "loss": 0.2349,
      "step": 1707
    },
    {
      "bce_eff_w": 0.3962,
      "bce_loss": -0.4630969762802124,
      "epoch": 8.535,
      "step": 1707
    },
    {
      "epoch": 8.54,
      "grad_norm": 14.220748901367188,
      "learning_rate": 9.15625e-06,
      "loss": 0.3002,
      "step": 1708
    },
    {
      "bce_eff_w": 0.39635,
      "bce_loss": -0.4649685323238373,
      "epoch": 8.54,
      "step": 1708
    },
    {
      "epoch": 8.545,
      "grad_norm": 25.222808837890625,
      "learning_rate": 9.125e-06,
      "loss": 0.4543,
      "step": 1709
    },
    {
      "bce_eff_w": 0.3965,
      "bce_loss": -0.46316272020339966,
      "epoch": 8.545,
      "step": 1709
    },
    {
      "epoch": 8.55,
      "grad_norm": 50.223297119140625,
      "learning_rate": 9.09375e-06,
      "loss": 0.3861,
      "step": 1710
    },
    {
      "bce_eff_w": 0.39665,
      "bce_loss": -0.4636187255382538,
      "epoch": 8.55,
      "step": 1710
    },
    {
      "epoch": 8.555,
      "grad_norm": 16.632524490356445,
      "learning_rate": 9.0625e-06,
      "loss": 0.9899,
      "step": 1711
    },
    {
      "bce_eff_w": 0.39680000000000004,
      "bce_loss": -0.4633902609348297,
      "epoch": 8.555,
      "step": 1711
    },
    {
      "epoch": 8.56,
      "grad_norm": 12.928359031677246,
      "learning_rate": 9.031250000000001e-06,
      "loss": 1.0362,
      "step": 1712
    },
    {
      "bce_eff_w": 0.39695,
      "bce_loss": -0.46232470870018005,
      "epoch": 8.56,
      "step": 1712
    },
    {
      "epoch": 8.565,
      "grad_norm": 16.86397933959961,
      "learning_rate": 9e-06,
      "loss": 1.1041,
      "step": 1713
    },
    {
      "bce_eff_w": 0.3971,
      "bce_loss": -0.4643988311290741,
      "epoch": 8.565,
      "step": 1713
    },
    {
      "epoch": 8.57,
      "grad_norm": 18.460708618164062,
      "learning_rate": 8.968750000000001e-06,
      "loss": 0.8693,
      "step": 1714
    },
    {
      "bce_eff_w": 0.39725,
      "bce_loss": -0.46255362033843994,
      "epoch": 8.57,
      "step": 1714
    },
    {
      "epoch": 8.575,
      "grad_norm": 18.489578247070312,
      "learning_rate": 8.9375e-06,
      "loss": 0.217,
      "step": 1715
    },
    {
      "bce_eff_w": 0.3974,
      "bce_loss": -0.4555853307247162,
      "epoch": 8.575,
      "step": 1715
    },
    {
      "epoch": 8.58,
      "grad_norm": 15.58626937866211,
      "learning_rate": 8.906250000000001e-06,
      "loss": 0.4169,
      "step": 1716
    },
    {
      "bce_eff_w": 0.39754999999999996,
      "bce_loss": -0.4601849615573883,
      "epoch": 8.58,
      "step": 1716
    },
    {
      "epoch": 8.585,
      "grad_norm": 17.34469223022461,
      "learning_rate": 8.875e-06,
      "loss": 0.0624,
      "step": 1717
    },
    {
      "bce_eff_w": 0.39770000000000005,
      "bce_loss": -0.4558808505535126,
      "epoch": 8.585,
      "step": 1717
    },
    {
      "epoch": 8.59,
      "grad_norm": 12.321369171142578,
      "learning_rate": 8.84375e-06,
      "loss": 1.9887,
      "step": 1718
    },
    {
      "bce_eff_w": 0.39785000000000004,
      "bce_loss": -0.4643036127090454,
      "epoch": 8.59,
      "step": 1718
    },
    {
      "epoch": 8.595,
      "grad_norm": 19.25600814819336,
      "learning_rate": 8.8125e-06,
      "loss": 0.0852,
      "step": 1719
    },
    {
      "bce_eff_w": 0.398,
      "bce_loss": -0.45825091004371643,
      "epoch": 8.595,
      "step": 1719
    },
    {
      "epoch": 8.6,
      "grad_norm": 14.570247650146484,
      "learning_rate": 8.78125e-06,
      "loss": 1.8491,
      "step": 1720
    },
    {
      "bce_eff_w": 0.39815,
      "bce_loss": -0.4588395059108734,
      "epoch": 8.6,
      "step": 1720
    },
    {
      "epoch": 8.605,
      "grad_norm": 5.804566860198975,
      "learning_rate": 8.75e-06,
      "loss": -0.0863,
      "step": 1721
    },
    {
      "bce_eff_w": 0.3983,
      "bce_loss": -0.45600956678390503,
      "epoch": 8.605,
      "step": 1721
    },
    {
      "epoch": 8.61,
      "grad_norm": 16.07774543762207,
      "learning_rate": 8.71875e-06,
      "loss": 0.5734,
      "step": 1722
    },
    {
      "bce_eff_w": 0.39844999999999997,
      "bce_loss": -0.4617924392223358,
      "epoch": 8.61,
      "step": 1722
    },
    {
      "epoch": 8.615,
      "grad_norm": 25.229291915893555,
      "learning_rate": 8.6875e-06,
      "loss": 0.6576,
      "step": 1723
    },
    {
      "bce_eff_w": 0.3986,
      "bce_loss": -0.45801398158073425,
      "epoch": 8.615,
      "step": 1723
    },
    {
      "epoch": 8.62,
      "grad_norm": 6.823099136352539,
      "learning_rate": 8.65625e-06,
      "loss": -0.0779,
      "step": 1724
    },
    {
      "bce_eff_w": 0.39875,
      "bce_loss": -0.46121683716773987,
      "epoch": 8.62,
      "step": 1724
    },
    {
      "epoch": 8.625,
      "grad_norm": 23.82064437866211,
      "learning_rate": 8.625e-06,
      "loss": 1.0762,
      "step": 1725
    },
    {
      "bce_eff_w": 0.39890000000000003,
      "bce_loss": -0.45879101753234863,
      "epoch": 8.625,
      "step": 1725
    },
    {
      "epoch": 8.63,
      "grad_norm": 8.935760498046875,
      "learning_rate": 8.59375e-06,
      "loss": -0.0572,
      "step": 1726
    },
    {
      "bce_eff_w": 0.39905,
      "bce_loss": -0.46031811833381653,
      "epoch": 8.63,
      "step": 1726
    },
    {
      "epoch": 8.635,
      "grad_norm": 1.3584215641021729,
      "learning_rate": 8.562500000000001e-06,
      "loss": -0.1681,
      "step": 1727
    },
    {
      "bce_eff_w": 0.3992,
      "bce_loss": -0.4620803892612457,
      "epoch": 8.635,
      "step": 1727
    },
    {
      "epoch": 8.64,
      "grad_norm": 26.32529640197754,
      "learning_rate": 8.53125e-06,
      "loss": 0.2939,
      "step": 1728
    },
    {
      "bce_eff_w": 0.39935,
      "bce_loss": -0.4608563184738159,
      "epoch": 8.64,
      "step": 1728
    },
    {
      "epoch": 8.645,
      "grad_norm": 16.53861427307129,
      "learning_rate": 8.500000000000002e-06,
      "loss": 0.2602,
      "step": 1729
    },
    {
      "bce_eff_w": 0.3995,
      "bce_loss": -0.45987004041671753,
      "epoch": 8.645,
      "step": 1729
    },
    {
      "epoch": 8.65,
      "grad_norm": 12.992063522338867,
      "learning_rate": 8.468750000000001e-06,
      "loss": -0.0402,
      "step": 1730
    },
    {
      "bce_eff_w": 0.39965,
      "bce_loss": -0.4625753164291382,
      "epoch": 8.65,
      "step": 1730
    },
    {
      "epoch": 8.655,
      "grad_norm": 24.911510467529297,
      "learning_rate": 8.437500000000002e-06,
      "loss": 0.7202,
      "step": 1731
    },
    {
      "bce_eff_w": 0.39980000000000004,
      "bce_loss": -0.452923983335495,
      "epoch": 8.655,
      "step": 1731
    },
    {
      "epoch": 8.66,
      "grad_norm": 14.138727188110352,
      "learning_rate": 8.406250000000001e-06,
      "loss": 0.4432,
      "step": 1732
    },
    {
      "bce_eff_w": 0.39995,
      "bce_loss": -0.46098896861076355,
      "epoch": 8.66,
      "step": 1732
    },
    {
      "epoch": 8.665,
      "grad_norm": 6.713146209716797,
      "learning_rate": 8.375e-06,
      "loss": -0.0847,
      "step": 1733
    },
    {
      "bce_eff_w": 0.4001,
      "bce_loss": -0.4557051956653595,
      "epoch": 8.665,
      "step": 1733
    },
    {
      "epoch": 8.67,
      "grad_norm": 31.20170021057129,
      "learning_rate": 8.34375e-06,
      "loss": 1.2207,
      "step": 1734
    },
    {
      "bce_eff_w": 0.40025,
      "bce_loss": -0.4601861238479614,
      "epoch": 8.67,
      "step": 1734
    },
    {
      "epoch": 8.675,
      "grad_norm": 35.026031494140625,
      "learning_rate": 8.3125e-06,
      "loss": 0.6248,
      "step": 1735
    },
    {
      "bce_eff_w": 0.4004,
      "bce_loss": -0.4613337814807892,
      "epoch": 8.675,
      "step": 1735
    },
    {
      "epoch": 8.68,
      "grad_norm": 29.289457321166992,
      "learning_rate": 8.28125e-06,
      "loss": 1.3573,
      "step": 1736
    },
    {
      "bce_eff_w": 0.40054999999999996,
      "bce_loss": -0.4636334776878357,
      "epoch": 8.68,
      "step": 1736
    },
    {
      "epoch": 8.685,
      "grad_norm": 26.651227951049805,
      "learning_rate": 8.25e-06,
      "loss": 0.5804,
      "step": 1737
    },
    {
      "bce_eff_w": 0.40070000000000006,
      "bce_loss": -0.4606567621231079,
      "epoch": 8.685,
      "step": 1737
    },
    {
      "epoch": 8.69,
      "grad_norm": 6.818277359008789,
      "learning_rate": 8.21875e-06,
      "loss": -0.1025,
      "step": 1738
    },
    {
      "bce_eff_w": 0.40085000000000004,
      "bce_loss": -0.4587174952030182,
      "epoch": 8.69,
      "step": 1738
    },
    {
      "epoch": 8.695,
      "grad_norm": 1.283720850944519,
      "learning_rate": 8.1875e-06,
      "loss": -0.1701,
      "step": 1739
    },
    {
      "bce_eff_w": 0.401,
      "bce_loss": -0.46123838424682617,
      "epoch": 8.695,
      "step": 1739
    },
    {
      "epoch": 8.7,
      "grad_norm": 15.444897651672363,
      "learning_rate": 8.15625e-06,
      "loss": 0.0658,
      "step": 1740
    },
    {
      "bce_eff_w": 0.40115,
      "bce_loss": -0.4627802073955536,
      "epoch": 8.7,
      "step": 1740
    },
    {
      "epoch": 8.705,
      "grad_norm": 15.834704399108887,
      "learning_rate": 8.125000000000001e-06,
      "loss": 0.7563,
      "step": 1741
    },
    {
      "bce_eff_w": 0.4013,
      "bce_loss": -0.46293383836746216,
      "epoch": 8.705,
      "step": 1741
    },
    {
      "epoch": 8.71,
      "grad_norm": 15.623018264770508,
      "learning_rate": 8.09375e-06,
      "loss": 1.0057,
      "step": 1742
    },
    {
      "bce_eff_w": 0.40145,
      "bce_loss": -0.46139898896217346,
      "epoch": 8.71,
      "step": 1742
    },
    {
      "epoch": 8.715,
      "grad_norm": 8.617947578430176,
      "learning_rate": 8.062500000000001e-06,
      "loss": -0.0397,
      "step": 1743
    },
    {
      "bce_eff_w": 0.4016,
      "bce_loss": -0.4604816436767578,
      "epoch": 8.715,
      "step": 1743
    },
    {
      "epoch": 8.72,
      "grad_norm": 14.175769805908203,
      "learning_rate": 8.03125e-06,
      "loss": 0.2595,
      "step": 1744
    },
    {
      "bce_eff_w": 0.40175,
      "bce_loss": -0.4602838158607483,
      "epoch": 8.72,
      "step": 1744
    },
    {
      "epoch": 8.725,
      "grad_norm": 19.475902557373047,
      "learning_rate": 8.000000000000001e-06,
      "loss": 0.1156,
      "step": 1745
    },
    {
      "bce_eff_w": 0.40190000000000003,
      "bce_loss": -0.46293407678604126,
      "epoch": 8.725,
      "step": 1745
    },
    {
      "epoch": 8.73,
      "grad_norm": 18.54654312133789,
      "learning_rate": 7.96875e-06,
      "loss": 0.4761,
      "step": 1746
    },
    {
      "bce_eff_w": 0.40205,
      "bce_loss": -0.45446205139160156,
      "epoch": 8.73,
      "step": 1746
    },
    {
      "epoch": 8.735,
      "grad_norm": 18.811412811279297,
      "learning_rate": 7.9375e-06,
      "loss": 0.5131,
      "step": 1747
    },
    {
      "bce_eff_w": 0.4022,
      "bce_loss": -0.45928046107292175,
      "epoch": 8.735,
      "step": 1747
    },
    {
      "epoch": 8.74,
      "grad_norm": 18.710433959960938,
      "learning_rate": 7.906249999999999e-06,
      "loss": 0.0615,
      "step": 1748
    },
    {
      "bce_eff_w": 0.40235,
      "bce_loss": -0.46257245540618896,
      "epoch": 8.74,
      "step": 1748
    },
    {
      "epoch": 8.745,
      "grad_norm": 26.101383209228516,
      "learning_rate": 7.875e-06,
      "loss": 0.3996,
      "step": 1749
    },
    {
      "bce_eff_w": 0.4025,
      "bce_loss": -0.4618575870990753,
      "epoch": 8.745,
      "step": 1749
    },
    {
      "epoch": 8.75,
      "grad_norm": 26.92528533935547,
      "learning_rate": 7.84375e-06,
      "loss": 1.769,
      "step": 1750
    },
    {
      "bce_eff_w": 0.40265,
      "bce_loss": -0.4622397720813751,
      "epoch": 8.75,
      "step": 1750
    },
    {
      "epoch": 8.755,
      "grad_norm": 10.996194839477539,
      "learning_rate": 7.8125e-06,
      "loss": -0.0765,
      "step": 1751
    },
    {
      "bce_eff_w": 0.40280000000000005,
      "bce_loss": -0.46147647500038147,
      "epoch": 8.755,
      "step": 1751
    },
    {
      "epoch": 8.76,
      "grad_norm": 17.399967193603516,
      "learning_rate": 7.781250000000001e-06,
      "loss": 0.0428,
      "step": 1752
    },
    {
      "bce_eff_w": 0.40295000000000003,
      "bce_loss": -0.45994001626968384,
      "epoch": 8.76,
      "step": 1752
    },
    {
      "epoch": 8.765,
      "grad_norm": 19.356935501098633,
      "learning_rate": 7.75e-06,
      "loss": 0.7143,
      "step": 1753
    },
    {
      "bce_eff_w": 0.4031,
      "bce_loss": -0.4616864323616028,
      "epoch": 8.765,
      "step": 1753
    },
    {
      "epoch": 8.77,
      "grad_norm": 10.860381126403809,
      "learning_rate": 7.718750000000001e-06,
      "loss": 0.0067,
      "step": 1754
    },
    {
      "bce_eff_w": 0.40325,
      "bce_loss": -0.46061426401138306,
      "epoch": 8.77,
      "step": 1754
    },
    {
      "epoch": 8.775,
      "grad_norm": 20.300559997558594,
      "learning_rate": 7.6875e-06,
      "loss": 0.0256,
      "step": 1755
    },
    {
      "bce_eff_w": 0.4034,
      "bce_loss": -0.46027621626853943,
      "epoch": 8.775,
      "step": 1755
    },
    {
      "epoch": 8.78,
      "grad_norm": 11.55007553100586,
      "learning_rate": 7.656250000000001e-06,
      "loss": 0.2141,
      "step": 1756
    },
    {
      "bce_eff_w": 0.40354999999999996,
      "bce_loss": -0.46253854036331177,
      "epoch": 8.78,
      "step": 1756
    },
    {
      "epoch": 8.785,
      "grad_norm": 15.180508613586426,
      "learning_rate": 7.625e-06,
      "loss": 0.0996,
      "step": 1757
    },
    {
      "bce_eff_w": 0.40370000000000006,
      "bce_loss": -0.4579257369041443,
      "epoch": 8.785,
      "step": 1757
    },
    {
      "epoch": 8.79,
      "grad_norm": 15.23679256439209,
      "learning_rate": 7.593750000000001e-06,
      "loss": 0.5461,
      "step": 1758
    },
    {
      "bce_eff_w": 0.40385000000000004,
      "bce_loss": -0.4609129726886749,
      "epoch": 8.79,
      "step": 1758
    },
    {
      "epoch": 8.795,
      "grad_norm": 17.148235321044922,
      "learning_rate": 7.5625e-06,
      "loss": 2.2384,
      "step": 1759
    },
    {
      "bce_eff_w": 0.404,
      "bce_loss": -0.4538893401622772,
      "epoch": 8.795,
      "step": 1759
    },
    {
      "epoch": 8.8,
      "grad_norm": 9.246530532836914,
      "learning_rate": 7.531250000000001e-06,
      "loss": 0.0371,
      "step": 1760
    },
    {
      "bce_eff_w": 0.40415,
      "bce_loss": -0.46016332507133484,
      "epoch": 8.8,
      "step": 1760
    },
    {
      "epoch": 8.805,
      "grad_norm": 17.5775146484375,
      "learning_rate": 7.5e-06,
      "loss": 0.6871,
      "step": 1761
    },
    {
      "bce_eff_w": 0.4043,
      "bce_loss": -0.461443692445755,
      "epoch": 8.805,
      "step": 1761
    },
    {
      "epoch": 8.81,
      "grad_norm": 21.419376373291016,
      "learning_rate": 7.468750000000001e-06,
      "loss": 0.5716,
      "step": 1762
    },
    {
      "bce_eff_w": 0.40445,
      "bce_loss": -0.45946118235588074,
      "epoch": 8.81,
      "step": 1762
    },
    {
      "epoch": 8.815,
      "grad_norm": 10.302382469177246,
      "learning_rate": 7.4375e-06,
      "loss": -0.0162,
      "step": 1763
    },
    {
      "bce_eff_w": 0.4046,
      "bce_loss": -0.4523314833641052,
      "epoch": 8.815,
      "step": 1763
    },
    {
      "epoch": 8.82,
      "grad_norm": 7.234523296356201,
      "learning_rate": 7.4062500000000005e-06,
      "loss": -0.1251,
      "step": 1764
    },
    {
      "bce_eff_w": 0.40475,
      "bce_loss": -0.46095070242881775,
      "epoch": 8.82,
      "step": 1764
    },
    {
      "epoch": 8.825,
      "grad_norm": 43.03627014160156,
      "learning_rate": 7.375e-06,
      "loss": 0.9903,
      "step": 1765
    },
    {
      "bce_eff_w": 0.40490000000000004,
      "bce_loss": -0.4626821279525757,
      "epoch": 8.825,
      "step": 1765
    },
    {
      "epoch": 8.83,
      "grad_norm": 21.16185188293457,
      "learning_rate": 7.343750000000001e-06,
      "loss": 1.1312,
      "step": 1766
    },
    {
      "bce_eff_w": 0.40505,
      "bce_loss": -0.4581558406352997,
      "epoch": 8.83,
      "step": 1766
    },
    {
      "epoch": 8.835,
      "grad_norm": 17.736509323120117,
      "learning_rate": 7.3125e-06,
      "loss": 1.4053,
      "step": 1767
    },
    {
      "bce_eff_w": 0.4052,
      "bce_loss": -0.45980575680732727,
      "epoch": 8.835,
      "step": 1767
    },
    {
      "epoch": 8.84,
      "grad_norm": 11.543705940246582,
      "learning_rate": 7.281250000000001e-06,
      "loss": 0.0228,
      "step": 1768
    },
    {
      "bce_eff_w": 0.40535,
      "bce_loss": -0.45709675550460815,
      "epoch": 8.84,
      "step": 1768
    },
    {
      "epoch": 8.845,
      "grad_norm": 28.99988555908203,
      "learning_rate": 7.25e-06,
      "loss": 1.6531,
      "step": 1769
    },
    {
      "bce_eff_w": 0.4055,
      "bce_loss": -0.45685720443725586,
      "epoch": 8.845,
      "step": 1769
    },
    {
      "epoch": 8.85,
      "grad_norm": 17.23370933532715,
      "learning_rate": 7.21875e-06,
      "loss": 0.5195,
      "step": 1770
    },
    {
      "bce_eff_w": 0.40565,
      "bce_loss": -0.4607970416545868,
      "epoch": 8.85,
      "step": 1770
    },
    {
      "epoch": 8.855,
      "grad_norm": 28.724279403686523,
      "learning_rate": 7.187499999999999e-06,
      "loss": 0.2128,
      "step": 1771
    },
    {
      "bce_eff_w": 0.40580000000000005,
      "bce_loss": -0.4597037136554718,
      "epoch": 8.855,
      "step": 1771
    },
    {
      "epoch": 8.86,
      "grad_norm": 26.962997436523438,
      "learning_rate": 7.15625e-06,
      "loss": 0.3216,
      "step": 1772
    },
    {
      "bce_eff_w": 0.40595000000000003,
      "bce_loss": -0.4641384184360504,
      "epoch": 8.86,
      "step": 1772
    },
    {
      "epoch": 8.865,
      "grad_norm": 11.462713241577148,
      "learning_rate": 7.1249999999999995e-06,
      "loss": 1.5986,
      "step": 1773
    },
    {
      "bce_eff_w": 0.4061,
      "bce_loss": -0.4617772400379181,
      "epoch": 8.865,
      "step": 1773
    },
    {
      "epoch": 8.87,
      "grad_norm": 12.76183032989502,
      "learning_rate": 7.0937500000000005e-06,
      "loss": -0.0162,
      "step": 1774
    },
    {
      "bce_eff_w": 0.40625,
      "bce_loss": -0.46449217200279236,
      "epoch": 8.87,
      "step": 1774
    },
    {
      "epoch": 8.875,
      "grad_norm": 28.734630584716797,
      "learning_rate": 7.0625e-06,
      "loss": 0.5871,
      "step": 1775
    },
    {
      "bce_eff_w": 0.4064,
      "bce_loss": -0.4589688777923584,
      "epoch": 8.875,
      "step": 1775
    },
    {
      "epoch": 8.88,
      "grad_norm": 6.136074066162109,
      "learning_rate": 7.031250000000001e-06,
      "loss": -0.0852,
      "step": 1776
    },
    {
      "bce_eff_w": 0.40654999999999997,
      "bce_loss": -0.4588457942008972,
      "epoch": 8.88,
      "step": 1776
    },
    {
      "epoch": 8.885,
      "grad_norm": 15.898494720458984,
      "learning_rate": 7.000000000000001e-06,
      "loss": 0.9761,
      "step": 1777
    },
    {
      "bce_eff_w": 0.40669999999999995,
      "bce_loss": -0.4616858959197998,
      "epoch": 8.885,
      "step": 1777
    },
    {
      "epoch": 8.89,
      "grad_norm": 15.010055541992188,
      "learning_rate": 6.96875e-06,
      "loss": 0.1902,
      "step": 1778
    },
    {
      "bce_eff_w": 0.40685000000000004,
      "bce_loss": -0.46347498893737793,
      "epoch": 8.89,
      "step": 1778
    },
    {
      "epoch": 8.895,
      "grad_norm": 36.449989318847656,
      "learning_rate": 6.937500000000001e-06,
      "loss": 0.3686,
      "step": 1779
    },
    {
      "bce_eff_w": 0.40700000000000003,
      "bce_loss": -0.4624474048614502,
      "epoch": 8.895,
      "step": 1779
    },
    {
      "epoch": 8.9,
      "grad_norm": 17.933481216430664,
      "learning_rate": 6.90625e-06,
      "loss": 1.4358,
      "step": 1780
    },
    {
      "bce_eff_w": 0.40715,
      "bce_loss": -0.4585159420967102,
      "epoch": 8.9,
      "step": 1780
    },
    {
      "epoch": 8.905,
      "grad_norm": 14.07497787475586,
      "learning_rate": 6.875000000000001e-06,
      "loss": 0.0999,
      "step": 1781
    },
    {
      "bce_eff_w": 0.4073,
      "bce_loss": -0.4641555845737457,
      "epoch": 8.905,
      "step": 1781
    },
    {
      "epoch": 8.91,
      "grad_norm": 12.406087875366211,
      "learning_rate": 6.84375e-06,
      "loss": 0.0609,
      "step": 1782
    },
    {
      "bce_eff_w": 0.40745,
      "bce_loss": -0.46157926321029663,
      "epoch": 8.91,
      "step": 1782
    },
    {
      "epoch": 8.915,
      "grad_norm": 20.89403533935547,
      "learning_rate": 6.8125e-06,
      "loss": 0.6509,
      "step": 1783
    },
    {
      "bce_eff_w": 0.40759999999999996,
      "bce_loss": -0.4633752703666687,
      "epoch": 8.915,
      "step": 1783
    },
    {
      "epoch": 8.92,
      "grad_norm": 15.365113258361816,
      "learning_rate": 6.7812500000000005e-06,
      "loss": 0.2266,
      "step": 1784
    },
    {
      "bce_eff_w": 0.40775,
      "bce_loss": -0.4472739100456238,
      "epoch": 8.92,
      "step": 1784
    },
    {
      "epoch": 8.925,
      "grad_norm": 13.387506484985352,
      "learning_rate": 6.750000000000001e-06,
      "loss": -0.026,
      "step": 1785
    },
    {
      "bce_eff_w": 0.4079,
      "bce_loss": -0.46433740854263306,
      "epoch": 8.925,
      "step": 1785
    },
    {
      "epoch": 8.93,
      "grad_norm": 24.726938247680664,
      "learning_rate": 6.71875e-06,
      "loss": 1.4762,
      "step": 1786
    },
    {
      "bce_eff_w": 0.40805,
      "bce_loss": -0.45927390456199646,
      "epoch": 8.93,
      "step": 1786
    },
    {
      "epoch": 8.935,
      "grad_norm": 15.34922981262207,
      "learning_rate": 6.687500000000001e-06,
      "loss": 0.5945,
      "step": 1787
    },
    {
      "bce_eff_w": 0.4082,
      "bce_loss": -0.461212158203125,
      "epoch": 8.935,
      "step": 1787
    },
    {
      "epoch": 8.94,
      "grad_norm": 14.448522567749023,
      "learning_rate": 6.65625e-06,
      "loss": 0.9042,
      "step": 1788
    },
    {
      "bce_eff_w": 0.40835,
      "bce_loss": -0.46313998103141785,
      "epoch": 8.94,
      "step": 1788
    },
    {
      "epoch": 8.945,
      "grad_norm": 19.542760848999023,
      "learning_rate": 6.625000000000001e-06,
      "loss": 0.958,
      "step": 1789
    },
    {
      "bce_eff_w": 0.4085,
      "bce_loss": -0.46346551179885864,
      "epoch": 8.945,
      "step": 1789
    },
    {
      "epoch": 8.95,
      "grad_norm": 20.869272232055664,
      "learning_rate": 6.59375e-06,
      "loss": 1.2387,
      "step": 1790
    },
    {
      "bce_eff_w": 0.40865,
      "bce_loss": -0.45894044637680054,
      "epoch": 8.95,
      "step": 1790
    },
    {
      "epoch": 8.955,
      "grad_norm": 14.00127124786377,
      "learning_rate": 6.5625e-06,
      "loss": 1.7245,
      "step": 1791
    },
    {
      "bce_eff_w": 0.4088,
      "bce_loss": -0.4554215371608734,
      "epoch": 8.955,
      "step": 1791
    },
    {
      "epoch": 8.96,
      "grad_norm": 9.62537670135498,
      "learning_rate": 6.5312499999999995e-06,
      "loss": -0.0817,
      "step": 1792
    },
    {
      "bce_eff_w": 0.40895000000000004,
      "bce_loss": -0.46202537417411804,
      "epoch": 8.96,
      "step": 1792
    },
    {
      "epoch": 8.965,
      "grad_norm": 14.65074348449707,
      "learning_rate": 6.5000000000000004e-06,
      "loss": 0.7158,
      "step": 1793
    },
    {
      "bce_eff_w": 0.4091,
      "bce_loss": -0.4596259891986847,
      "epoch": 8.965,
      "step": 1793
    },
    {
      "epoch": 8.97,
      "grad_norm": 19.354379653930664,
      "learning_rate": 6.46875e-06,
      "loss": 1.1853,
      "step": 1794
    },
    {
      "bce_eff_w": 0.40925,
      "bce_loss": -0.4603036940097809,
      "epoch": 8.97,
      "step": 1794
    },
    {
      "epoch": 8.975,
      "grad_norm": 28.69980812072754,
      "learning_rate": 6.437500000000001e-06,
      "loss": 0.2211,
      "step": 1795
    },
    {
      "bce_eff_w": 0.4094,
      "bce_loss": -0.4601537585258484,
      "epoch": 8.975,
      "step": 1795
    },
    {
      "epoch": 8.98,
      "grad_norm": 12.781373977661133,
      "learning_rate": 6.40625e-06,
      "loss": 0.6109,
      "step": 1796
    },
    {
      "bce_eff_w": 0.40954999999999997,
      "bce_loss": -0.45573779940605164,
      "epoch": 8.98,
      "step": 1796
    },
    {
      "epoch": 8.985,
      "grad_norm": 15.010100364685059,
      "learning_rate": 6.375000000000001e-06,
      "loss": 0.2708,
      "step": 1797
    },
    {
      "bce_eff_w": 0.40969999999999995,
      "bce_loss": -0.44787707924842834,
      "epoch": 8.985,
      "step": 1797
    },
    {
      "epoch": 8.99,
      "grad_norm": 13.07719612121582,
      "learning_rate": 6.34375e-06,
      "loss": 0.1588,
      "step": 1798
    },
    {
      "bce_eff_w": 0.40985000000000005,
      "bce_loss": -0.46211984753608704,
      "epoch": 8.99,
      "step": 1798
    },
    {
      "epoch": 8.995,
      "grad_norm": 19.907398223876953,
      "learning_rate": 6.3125e-06,
      "loss": 0.2245,
      "step": 1799
    },
    {
      "bce_eff_w": 0.41000000000000003,
      "bce_loss": -0.46246349811553955,
      "epoch": 8.995,
      "step": 1799
    },
    {
      "epoch": 9.0,
      "grad_norm": 25.161052703857422,
      "learning_rate": 6.281249999999999e-06,
      "loss": 0.8249,
      "step": 1800
    },
    {
      "bce_eff_w": 0.41015,
      "bce_loss": -0.4607653021812439,
      "epoch": 9.0,
      "step": 1800
    },
    {
      "epoch": 9.005,
      "grad_norm": 16.279308319091797,
      "learning_rate": 6.25e-06,
      "loss": 0.8478,
      "step": 1801
    },
    {
      "bce_eff_w": 0.4103,
      "bce_loss": -0.4603279232978821,
      "epoch": 9.005,
      "step": 1801
    },
    {
      "epoch": 9.01,
      "grad_norm": 16.5780029296875,
      "learning_rate": 6.21875e-06,
      "loss": 0.1475,
      "step": 1802
    },
    {
      "bce_eff_w": 0.41045,
      "bce_loss": -0.45974016189575195,
      "epoch": 9.01,
      "step": 1802
    },
    {
      "epoch": 9.015,
      "grad_norm": 6.693785190582275,
      "learning_rate": 6.1875000000000005e-06,
      "loss": -0.1121,
      "step": 1803
    },
    {
      "bce_eff_w": 0.41059999999999997,
      "bce_loss": -0.4609884023666382,
      "epoch": 9.015,
      "step": 1803
    },
    {
      "epoch": 9.02,
      "grad_norm": 22.291534423828125,
      "learning_rate": 6.1562500000000006e-06,
      "loss": 0.4316,
      "step": 1804
    },
    {
      "bce_eff_w": 0.41075,
      "bce_loss": -0.463191956281662,
      "epoch": 9.02,
      "step": 1804
    },
    {
      "epoch": 9.025,
      "grad_norm": 13.562699317932129,
      "learning_rate": 6.125e-06,
      "loss": 1.3284,
      "step": 1805
    },
    {
      "bce_eff_w": 0.4109,
      "bce_loss": -0.4554780423641205,
      "epoch": 9.025,
      "step": 1805
    },
    {
      "epoch": 9.03,
      "grad_norm": 13.433125495910645,
      "learning_rate": 6.09375e-06,
      "loss": 0.6906,
      "step": 1806
    },
    {
      "bce_eff_w": 0.41105,
      "bce_loss": -0.4606659412384033,
      "epoch": 9.03,
      "step": 1806
    },
    {
      "epoch": 9.035,
      "grad_norm": 3.193596363067627,
      "learning_rate": 6.0625e-06,
      "loss": -0.1589,
      "step": 1807
    },
    {
      "bce_eff_w": 0.4112,
      "bce_loss": -0.4640791714191437,
      "epoch": 9.035,
      "step": 1807
    },
    {
      "epoch": 9.04,
      "grad_norm": 17.06865692138672,
      "learning_rate": 6.03125e-06,
      "loss": 0.0035,
      "step": 1808
    },
    {
      "bce_eff_w": 0.41135,
      "bce_loss": -0.4623461663722992,
      "epoch": 9.04,
      "step": 1808
    },
    {
      "epoch": 9.045,
      "grad_norm": 15.74300765991211,
      "learning_rate": 6e-06,
      "loss": 0.5077,
      "step": 1809
    },
    {
      "bce_eff_w": 0.4115,
      "bce_loss": -0.45730870962142944,
      "epoch": 9.045,
      "step": 1809
    },
    {
      "epoch": 9.05,
      "grad_norm": 11.602290153503418,
      "learning_rate": 5.96875e-06,
      "loss": -0.0012,
      "step": 1810
    },
    {
      "bce_eff_w": 0.41165,
      "bce_loss": -0.46154269576072693,
      "epoch": 9.05,
      "step": 1810
    },
    {
      "epoch": 9.055,
      "grad_norm": 20.485002517700195,
      "learning_rate": 5.9375e-06,
      "loss": 1.0527,
      "step": 1811
    },
    {
      "bce_eff_w": 0.4118,
      "bce_loss": -0.4632335603237152,
      "epoch": 9.055,
      "step": 1811
    },
    {
      "epoch": 9.06,
      "grad_norm": 14.792534828186035,
      "learning_rate": 5.9062499999999996e-06,
      "loss": 1.0168,
      "step": 1812
    },
    {
      "bce_eff_w": 0.41195000000000004,
      "bce_loss": -0.45724979043006897,
      "epoch": 9.06,
      "step": 1812
    },
    {
      "epoch": 9.065,
      "grad_norm": 8.01972770690918,
      "learning_rate": 5.875e-06,
      "loss": -0.058,
      "step": 1813
    },
    {
      "bce_eff_w": 0.4121,
      "bce_loss": -0.46221327781677246,
      "epoch": 9.065,
      "step": 1813
    },
    {
      "epoch": 9.07,
      "grad_norm": 21.23798179626465,
      "learning_rate": 5.843750000000001e-06,
      "loss": 0.4703,
      "step": 1814
    },
    {
      "bce_eff_w": 0.41225,
      "bce_loss": -0.45526689291000366,
      "epoch": 9.07,
      "step": 1814
    },
    {
      "epoch": 9.075,
      "grad_norm": 8.385685920715332,
      "learning_rate": 5.812500000000001e-06,
      "loss": -0.0557,
      "step": 1815
    },
    {
      "bce_eff_w": 0.4124,
      "bce_loss": -0.4560355246067047,
      "epoch": 9.075,
      "step": 1815
    },
    {
      "epoch": 9.08,
      "grad_norm": 12.504602432250977,
      "learning_rate": 5.781250000000001e-06,
      "loss": 0.1285,
      "step": 1816
    },
    {
      "bce_eff_w": 0.41255,
      "bce_loss": -0.4596477448940277,
      "epoch": 9.08,
      "step": 1816
    },
    {
      "epoch": 9.085,
      "grad_norm": 21.355606079101562,
      "learning_rate": 5.750000000000001e-06,
      "loss": 0.2464,
      "step": 1817
    },
    {
      "bce_eff_w": 0.41269999999999996,
      "bce_loss": -0.46161502599716187,
      "epoch": 9.085,
      "step": 1817
    },
    {
      "epoch": 9.09,
      "grad_norm": 15.15028190612793,
      "learning_rate": 5.71875e-06,
      "loss": 0.6779,
      "step": 1818
    },
    {
      "bce_eff_w": 0.41285000000000005,
      "bce_loss": -0.4562259018421173,
      "epoch": 9.09,
      "step": 1818
    },
    {
      "epoch": 9.095,
      "grad_norm": 14.933611869812012,
      "learning_rate": 5.6875e-06,
      "loss": 0.0337,
      "step": 1819
    },
    {
      "bce_eff_w": 0.41300000000000003,
      "bce_loss": -0.45762625336647034,
      "epoch": 9.095,
      "step": 1819
    },
    {
      "epoch": 9.1,
      "grad_norm": 16.009952545166016,
      "learning_rate": 5.65625e-06,
      "loss": 0.4376,
      "step": 1820
    },
    {
      "bce_eff_w": 0.41315,
      "bce_loss": -0.4636031687259674,
      "epoch": 9.1,
      "step": 1820
    },
    {
      "epoch": 9.105,
      "grad_norm": 13.854453086853027,
      "learning_rate": 5.625e-06,
      "loss": 0.0422,
      "step": 1821
    },
    {
      "bce_eff_w": 0.4133,
      "bce_loss": -0.452406644821167,
      "epoch": 9.105,
      "step": 1821
    },
    {
      "epoch": 9.11,
      "grad_norm": 12.832812309265137,
      "learning_rate": 5.5937500000000004e-06,
      "loss": 0.2351,
      "step": 1822
    },
    {
      "bce_eff_w": 0.41345,
      "bce_loss": -0.46187639236450195,
      "epoch": 9.11,
      "step": 1822
    },
    {
      "epoch": 9.115,
      "grad_norm": 20.239295959472656,
      "learning_rate": 5.5625000000000005e-06,
      "loss": 0.2235,
      "step": 1823
    },
    {
      "bce_eff_w": 0.41359999999999997,
      "bce_loss": -0.45515531301498413,
      "epoch": 9.115,
      "step": 1823
    },
    {
      "epoch": 9.12,
      "grad_norm": 19.413040161132812,
      "learning_rate": 5.531250000000001e-06,
      "loss": 0.2079,
      "step": 1824
    },
    {
      "bce_eff_w": 0.41375,
      "bce_loss": -0.45981156826019287,
      "epoch": 9.12,
      "step": 1824
    },
    {
      "epoch": 9.125,
      "grad_norm": 16.229751586914062,
      "learning_rate": 5.500000000000001e-06,
      "loss": 0.2072,
      "step": 1825
    },
    {
      "bce_eff_w": 0.4139,
      "bce_loss": -0.4548616111278534,
      "epoch": 9.125,
      "step": 1825
    },
    {
      "epoch": 9.13,
      "grad_norm": 18.78643226623535,
      "learning_rate": 5.46875e-06,
      "loss": 1.3485,
      "step": 1826
    },
    {
      "bce_eff_w": 0.41405000000000003,
      "bce_loss": -0.4556325376033783,
      "epoch": 9.13,
      "step": 1826
    },
    {
      "epoch": 9.135,
      "grad_norm": 21.72327423095703,
      "learning_rate": 5.4375e-06,
      "loss": 1.2482,
      "step": 1827
    },
    {
      "bce_eff_w": 0.4142,
      "bce_loss": -0.45748764276504517,
      "epoch": 9.135,
      "step": 1827
    },
    {
      "epoch": 9.14,
      "grad_norm": 21.64126968383789,
      "learning_rate": 5.40625e-06,
      "loss": 0.6561,
      "step": 1828
    },
    {
      "bce_eff_w": 0.41435,
      "bce_loss": -0.4606695771217346,
      "epoch": 9.14,
      "step": 1828
    },
    {
      "epoch": 9.145,
      "grad_norm": 15.476204872131348,
      "learning_rate": 5.375e-06,
      "loss": 2.3071,
      "step": 1829
    },
    {
      "bce_eff_w": 0.4145,
      "bce_loss": -0.4617364704608917,
      "epoch": 9.145,
      "step": 1829
    },
    {
      "epoch": 9.15,
      "grad_norm": 22.18501853942871,
      "learning_rate": 5.34375e-06,
      "loss": 1.308,
      "step": 1830
    },
    {
      "bce_eff_w": 0.41465,
      "bce_loss": -0.46143588423728943,
      "epoch": 9.15,
      "step": 1830
    },
    {
      "epoch": 9.155,
      "grad_norm": 15.10649585723877,
      "learning_rate": 5.3125e-06,
      "loss": -0.0088,
      "step": 1831
    },
    {
      "bce_eff_w": 0.4148,
      "bce_loss": -0.46259814500808716,
      "epoch": 9.155,
      "step": 1831
    },
    {
      "epoch": 9.16,
      "grad_norm": 0.9848763346672058,
      "learning_rate": 5.2812500000000005e-06,
      "loss": -0.1819,
      "step": 1832
    },
    {
      "bce_eff_w": 0.41495000000000004,
      "bce_loss": -0.461788147687912,
      "epoch": 9.16,
      "step": 1832
    },
    {
      "epoch": 9.165,
      "grad_norm": 20.00514793395996,
      "learning_rate": 5.25e-06,
      "loss": 0.1728,
      "step": 1833
    },
    {
      "bce_eff_w": 0.4151,
      "bce_loss": -0.460525244474411,
      "epoch": 9.165,
      "step": 1833
    },
    {
      "epoch": 9.17,
      "grad_norm": 13.163766860961914,
      "learning_rate": 5.21875e-06,
      "loss": 0.2264,
      "step": 1834
    },
    {
      "bce_eff_w": 0.41525,
      "bce_loss": -0.46231967210769653,
      "epoch": 9.17,
      "step": 1834
    },
    {
      "epoch": 9.175,
      "grad_norm": 4.765690803527832,
      "learning_rate": 5.1875e-06,
      "loss": -0.127,
      "step": 1835
    },
    {
      "bce_eff_w": 0.4154,
      "bce_loss": -0.4573764503002167,
      "epoch": 9.175,
      "step": 1835
    },
    {
      "epoch": 9.18,
      "grad_norm": 23.542245864868164,
      "learning_rate": 5.15625e-06,
      "loss": 1.6381,
      "step": 1836
    },
    {
      "bce_eff_w": 0.41555,
      "bce_loss": -0.459246426820755,
      "epoch": 9.18,
      "step": 1836
    },
    {
      "epoch": 9.185,
      "grad_norm": 13.670642852783203,
      "learning_rate": 5.125e-06,
      "loss": 0.0307,
      "step": 1837
    },
    {
      "bce_eff_w": 0.41569999999999996,
      "bce_loss": -0.4608772099018097,
      "epoch": 9.185,
      "step": 1837
    },
    {
      "epoch": 9.19,
      "grad_norm": 20.928316116333008,
      "learning_rate": 5.09375e-06,
      "loss": 0.3378,
      "step": 1838
    },
    {
      "bce_eff_w": 0.41585000000000005,
      "bce_loss": -0.46217823028564453,
      "epoch": 9.19,
      "step": 1838
    },
    {
      "epoch": 9.195,
      "grad_norm": 21.59372901916504,
      "learning_rate": 5.0625e-06,
      "loss": 0.6248,
      "step": 1839
    },
    {
      "bce_eff_w": 0.41600000000000004,
      "bce_loss": -0.46353283524513245,
      "epoch": 9.195,
      "step": 1839
    },
    {
      "epoch": 9.2,
      "grad_norm": 35.699764251708984,
      "learning_rate": 5.03125e-06,
      "loss": 1.3469,
      "step": 1840
    },
    {
      "bce_eff_w": 0.41615,
      "bce_loss": -0.45893341302871704,
      "epoch": 9.2,
      "step": 1840
    },
    {
      "epoch": 9.205,
      "grad_norm": 17.32193946838379,
      "learning_rate": 5e-06,
      "loss": 0.3727,
      "step": 1841
    },
    {
      "bce_eff_w": 0.4163,
      "bce_loss": -0.4637782871723175,
      "epoch": 9.205,
      "step": 1841
    },
    {
      "epoch": 9.21,
      "grad_norm": 11.216791152954102,
      "learning_rate": 4.9687500000000005e-06,
      "loss": 0.6563,
      "step": 1842
    },
    {
      "bce_eff_w": 0.41645,
      "bce_loss": -0.4575903117656708,
      "epoch": 9.21,
      "step": 1842
    },
    {
      "epoch": 9.215,
      "grad_norm": 4.256272792816162,
      "learning_rate": 4.937500000000001e-06,
      "loss": -0.1417,
      "step": 1843
    },
    {
      "bce_eff_w": 0.41659999999999997,
      "bce_loss": -0.46433132886886597,
      "epoch": 9.215,
      "step": 1843
    },
    {
      "epoch": 9.22,
      "grad_norm": 11.149785995483398,
      "learning_rate": 4.906250000000001e-06,
      "loss": 0.0112,
      "step": 1844
    },
    {
      "bce_eff_w": 0.41675,
      "bce_loss": -0.46265074610710144,
      "epoch": 9.22,
      "step": 1844
    },
    {
      "epoch": 9.225,
      "grad_norm": 30.293071746826172,
      "learning_rate": 4.875000000000001e-06,
      "loss": 1.1406,
      "step": 1845
    },
    {
      "bce_eff_w": 0.4169,
      "bce_loss": -0.45028457045555115,
      "epoch": 9.225,
      "step": 1845
    },
    {
      "epoch": 9.23,
      "grad_norm": 14.46239185333252,
      "learning_rate": 4.84375e-06,
      "loss": 0.4099,
      "step": 1846
    },
    {
      "bce_eff_w": 0.41705000000000003,
      "bce_loss": -0.4627818167209625,
      "epoch": 9.23,
      "step": 1846
    },
    {
      "epoch": 9.235,
      "grad_norm": 6.302977085113525,
      "learning_rate": 4.8125e-06,
      "loss": -0.0976,
      "step": 1847
    },
    {
      "bce_eff_w": 0.4172,
      "bce_loss": -0.4554269015789032,
      "epoch": 9.235,
      "step": 1847
    },
    {
      "epoch": 9.24,
      "grad_norm": 14.226237297058105,
      "learning_rate": 4.78125e-06,
      "loss": 0.1854,
      "step": 1848
    },
    {
      "bce_eff_w": 0.41735,
      "bce_loss": -0.4539618194103241,
      "epoch": 9.24,
      "step": 1848
    },
    {
      "epoch": 9.245,
      "grad_norm": 16.237756729125977,
      "learning_rate": 4.75e-06,
      "loss": 0.7671,
      "step": 1849
    },
    {
      "bce_eff_w": 0.4175,
      "bce_loss": -0.46327945590019226,
      "epoch": 9.245,
      "step": 1849
    },
    {
      "epoch": 9.25,
      "grad_norm": 14.622231483459473,
      "learning_rate": 4.71875e-06,
      "loss": 0.1603,
      "step": 1850
    },
    {
      "bce_eff_w": 0.41765,
      "bce_loss": -0.4614471197128296,
      "epoch": 9.25,
      "step": 1850
    },
    {
      "epoch": 9.255,
      "grad_norm": 13.309947967529297,
      "learning_rate": 4.6875000000000004e-06,
      "loss": 0.517,
      "step": 1851
    },
    {
      "bce_eff_w": 0.4178,
      "bce_loss": -0.46120014786720276,
      "epoch": 9.255,
      "step": 1851
    },
    {
      "epoch": 9.26,
      "grad_norm": 5.169863700866699,
      "learning_rate": 4.6562500000000005e-06,
      "loss": -0.0866,
      "step": 1852
    },
    {
      "bce_eff_w": 0.41795000000000004,
      "bce_loss": -0.4608377516269684,
      "epoch": 9.26,
      "step": 1852
    },
    {
      "epoch": 9.265,
      "grad_norm": 17.916419982910156,
      "learning_rate": 4.625e-06,
      "loss": 1.0482,
      "step": 1853
    },
    {
      "bce_eff_w": 0.4181,
      "bce_loss": -0.4604823887348175,
      "epoch": 9.265,
      "step": 1853
    },
    {
      "epoch": 9.27,
      "grad_norm": 22.186742782592773,
      "learning_rate": 4.59375e-06,
      "loss": 0.7716,
      "step": 1854
    },
    {
      "bce_eff_w": 0.41825,
      "bce_loss": -0.4579344689846039,
      "epoch": 9.27,
      "step": 1854
    },
    {
      "epoch": 9.275,
      "grad_norm": 23.592117309570312,
      "learning_rate": 4.5625e-06,
      "loss": 2.0161,
      "step": 1855
    },
    {
      "bce_eff_w": 0.4184,
      "bce_loss": -0.4623209238052368,
      "epoch": 9.275,
      "step": 1855
    },
    {
      "epoch": 9.28,
      "grad_norm": 18.729537963867188,
      "learning_rate": 4.53125e-06,
      "loss": 0.1071,
      "step": 1856
    },
    {
      "bce_eff_w": 0.41855,
      "bce_loss": -0.4587306082248688,
      "epoch": 9.28,
      "step": 1856
    },
    {
      "epoch": 9.285,
      "grad_norm": 9.502950668334961,
      "learning_rate": 4.5e-06,
      "loss": -0.0552,
      "step": 1857
    },
    {
      "bce_eff_w": 0.41869999999999996,
      "bce_loss": -0.4603258967399597,
      "epoch": 9.285,
      "step": 1857
    },
    {
      "epoch": 9.29,
      "grad_norm": 11.398785591125488,
      "learning_rate": 4.46875e-06,
      "loss": 0.1089,
      "step": 1858
    },
    {
      "bce_eff_w": 0.41885000000000006,
      "bce_loss": -0.4616234302520752,
      "epoch": 9.29,
      "step": 1858
    },
    {
      "epoch": 9.295,
      "grad_norm": 17.094831466674805,
      "learning_rate": 4.4375e-06,
      "loss": 0.7082,
      "step": 1859
    },
    {
      "bce_eff_w": 0.41900000000000004,
      "bce_loss": -0.4610828757286072,
      "epoch": 9.295,
      "step": 1859
    },
    {
      "epoch": 9.3,
      "grad_norm": 22.358945846557617,
      "learning_rate": 4.40625e-06,
      "loss": 0.7336,
      "step": 1860
    },
    {
      "bce_eff_w": 0.41915,
      "bce_loss": -0.46287667751312256,
      "epoch": 9.3,
      "step": 1860
    },
    {
      "epoch": 9.305,
      "grad_norm": 17.53683090209961,
      "learning_rate": 4.375e-06,
      "loss": 0.0437,
      "step": 1861
    },
    {
      "bce_eff_w": 0.4193,
      "bce_loss": -0.4597145915031433,
      "epoch": 9.305,
      "step": 1861
    },
    {
      "epoch": 9.31,
      "grad_norm": 7.968703269958496,
      "learning_rate": 4.34375e-06,
      "loss": -0.0824,
      "step": 1862
    },
    {
      "bce_eff_w": 0.41945,
      "bce_loss": -0.46182385087013245,
      "epoch": 9.31,
      "step": 1862
    },
    {
      "epoch": 9.315,
      "grad_norm": 7.046013355255127,
      "learning_rate": 4.3125e-06,
      "loss": -0.1009,
      "step": 1863
    },
    {
      "bce_eff_w": 0.4196,
      "bce_loss": -0.4632408320903778,
      "epoch": 9.315,
      "step": 1863
    },
    {
      "epoch": 9.32,
      "grad_norm": 28.157896041870117,
      "learning_rate": 4.281250000000001e-06,
      "loss": 0.8682,
      "step": 1864
    },
    {
      "bce_eff_w": 0.41975,
      "bce_loss": -0.45854321122169495,
      "epoch": 9.32,
      "step": 1864
    },
    {
      "epoch": 9.325,
      "grad_norm": 17.68918800354004,
      "learning_rate": 4.250000000000001e-06,
      "loss": 0.1348,
      "step": 1865
    },
    {
      "bce_eff_w": 0.4199,
      "bce_loss": -0.4571543037891388,
      "epoch": 9.325,
      "step": 1865
    },
    {
      "epoch": 9.33,
      "grad_norm": 15.518258094787598,
      "learning_rate": 4.218750000000001e-06,
      "loss": 1.8059,
      "step": 1866
    },
    {
      "bce_eff_w": 0.42005000000000003,
      "bce_loss": -0.4621550142765045,
      "epoch": 9.33,
      "step": 1866
    },
    {
      "epoch": 9.335,
      "grad_norm": 22.43596839904785,
      "learning_rate": 4.1875e-06,
      "loss": 0.3607,
      "step": 1867
    },
    {
      "bce_eff_w": 0.4202,
      "bce_loss": -0.4628905653953552,
      "epoch": 9.335,
      "step": 1867
    },
    {
      "epoch": 9.34,
      "grad_norm": 30.2547550201416,
      "learning_rate": 4.15625e-06,
      "loss": 0.766,
      "step": 1868
    },
    {
      "bce_eff_w": 0.42035,
      "bce_loss": -0.4579310715198517,
      "epoch": 9.34,
      "step": 1868
    },
    {
      "epoch": 9.345,
      "grad_norm": 12.281027793884277,
      "learning_rate": 4.125e-06,
      "loss": 1.9248,
      "step": 1869
    },
    {
      "bce_eff_w": 0.4205,
      "bce_loss": -0.4606834948062897,
      "epoch": 9.345,
      "step": 1869
    },
    {
      "epoch": 9.35,
      "grad_norm": 16.166610717773438,
      "learning_rate": 4.09375e-06,
      "loss": 0.6672,
      "step": 1870
    },
    {
      "bce_eff_w": 0.42065,
      "bce_loss": -0.4597458839416504,
      "epoch": 9.35,
      "step": 1870
    },
    {
      "epoch": 9.355,
      "grad_norm": 19.20970344543457,
      "learning_rate": 4.0625000000000005e-06,
      "loss": 0.2456,
      "step": 1871
    },
    {
      "bce_eff_w": 0.4208,
      "bce_loss": -0.45992040634155273,
      "epoch": 9.355,
      "step": 1871
    },
    {
      "epoch": 9.36,
      "grad_norm": 17.262218475341797,
      "learning_rate": 4.031250000000001e-06,
      "loss": 0.578,
      "step": 1872
    },
    {
      "bce_eff_w": 0.42095000000000005,
      "bce_loss": -0.4612793028354645,
      "epoch": 9.36,
      "step": 1872
    },
    {
      "epoch": 9.365,
      "grad_norm": 19.47782325744629,
      "learning_rate": 4.000000000000001e-06,
      "loss": 0.9482,
      "step": 1873
    },
    {
      "bce_eff_w": 0.42110000000000003,
      "bce_loss": -0.46315625309944153,
      "epoch": 9.365,
      "step": 1873
    },
    {
      "epoch": 9.37,
      "grad_norm": 17.201980590820312,
      "learning_rate": 3.96875e-06,
      "loss": 0.6624,
      "step": 1874
    },
    {
      "bce_eff_w": 0.42125,
      "bce_loss": -0.4569583237171173,
      "epoch": 9.37,
      "step": 1874
    },
    {
      "epoch": 9.375,
      "grad_norm": 22.496158599853516,
      "learning_rate": 3.9375e-06,
      "loss": 0.8218,
      "step": 1875
    },
    {
      "bce_eff_w": 0.4214,
      "bce_loss": -0.46306437253952026,
      "epoch": 9.375,
      "step": 1875
    },
    {
      "epoch": 9.38,
      "grad_norm": 22.63091468811035,
      "learning_rate": 3.90625e-06,
      "loss": 0.71,
      "step": 1876
    },
    {
      "bce_eff_w": 0.42155,
      "bce_loss": -0.4625409245491028,
      "epoch": 9.38,
      "step": 1876
    },
    {
      "epoch": 9.385,
      "grad_norm": 20.668344497680664,
      "learning_rate": 3.875e-06,
      "loss": 1.0701,
      "step": 1877
    },
    {
      "bce_eff_w": 0.42169999999999996,
      "bce_loss": -0.4528300166130066,
      "epoch": 9.385,
      "step": 1877
    },
    {
      "epoch": 9.39,
      "grad_norm": 12.697352409362793,
      "learning_rate": 3.84375e-06,
      "loss": 1.8017,
      "step": 1878
    },
    {
      "bce_eff_w": 0.42185000000000006,
      "bce_loss": -0.45804816484451294,
      "epoch": 9.39,
      "step": 1878
    },
    {
      "epoch": 9.395,
      "grad_norm": 22.289491653442383,
      "learning_rate": 3.8125e-06,
      "loss": 0.1999,
      "step": 1879
    },
    {
      "bce_eff_w": 0.42200000000000004,
      "bce_loss": -0.46244028210639954,
      "epoch": 9.395,
      "step": 1879
    },
    {
      "epoch": 9.4,
      "grad_norm": 24.803382873535156,
      "learning_rate": 3.78125e-06,
      "loss": 0.4011,
      "step": 1880
    },
    {
      "bce_eff_w": 0.42215,
      "bce_loss": -0.46338987350463867,
      "epoch": 9.4,
      "step": 1880
    },
    {
      "epoch": 9.405,
      "grad_norm": 23.58450698852539,
      "learning_rate": 3.75e-06,
      "loss": 1.4099,
      "step": 1881
    },
    {
      "bce_eff_w": 0.4223,
      "bce_loss": -0.4595435559749603,
      "epoch": 9.405,
      "step": 1881
    },
    {
      "epoch": 9.41,
      "grad_norm": 28.68011474609375,
      "learning_rate": 3.71875e-06,
      "loss": 0.8026,
      "step": 1882
    },
    {
      "bce_eff_w": 0.42245,
      "bce_loss": -0.46403029561042786,
      "epoch": 9.41,
      "step": 1882
    },
    {
      "epoch": 9.415,
      "grad_norm": 16.67622184753418,
      "learning_rate": 3.6875e-06,
      "loss": 0.4746,
      "step": 1883
    },
    {
      "bce_eff_w": 0.4226,
      "bce_loss": -0.45967695116996765,
      "epoch": 9.415,
      "step": 1883
    },
    {
      "epoch": 9.42,
      "grad_norm": 14.076147079467773,
      "learning_rate": 3.65625e-06,
      "loss": 0.5736,
      "step": 1884
    },
    {
      "bce_eff_w": 0.42275,
      "bce_loss": -0.46186143159866333,
      "epoch": 9.42,
      "step": 1884
    },
    {
      "epoch": 9.425,
      "grad_norm": 17.59980010986328,
      "learning_rate": 3.625e-06,
      "loss": 1.5253,
      "step": 1885
    },
    {
      "bce_eff_w": 0.4229,
      "bce_loss": -0.4619508683681488,
      "epoch": 9.425,
      "step": 1885
    },
    {
      "epoch": 9.43,
      "grad_norm": 25.727806091308594,
      "learning_rate": 3.5937499999999997e-06,
      "loss": 1.8408,
      "step": 1886
    },
    {
      "bce_eff_w": 0.42305000000000004,
      "bce_loss": -0.4576832354068756,
      "epoch": 9.43,
      "step": 1886
    },
    {
      "epoch": 9.435,
      "grad_norm": 19.118206024169922,
      "learning_rate": 3.5624999999999998e-06,
      "loss": 1.3458,
      "step": 1887
    },
    {
      "bce_eff_w": 0.4232,
      "bce_loss": -0.4610702693462372,
      "epoch": 9.435,
      "step": 1887
    },
    {
      "epoch": 9.44,
      "grad_norm": 15.562589645385742,
      "learning_rate": 3.53125e-06,
      "loss": 0.0491,
      "step": 1888
    },
    {
      "bce_eff_w": 0.42335,
      "bce_loss": -0.4590681493282318,
      "epoch": 9.44,
      "step": 1888
    },
    {
      "epoch": 9.445,
      "grad_norm": 15.948570251464844,
      "learning_rate": 3.5000000000000004e-06,
      "loss": 0.0475,
      "step": 1889
    },
    {
      "bce_eff_w": 0.4235,
      "bce_loss": -0.4597252905368805,
      "epoch": 9.445,
      "step": 1889
    },
    {
      "epoch": 9.45,
      "grad_norm": 11.961782455444336,
      "learning_rate": 3.4687500000000005e-06,
      "loss": 0.0875,
      "step": 1890
    },
    {
      "bce_eff_w": 0.42365,
      "bce_loss": -0.4637438654899597,
      "epoch": 9.45,
      "step": 1890
    },
    {
      "epoch": 9.455,
      "grad_norm": 45.763519287109375,
      "learning_rate": 3.4375000000000005e-06,
      "loss": 0.491,
      "step": 1891
    },
    {
      "bce_eff_w": 0.4238,
      "bce_loss": -0.458968847990036,
      "epoch": 9.455,
      "step": 1891
    },
    {
      "epoch": 9.46,
      "grad_norm": 21.963594436645508,
      "learning_rate": 3.40625e-06,
      "loss": 0.7877,
      "step": 1892
    },
    {
      "bce_eff_w": 0.42395000000000005,
      "bce_loss": -0.46363574266433716,
      "epoch": 9.46,
      "step": 1892
    },
    {
      "epoch": 9.465,
      "grad_norm": 18.681917190551758,
      "learning_rate": 3.3750000000000003e-06,
      "loss": 0.6386,
      "step": 1893
    },
    {
      "bce_eff_w": 0.42410000000000003,
      "bce_loss": -0.46301886439323425,
      "epoch": 9.465,
      "step": 1893
    },
    {
      "epoch": 9.47,
      "grad_norm": 33.447105407714844,
      "learning_rate": 3.3437500000000004e-06,
      "loss": 0.4268,
      "step": 1894
    },
    {
      "bce_eff_w": 0.42425,
      "bce_loss": -0.45805299282073975,
      "epoch": 9.47,
      "step": 1894
    },
    {
      "epoch": 9.475,
      "grad_norm": 17.48807144165039,
      "learning_rate": 3.3125000000000005e-06,
      "loss": 0.8484,
      "step": 1895
    },
    {
      "bce_eff_w": 0.4244,
      "bce_loss": -0.4551869332790375,
      "epoch": 9.475,
      "step": 1895
    },
    {
      "epoch": 9.48,
      "grad_norm": 17.534414291381836,
      "learning_rate": 3.28125e-06,
      "loss": 2.1238,
      "step": 1896
    },
    {
      "bce_eff_w": 0.42455,
      "bce_loss": -0.45941099524497986,
      "epoch": 9.48,
      "step": 1896
    },
    {
      "epoch": 9.485,
      "grad_norm": 18.971923828125,
      "learning_rate": 3.2500000000000002e-06,
      "loss": 1.5342,
      "step": 1897
    },
    {
      "bce_eff_w": 0.42469999999999997,
      "bce_loss": -0.45661184191703796,
      "epoch": 9.485,
      "step": 1897
    },
    {
      "epoch": 9.49,
      "grad_norm": 16.624347686767578,
      "learning_rate": 3.2187500000000003e-06,
      "loss": 0.3137,
      "step": 1898
    },
    {
      "bce_eff_w": 0.42485000000000006,
      "bce_loss": -0.4613000154495239,
      "epoch": 9.49,
      "step": 1898
    },
    {
      "epoch": 9.495,
      "grad_norm": 9.775066375732422,
      "learning_rate": 3.1875000000000004e-06,
      "loss": -0.0911,
      "step": 1899
    },
    {
      "bce_eff_w": 0.425,
      "bce_loss": -0.44926440715789795,
      "epoch": 9.495,
      "step": 1899
    },
    {
      "epoch": 9.5,
      "grad_norm": 17.74997901916504,
      "learning_rate": 3.15625e-06,
      "loss": 0.6719,
      "step": 1900
    },
    {
      "bce_eff_w": 0.42515,
      "bce_loss": -0.46200770139694214,
      "epoch": 9.5,
      "step": 1900
    },
    {
      "epoch": 9.505,
      "grad_norm": 15.324233055114746,
      "learning_rate": 3.125e-06,
      "loss": 0.0182,
      "step": 1901
    },
    {
      "bce_eff_w": 0.4253,
      "bce_loss": -0.45529937744140625,
      "epoch": 9.505,
      "step": 1901
    },
    {
      "epoch": 9.51,
      "grad_norm": 30.670194625854492,
      "learning_rate": 3.0937500000000002e-06,
      "loss": 0.907,
      "step": 1902
    },
    {
      "bce_eff_w": 0.42545,
      "bce_loss": -0.46439680457115173,
      "epoch": 9.51,
      "step": 1902
    },
    {
      "epoch": 9.515,
      "grad_norm": 14.919407844543457,
      "learning_rate": 3.0625e-06,
      "loss": 0.1934,
      "step": 1903
    },
    {
      "bce_eff_w": 0.4256,
      "bce_loss": -0.45369869470596313,
      "epoch": 9.515,
      "step": 1903
    },
    {
      "epoch": 9.52,
      "grad_norm": 10.256668090820312,
      "learning_rate": 3.03125e-06,
      "loss": -0.0279,
      "step": 1904
    },
    {
      "bce_eff_w": 0.42574999999999996,
      "bce_loss": -0.46102097630500793,
      "epoch": 9.52,
      "step": 1904
    },
    {
      "epoch": 9.525,
      "grad_norm": 20.042943954467773,
      "learning_rate": 3e-06,
      "loss": 0.1503,
      "step": 1905
    },
    {
      "bce_eff_w": 0.4259,
      "bce_loss": -0.46063047647476196,
      "epoch": 9.525,
      "step": 1905
    },
    {
      "epoch": 9.53,
      "grad_norm": 27.28995132446289,
      "learning_rate": 2.96875e-06,
      "loss": 0.4458,
      "step": 1906
    },
    {
      "bce_eff_w": 0.42605,
      "bce_loss": -0.4582158327102661,
      "epoch": 9.53,
      "step": 1906
    },
    {
      "epoch": 9.535,
      "grad_norm": 17.35418128967285,
      "learning_rate": 2.9375e-06,
      "loss": 0.1029,
      "step": 1907
    },
    {
      "bce_eff_w": 0.4262,
      "bce_loss": -0.46502894163131714,
      "epoch": 9.535,
      "step": 1907
    },
    {
      "epoch": 9.54,
      "grad_norm": 18.984100341796875,
      "learning_rate": 2.9062500000000003e-06,
      "loss": 0.1886,
      "step": 1908
    },
    {
      "bce_eff_w": 0.42635,
      "bce_loss": -0.46190840005874634,
      "epoch": 9.54,
      "step": 1908
    },
    {
      "epoch": 9.545,
      "grad_norm": 22.071020126342773,
      "learning_rate": 2.8750000000000004e-06,
      "loss": 0.5355,
      "step": 1909
    },
    {
      "bce_eff_w": 0.4265,
      "bce_loss": -0.4563562273979187,
      "epoch": 9.545,
      "step": 1909
    },
    {
      "epoch": 9.55,
      "grad_norm": 23.87075424194336,
      "learning_rate": 2.84375e-06,
      "loss": 0.4942,
      "step": 1910
    },
    {
      "bce_eff_w": 0.42665,
      "bce_loss": -0.45089706778526306,
      "epoch": 9.55,
      "step": 1910
    },
    {
      "epoch": 9.555,
      "grad_norm": 12.263566970825195,
      "learning_rate": 2.8125e-06,
      "loss": 0.1318,
      "step": 1911
    },
    {
      "bce_eff_w": 0.4268,
      "bce_loss": -0.4615544080734253,
      "epoch": 9.555,
      "step": 1911
    },
    {
      "epoch": 9.56,
      "grad_norm": 12.280269622802734,
      "learning_rate": 2.7812500000000003e-06,
      "loss": 0.0112,
      "step": 1912
    },
    {
      "bce_eff_w": 0.42695,
      "bce_loss": -0.4635571539402008,
      "epoch": 9.56,
      "step": 1912
    },
    {
      "epoch": 9.565,
      "grad_norm": 12.468164443969727,
      "learning_rate": 2.7500000000000004e-06,
      "loss": -0.0207,
      "step": 1913
    },
    {
      "bce_eff_w": 0.42710000000000004,
      "bce_loss": -0.448727011680603,
      "epoch": 9.565,
      "step": 1913
    },
    {
      "epoch": 9.57,
      "grad_norm": 19.273162841796875,
      "learning_rate": 2.71875e-06,
      "loss": 1.7005,
      "step": 1914
    },
    {
      "bce_eff_w": 0.42725,
      "bce_loss": -0.4537507891654968,
      "epoch": 9.57,
      "step": 1914
    },
    {
      "epoch": 9.575,
      "grad_norm": 12.81432056427002,
      "learning_rate": 2.6875e-06,
      "loss": 0.1145,
      "step": 1915
    },
    {
      "bce_eff_w": 0.4274,
      "bce_loss": -0.4578796625137329,
      "epoch": 9.575,
      "step": 1915
    },
    {
      "epoch": 9.58,
      "grad_norm": 18.949337005615234,
      "learning_rate": 2.65625e-06,
      "loss": 0.5172,
      "step": 1916
    },
    {
      "bce_eff_w": 0.42755,
      "bce_loss": -0.4633704721927643,
      "epoch": 9.58,
      "step": 1916
    },
    {
      "epoch": 9.585,
      "grad_norm": 16.467235565185547,
      "learning_rate": 2.625e-06,
      "loss": 1.1217,
      "step": 1917
    },
    {
      "bce_eff_w": 0.42769999999999997,
      "bce_loss": -0.46198421716690063,
      "epoch": 9.585,
      "step": 1917
    },
    {
      "epoch": 9.59,
      "grad_norm": 22.705299377441406,
      "learning_rate": 2.59375e-06,
      "loss": 1.1588,
      "step": 1918
    },
    {
      "bce_eff_w": 0.42784999999999995,
      "bce_loss": -0.46048668026924133,
      "epoch": 9.59,
      "step": 1918
    },
    {
      "epoch": 9.595,
      "grad_norm": 20.076583862304688,
      "learning_rate": 2.5625e-06,
      "loss": 1.2498,
      "step": 1919
    },
    {
      "bce_eff_w": 0.428,
      "bce_loss": -0.4644078314304352,
      "epoch": 9.595,
      "step": 1919
    },
    {
      "epoch": 9.6,
      "grad_norm": 19.034687042236328,
      "learning_rate": 2.53125e-06,
      "loss": 1.0276,
      "step": 1920
    },
    {
      "bce_eff_w": 0.42815,
      "bce_loss": -0.4549424648284912,
      "epoch": 9.6,
      "step": 1920
    },
    {
      "epoch": 9.605,
      "grad_norm": 17.225139617919922,
      "learning_rate": 2.5e-06,
      "loss": 1.6701,
      "step": 1921
    },
    {
      "bce_eff_w": 0.4283,
      "bce_loss": -0.4623344838619232,
      "epoch": 9.605,
      "step": 1921
    },
    {
      "epoch": 9.61,
      "grad_norm": 14.032279968261719,
      "learning_rate": 2.4687500000000003e-06,
      "loss": 0.096,
      "step": 1922
    },
    {
      "bce_eff_w": 0.42845,
      "bce_loss": -0.46245652437210083,
      "epoch": 9.61,
      "step": 1922
    },
    {
      "epoch": 9.615,
      "grad_norm": 6.540911674499512,
      "learning_rate": 2.4375000000000004e-06,
      "loss": -0.1167,
      "step": 1923
    },
    {
      "bce_eff_w": 0.4286,
      "bce_loss": -0.460131973028183,
      "epoch": 9.615,
      "step": 1923
    },
    {
      "epoch": 9.62,
      "grad_norm": 17.380876541137695,
      "learning_rate": 2.40625e-06,
      "loss": 0.7535,
      "step": 1924
    },
    {
      "bce_eff_w": 0.42874999999999996,
      "bce_loss": -0.4609625041484833,
      "epoch": 9.62,
      "step": 1924
    },
    {
      "epoch": 9.625,
      "grad_norm": 16.33724594116211,
      "learning_rate": 2.375e-06,
      "loss": 0.7812,
      "step": 1925
    },
    {
      "bce_eff_w": 0.4289,
      "bce_loss": -0.4609704911708832,
      "epoch": 9.625,
      "step": 1925
    },
    {
      "epoch": 9.63,
      "grad_norm": 25.24786949157715,
      "learning_rate": 2.3437500000000002e-06,
      "loss": 1.5827,
      "step": 1926
    },
    {
      "bce_eff_w": 0.42905,
      "bce_loss": -0.4624490439891815,
      "epoch": 9.63,
      "step": 1926
    },
    {
      "epoch": 9.635,
      "grad_norm": 48.569435119628906,
      "learning_rate": 2.3125e-06,
      "loss": 0.8631,
      "step": 1927
    },
    {
      "bce_eff_w": 0.4292,
      "bce_loss": -0.4615464210510254,
      "epoch": 9.635,
      "step": 1927
    },
    {
      "epoch": 9.64,
      "grad_norm": 41.41294479370117,
      "learning_rate": 2.28125e-06,
      "loss": 0.5135,
      "step": 1928
    },
    {
      "bce_eff_w": 0.42935,
      "bce_loss": -0.46225133538246155,
      "epoch": 9.64,
      "step": 1928
    },
    {
      "epoch": 9.645,
      "grad_norm": 19.585384368896484,
      "learning_rate": 2.25e-06,
      "loss": 0.5047,
      "step": 1929
    },
    {
      "bce_eff_w": 0.4295,
      "bce_loss": -0.46111348271369934,
      "epoch": 9.645,
      "step": 1929
    },
    {
      "epoch": 9.65,
      "grad_norm": 17.755075454711914,
      "learning_rate": 2.21875e-06,
      "loss": 0.2867,
      "step": 1930
    },
    {
      "bce_eff_w": 0.42965,
      "bce_loss": -0.4642276465892792,
      "epoch": 9.65,
      "step": 1930
    },
    {
      "epoch": 9.655,
      "grad_norm": 11.960714340209961,
      "learning_rate": 2.1875e-06,
      "loss": 1.2581,
      "step": 1931
    },
    {
      "bce_eff_w": 0.4298,
      "bce_loss": -0.461260586977005,
      "epoch": 9.655,
      "step": 1931
    },
    {
      "epoch": 9.66,
      "grad_norm": 16.42441177368164,
      "learning_rate": 2.15625e-06,
      "loss": 0.2382,
      "step": 1932
    },
    {
      "bce_eff_w": 0.42995,
      "bce_loss": -0.4592512845993042,
      "epoch": 9.66,
      "step": 1932
    },
    {
      "epoch": 9.665,
      "grad_norm": 16.976242065429688,
      "learning_rate": 2.1250000000000004e-06,
      "loss": 0.8767,
      "step": 1933
    },
    {
      "bce_eff_w": 0.43010000000000004,
      "bce_loss": -0.45841825008392334,
      "epoch": 9.665,
      "step": 1933
    },
    {
      "epoch": 9.67,
      "grad_norm": 19.773967742919922,
      "learning_rate": 2.09375e-06,
      "loss": 0.0681,
      "step": 1934
    },
    {
      "bce_eff_w": 0.43025,
      "bce_loss": -0.45655226707458496,
      "epoch": 9.67,
      "step": 1934
    },
    {
      "epoch": 9.675,
      "grad_norm": 25.93572998046875,
      "learning_rate": 2.0625e-06,
      "loss": 1.5186,
      "step": 1935
    },
    {
      "bce_eff_w": 0.4304,
      "bce_loss": -0.4606175720691681,
      "epoch": 9.675,
      "step": 1935
    },
    {
      "epoch": 9.68,
      "grad_norm": 15.873064994812012,
      "learning_rate": 2.0312500000000002e-06,
      "loss": 0.0899,
      "step": 1936
    },
    {
      "bce_eff_w": 0.43055,
      "bce_loss": -0.4636889696121216,
      "epoch": 9.68,
      "step": 1936
    },
    {
      "epoch": 9.685,
      "grad_norm": 18.853832244873047,
      "learning_rate": 2.0000000000000003e-06,
      "loss": 0.4165,
      "step": 1937
    },
    {
      "bce_eff_w": 0.43069999999999997,
      "bce_loss": -0.46165600419044495,
      "epoch": 9.685,
      "step": 1937
    },
    {
      "epoch": 9.69,
      "grad_norm": 21.76918601989746,
      "learning_rate": 1.96875e-06,
      "loss": 1.0091,
      "step": 1938
    },
    {
      "bce_eff_w": 0.43084999999999996,
      "bce_loss": -0.4635956585407257,
      "epoch": 9.69,
      "step": 1938
    },
    {
      "epoch": 9.695,
      "grad_norm": 1.5338083505630493,
      "learning_rate": 1.9375e-06,
      "loss": -0.1832,
      "step": 1939
    },
    {
      "bce_eff_w": 0.431,
      "bce_loss": -0.4611015319824219,
      "epoch": 9.695,
      "step": 1939
    },
    {
      "epoch": 9.7,
      "grad_norm": 19.874990463256836,
      "learning_rate": 1.90625e-06,
      "loss": 0.9036,
      "step": 1940
    },
    {
      "bce_eff_w": 0.43115,
      "bce_loss": -0.45724692940711975,
      "epoch": 9.7,
      "step": 1940
    },
    {
      "epoch": 9.705,
      "grad_norm": 15.021146774291992,
      "learning_rate": 1.875e-06,
      "loss": 0.8393,
      "step": 1941
    },
    {
      "bce_eff_w": 0.4313,
      "bce_loss": -0.45848456025123596,
      "epoch": 9.705,
      "step": 1941
    },
    {
      "epoch": 9.71,
      "grad_norm": 5.076292037963867,
      "learning_rate": 1.84375e-06,
      "loss": -0.1429,
      "step": 1942
    },
    {
      "bce_eff_w": 0.43145,
      "bce_loss": -0.45860013365745544,
      "epoch": 9.71,
      "step": 1942
    },
    {
      "epoch": 9.715,
      "grad_norm": 21.71879005432129,
      "learning_rate": 1.8125e-06,
      "loss": 0.4728,
      "step": 1943
    },
    {
      "bce_eff_w": 0.4316,
      "bce_loss": -0.45580363273620605,
      "epoch": 9.715,
      "step": 1943
    },
    {
      "epoch": 9.72,
      "grad_norm": 21.593339920043945,
      "learning_rate": 1.7812499999999999e-06,
      "loss": 0.2749,
      "step": 1944
    },
    {
      "bce_eff_w": 0.43174999999999997,
      "bce_loss": -0.46195659041404724,
      "epoch": 9.72,
      "step": 1944
    },
    {
      "epoch": 9.725,
      "grad_norm": 23.074323654174805,
      "learning_rate": 1.7500000000000002e-06,
      "loss": 0.4211,
      "step": 1945
    },
    {
      "bce_eff_w": 0.4319,
      "bce_loss": -0.45664849877357483,
      "epoch": 9.725,
      "step": 1945
    },
    {
      "epoch": 9.73,
      "grad_norm": 18.188425064086914,
      "learning_rate": 1.7187500000000003e-06,
      "loss": 0.645,
      "step": 1946
    },
    {
      "bce_eff_w": 0.43205,
      "bce_loss": -0.46064314246177673,
      "epoch": 9.73,
      "step": 1946
    },
    {
      "epoch": 9.735,
      "grad_norm": 15.386116027832031,
      "learning_rate": 1.6875000000000001e-06,
      "loss": 2.0464,
      "step": 1947
    },
    {
      "bce_eff_w": 0.43220000000000003,
      "bce_loss": -0.4615887999534607,
      "epoch": 9.735,
      "step": 1947
    },
    {
      "epoch": 9.74,
      "grad_norm": 10.734820365905762,
      "learning_rate": 1.6562500000000002e-06,
      "loss": -0.0324,
      "step": 1948
    },
    {
      "bce_eff_w": 0.43235,
      "bce_loss": -0.462035596370697,
      "epoch": 9.74,
      "step": 1948
    },
    {
      "epoch": 9.745,
      "grad_norm": 14.41287899017334,
      "learning_rate": 1.6250000000000001e-06,
      "loss": 0.8038,
      "step": 1949
    },
    {
      "bce_eff_w": 0.4325,
      "bce_loss": -0.45665740966796875,
      "epoch": 9.745,
      "step": 1949
    },
    {
      "epoch": 9.75,
      "grad_norm": 17.854969024658203,
      "learning_rate": 1.5937500000000002e-06,
      "loss": 0.8661,
      "step": 1950
    },
    {
      "bce_eff_w": 0.43265,
      "bce_loss": -0.45954206585884094,
      "epoch": 9.75,
      "step": 1950
    },
    {
      "epoch": 9.755,
      "grad_norm": 12.30534839630127,
      "learning_rate": 1.5625e-06,
      "loss": 1.7784,
      "step": 1951
    },
    {
      "bce_eff_w": 0.4328,
      "bce_loss": -0.45816510915756226,
      "epoch": 9.755,
      "step": 1951
    },
    {
      "epoch": 9.76,
      "grad_norm": 13.783720970153809,
      "learning_rate": 1.53125e-06,
      "loss": 1.6514,
      "step": 1952
    },
    {
      "bce_eff_w": 0.43295,
      "bce_loss": -0.46169522404670715,
      "epoch": 9.76,
      "step": 1952
    },
    {
      "epoch": 9.765,
      "grad_norm": 16.997331619262695,
      "learning_rate": 1.5e-06,
      "loss": 0.1753,
      "step": 1953
    },
    {
      "bce_eff_w": 0.43310000000000004,
      "bce_loss": -0.46071451902389526,
      "epoch": 9.765,
      "step": 1953
    },
    {
      "epoch": 9.77,
      "grad_norm": 17.967605590820312,
      "learning_rate": 1.46875e-06,
      "loss": 0.6355,
      "step": 1954
    },
    {
      "bce_eff_w": 0.43325,
      "bce_loss": -0.45466864109039307,
      "epoch": 9.77,
      "step": 1954
    },
    {
      "epoch": 9.775,
      "grad_norm": 18.813541412353516,
      "learning_rate": 1.4375000000000002e-06,
      "loss": 0.1245,
      "step": 1955
    },
    {
      "bce_eff_w": 0.4334,
      "bce_loss": -0.46274054050445557,
      "epoch": 9.775,
      "step": 1955
    },
    {
      "epoch": 9.78,
      "grad_norm": 15.508420944213867,
      "learning_rate": 1.40625e-06,
      "loss": 1.2241,
      "step": 1956
    },
    {
      "bce_eff_w": 0.43355,
      "bce_loss": -0.461639940738678,
      "epoch": 9.78,
      "step": 1956
    },
    {
      "epoch": 9.785,
      "grad_norm": 24.749977111816406,
      "learning_rate": 1.3750000000000002e-06,
      "loss": 0.4496,
      "step": 1957
    },
    {
      "bce_eff_w": 0.4337,
      "bce_loss": -0.4555557072162628,
      "epoch": 9.785,
      "step": 1957
    },
    {
      "epoch": 9.79,
      "grad_norm": 21.912160873413086,
      "learning_rate": 1.34375e-06,
      "loss": 0.7411,
      "step": 1958
    },
    {
      "bce_eff_w": 0.43384999999999996,
      "bce_loss": -0.4638396203517914,
      "epoch": 9.79,
      "step": 1958
    },
    {
      "epoch": 9.795,
      "grad_norm": 17.53705406188965,
      "learning_rate": 1.3125e-06,
      "loss": 0.8439,
      "step": 1959
    },
    {
      "bce_eff_w": 0.434,
      "bce_loss": -0.4602522850036621,
      "epoch": 9.795,
      "step": 1959
    },
    {
      "epoch": 9.8,
      "grad_norm": 15.487887382507324,
      "learning_rate": 1.28125e-06,
      "loss": 0.0478,
      "step": 1960
    },
    {
      "bce_eff_w": 0.43415,
      "bce_loss": -0.46295690536499023,
      "epoch": 9.8,
      "step": 1960
    },
    {
      "epoch": 9.805,
      "grad_norm": 10.41939640045166,
      "learning_rate": 1.25e-06,
      "loss": -0.0198,
      "step": 1961
    },
    {
      "bce_eff_w": 0.4343,
      "bce_loss": -0.46421018242836,
      "epoch": 9.805,
      "step": 1961
    },
    {
      "epoch": 9.81,
      "grad_norm": 19.55001449584961,
      "learning_rate": 1.2187500000000002e-06,
      "loss": 0.02,
      "step": 1962
    },
    {
      "bce_eff_w": 0.43445,
      "bce_loss": -0.46294674277305603,
      "epoch": 9.81,
      "step": 1962
    },
    {
      "epoch": 9.815,
      "grad_norm": 19.332216262817383,
      "learning_rate": 1.1875e-06,
      "loss": 0.2681,
      "step": 1963
    },
    {
      "bce_eff_w": 0.4346,
      "bce_loss": -0.45591992139816284,
      "epoch": 9.815,
      "step": 1963
    },
    {
      "epoch": 9.82,
      "grad_norm": 11.730533599853516,
      "learning_rate": 1.15625e-06,
      "loss": 0.2125,
      "step": 1964
    },
    {
      "bce_eff_w": 0.43474999999999997,
      "bce_loss": -0.45599934458732605,
      "epoch": 9.82,
      "step": 1964
    },
    {
      "epoch": 9.825,
      "grad_norm": 1.4482758045196533,
      "learning_rate": 1.125e-06,
      "loss": -0.1807,
      "step": 1965
    },
    {
      "bce_eff_w": 0.4349,
      "bce_loss": -0.45928433537483215,
      "epoch": 9.825,
      "step": 1965
    },
    {
      "epoch": 9.83,
      "grad_norm": 19.672748565673828,
      "learning_rate": 1.09375e-06,
      "loss": 0.0385,
      "step": 1966
    },
    {
      "bce_eff_w": 0.43505,
      "bce_loss": -0.45871177315711975,
      "epoch": 9.83,
      "step": 1966
    },
    {
      "epoch": 9.835,
      "grad_norm": 14.829889297485352,
      "learning_rate": 1.0625000000000002e-06,
      "loss": -0.0464,
      "step": 1967
    },
    {
      "bce_eff_w": 0.43520000000000003,
      "bce_loss": -0.4641610383987427,
      "epoch": 9.835,
      "step": 1967
    },
    {
      "epoch": 9.84,
      "grad_norm": 24.64482879638672,
      "learning_rate": 1.03125e-06,
      "loss": 0.6286,
      "step": 1968
    },
    {
      "bce_eff_w": 0.43535,
      "bce_loss": -0.4584423005580902,
      "epoch": 9.84,
      "step": 1968
    },
    {
      "epoch": 9.845,
      "grad_norm": 17.449913024902344,
      "learning_rate": 1.0000000000000002e-06,
      "loss": 0.6441,
      "step": 1969
    },
    {
      "bce_eff_w": 0.4355,
      "bce_loss": -0.4611515700817108,
      "epoch": 9.845,
      "step": 1969
    },
    {
      "epoch": 9.85,
      "grad_norm": 25.050260543823242,
      "learning_rate": 9.6875e-07,
      "loss": 0.6808,
      "step": 1970
    },
    {
      "bce_eff_w": 0.43565,
      "bce_loss": -0.4617054760456085,
      "epoch": 9.85,
      "step": 1970
    },
    {
      "epoch": 9.855,
      "grad_norm": 17.303577423095703,
      "learning_rate": 9.375e-07,
      "loss": 0.0535,
      "step": 1971
    },
    {
      "bce_eff_w": 0.4358,
      "bce_loss": -0.4603152573108673,
      "epoch": 9.855,
      "step": 1971
    },
    {
      "epoch": 9.86,
      "grad_norm": 18.4334716796875,
      "learning_rate": 9.0625e-07,
      "loss": 0.8041,
      "step": 1972
    },
    {
      "bce_eff_w": 0.43595,
      "bce_loss": -0.44644859433174133,
      "epoch": 9.86,
      "step": 1972
    },
    {
      "epoch": 9.865,
      "grad_norm": 14.846516609191895,
      "learning_rate": 8.750000000000001e-07,
      "loss": 0.2294,
      "step": 1973
    },
    {
      "bce_eff_w": 0.43610000000000004,
      "bce_loss": -0.4621075391769409,
      "epoch": 9.865,
      "step": 1973
    },
    {
      "epoch": 9.87,
      "grad_norm": 46.258731842041016,
      "learning_rate": 8.437500000000001e-07,
      "loss": 0.9882,
      "step": 1974
    },
    {
      "bce_eff_w": 0.43625,
      "bce_loss": -0.4517330825328827,
      "epoch": 9.87,
      "step": 1974
    },
    {
      "epoch": 9.875,
      "grad_norm": 17.569835662841797,
      "learning_rate": 8.125000000000001e-07,
      "loss": 0.0414,
      "step": 1975
    },
    {
      "bce_eff_w": 0.4364,
      "bce_loss": -0.46083104610443115,
      "epoch": 9.875,
      "step": 1975
    },
    {
      "epoch": 9.88,
      "grad_norm": 17.64826202392578,
      "learning_rate": 7.8125e-07,
      "loss": 0.1345,
      "step": 1976
    },
    {
      "bce_eff_w": 0.43655,
      "bce_loss": -0.4576181471347809,
      "epoch": 9.88,
      "step": 1976
    },
    {
      "epoch": 9.885,
      "grad_norm": 29.096715927124023,
      "learning_rate": 7.5e-07,
      "loss": 1.0719,
      "step": 1977
    },
    {
      "bce_eff_w": 0.4367,
      "bce_loss": -0.46340304613113403,
      "epoch": 9.885,
      "step": 1977
    },
    {
      "epoch": 9.89,
      "grad_norm": 5.7475152015686035,
      "learning_rate": 7.187500000000001e-07,
      "loss": -0.116,
      "step": 1978
    },
    {
      "bce_eff_w": 0.43684999999999996,
      "bce_loss": -0.4621433913707733,
      "epoch": 9.89,
      "step": 1978
    },
    {
      "epoch": 9.895,
      "grad_norm": 7.620483875274658,
      "learning_rate": 6.875000000000001e-07,
      "loss": -0.0265,
      "step": 1979
    },
    {
      "bce_eff_w": 0.437,
      "bce_loss": -0.45854875445365906,
      "epoch": 9.895,
      "step": 1979
    },
    {
      "epoch": 9.9,
      "grad_norm": 26.68508529663086,
      "learning_rate": 6.5625e-07,
      "loss": 1.0859,
      "step": 1980
    },
    {
      "bce_eff_w": 0.43715,
      "bce_loss": -0.46323028206825256,
      "epoch": 9.9,
      "step": 1980
    },
    {
      "epoch": 9.905,
      "grad_norm": 21.028385162353516,
      "learning_rate": 6.25e-07,
      "loss": 0.6845,
      "step": 1981
    },
    {
      "bce_eff_w": 0.4373,
      "bce_loss": -0.4603371322154999,
      "epoch": 9.905,
      "step": 1981
    },
    {
      "epoch": 9.91,
      "grad_norm": 21.807584762573242,
      "learning_rate": 5.9375e-07,
      "loss": 0.9836,
      "step": 1982
    },
    {
      "bce_eff_w": 0.43745,
      "bce_loss": -0.4606950879096985,
      "epoch": 9.91,
      "step": 1982
    },
    {
      "epoch": 9.915,
      "grad_norm": 10.170205116271973,
      "learning_rate": 5.625e-07,
      "loss": 0.9585,
      "step": 1983
    },
    {
      "bce_eff_w": 0.4376,
      "bce_loss": -0.46215781569480896,
      "epoch": 9.915,
      "step": 1983
    },
    {
      "epoch": 9.92,
      "grad_norm": 25.17640495300293,
      "learning_rate": 5.312500000000001e-07,
      "loss": 0.5132,
      "step": 1984
    },
    {
      "bce_eff_w": 0.43775,
      "bce_loss": -0.461904913187027,
      "epoch": 9.92,
      "step": 1984
    },
    {
      "epoch": 9.925,
      "grad_norm": 22.746740341186523,
      "learning_rate": 5.000000000000001e-07,
      "loss": 0.0795,
      "step": 1985
    },
    {
      "bce_eff_w": 0.4379,
      "bce_loss": -0.46168896555900574,
      "epoch": 9.925,
      "step": 1985
    },
    {
      "epoch": 9.93,
      "grad_norm": 19.02983856201172,
      "learning_rate": 4.6875e-07,
      "loss": 0.438,
      "step": 1986
    },
    {
      "bce_eff_w": 0.43805,
      "bce_loss": -0.45866912603378296,
      "epoch": 9.93,
      "step": 1986
    },
    {
      "epoch": 9.935,
      "grad_norm": 16.143272399902344,
      "learning_rate": 4.3750000000000005e-07,
      "loss": 1.4012,
      "step": 1987
    },
    {
      "bce_eff_w": 0.43820000000000003,
      "bce_loss": -0.461343377828598,
      "epoch": 9.935,
      "step": 1987
    },
    {
      "epoch": 9.94,
      "grad_norm": 22.916034698486328,
      "learning_rate": 4.0625000000000003e-07,
      "loss": 0.5849,
      "step": 1988
    },
    {
      "bce_eff_w": 0.43835,
      "bce_loss": -0.461159884929657,
      "epoch": 9.94,
      "step": 1988
    },
    {
      "epoch": 9.945,
      "grad_norm": 18.899585723876953,
      "learning_rate": 3.75e-07,
      "loss": 0.3086,
      "step": 1989
    },
    {
      "bce_eff_w": 0.4385,
      "bce_loss": -0.46315091848373413,
      "epoch": 9.945,
      "step": 1989
    },
    {
      "epoch": 9.95,
      "grad_norm": 27.507984161376953,
      "learning_rate": 3.4375000000000004e-07,
      "loss": 0.9296,
      "step": 1990
    },
    {
      "bce_eff_w": 0.43865,
      "bce_loss": -0.46245285868644714,
      "epoch": 9.95,
      "step": 1990
    },
    {
      "epoch": 9.955,
      "grad_norm": 15.681512832641602,
      "learning_rate": 3.125e-07,
      "loss": 1.0719,
      "step": 1991
    },
    {
      "bce_eff_w": 0.4388,
      "bce_loss": -0.4584061801433563,
      "epoch": 9.955,
      "step": 1991
    },
    {
      "epoch": 9.96,
      "grad_norm": 17.116241455078125,
      "learning_rate": 2.8125e-07,
      "loss": 0.2996,
      "step": 1992
    },
    {
      "bce_eff_w": 0.43895,
      "bce_loss": -0.4609115421772003,
      "epoch": 9.96,
      "step": 1992
    },
    {
      "epoch": 9.965,
      "grad_norm": 15.776603698730469,
      "learning_rate": 2.5000000000000004e-07,
      "loss": 0.0743,
      "step": 1993
    },
    {
      "bce_eff_w": 0.43910000000000005,
      "bce_loss": -0.46292591094970703,
      "epoch": 9.965,
      "step": 1993
    },
    {
      "epoch": 9.97,
      "grad_norm": 13.83265209197998,
      "learning_rate": 2.1875000000000002e-07,
      "loss": 1.2401,
      "step": 1994
    },
    {
      "bce_eff_w": 0.43925000000000003,
      "bce_loss": -0.46001529693603516,
      "epoch": 9.97,
      "step": 1994
    },
    {
      "epoch": 9.975,
      "grad_norm": 9.046210289001465,
      "learning_rate": 1.875e-07,
      "loss": -0.0275,
      "step": 1995
    },
    {
      "bce_eff_w": 0.4394,
      "bce_loss": -0.46047428250312805,
      "epoch": 9.975,
      "step": 1995
    },
    {
      "epoch": 9.98,
      "grad_norm": 15.183917045593262,
      "learning_rate": 1.5625e-07,
      "loss": 0.3787,
      "step": 1996
    },
    {
      "bce_eff_w": 0.43955,
      "bce_loss": -0.4633486568927765,
      "epoch": 9.98,
      "step": 1996
    },
    {
      "epoch": 9.985,
      "grad_norm": 12.028801918029785,
      "learning_rate": 1.2500000000000002e-07,
      "loss": 1.071,
      "step": 1997
    },
    {
      "bce_eff_w": 0.4397,
      "bce_loss": -0.4595849812030792,
      "epoch": 9.985,
      "step": 1997
    },
    {
      "epoch": 9.99,
      "grad_norm": 29.455095291137695,
      "learning_rate": 9.375e-08,
      "loss": 0.2974,
      "step": 1998
    },
    {
      "bce_eff_w": 0.43984999999999996,
      "bce_loss": -0.45878708362579346,
      "epoch": 9.99,
      "step": 1998
    },
    {
      "epoch": 9.995,
      "grad_norm": 17.341997146606445,
      "learning_rate": 6.250000000000001e-08,
      "loss": 0.6799,
      "step": 1999
    },
    {
      "bce_eff_w": 0.44,
      "bce_loss": -0.4630375802516937,
      "epoch": 9.995,
      "step": 1999
    },
    {
      "epoch": 10.0,
      "grad_norm": 16.2457275390625,
      "learning_rate": 3.1250000000000005e-08,
      "loss": 0.4224,
      "step": 2000
    }
  ],
  "logging_steps": 1,
  "max_steps": 2000,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 10,
  "save_steps": 1000,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 144871039800.0,
  "train_batch_size": 1,
  "trial_name": null,
  "trial_params": null
}
